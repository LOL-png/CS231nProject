{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transition Merger with Pairwise HOISDF Outputs\n",
    "\n",
    "This notebook demonstrates how to train the transition merger model using a dataset of HOISDF outputs.\n",
    "The model learns to create smooth transitions between different hand-object interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from transition_merger_model import TransitionMergerModel, HOISDFOutputs\n",
    "from transition_dataset import HOISDFTransitionDataset, TransitionTrainer\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare HOISDF Output Data\n",
    "\n",
    "First, you need to extract HOISDF outputs from your trained model and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hoisdf_outputs(hoisdf_model, video_frames, save_path, sequence_name):\n",
    "    \"\"\"Extract and save HOISDF outputs for a video sequence.\"\"\"\n",
    "    hoisdf_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process video through HOISDF\n",
    "        outputs = hoisdf_model(video_frames)\n",
    "        \n",
    "        # Extract relevant outputs\n",
    "        hoisdf_data = {\n",
    "            'mano_params': outputs['mano_params'].cpu(),\n",
    "            'hand_sdf': outputs['hand_sdf'].cpu(),\n",
    "            'object_sdf': outputs['object_sdf'].cpu(),\n",
    "            'contact_points': outputs['contact_points'].cpu(),\n",
    "            'contact_frames': outputs['contact_frames'].cpu(),\n",
    "            'hand_vertices': outputs['hand_vertices'].cpu(),\n",
    "            'object_center': outputs['object_center'].cpu(),\n",
    "            'sequence_name': sequence_name\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        save_file = Path(save_path) / f\"{sequence_name}.pkl\"\n",
    "        with open(save_file, 'wb') as f:\n",
    "            pickle.dump(hoisdf_data, f)\n",
    "            \n",
    "    print(f\"Saved HOISDF outputs to {save_file}\")\n",
    "\n",
    "# Example: Save outputs for multiple sequences\n",
    "# data_dir = Path('data/hoisdf_outputs')\n",
    "# data_dir.mkdir(exist_ok=True)\n",
    "#\n",
    "# for video_path in video_paths:\n",
    "#     video_frames = load_video(video_path)\n",
    "#     save_hoisdf_outputs(hoisdf_model, video_frames, data_dir, video_path.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Example HOISDF Outputs\n",
    "\n",
    "For demonstration, let's create synthetic HOISDF outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_hoisdf_sequence(sequence_type='pick_phone', num_frames=100):\n",
    "    \"\"\"Create synthetic HOISDF outputs for different tasks.\"\"\"\n",
    "    \n",
    "    # Base MANO parameters\n",
    "    mano_params = torch.zeros(num_frames, 51)\n",
    "    \n",
    "    if sequence_type == 'pick_phone':\n",
    "        # Simulate picking up phone motion\n",
    "        t = torch.linspace(0, 1, num_frames)\n",
    "        mano_params[:, 0] = 0.2 * torch.sin(t * np.pi)  # X translation\n",
    "        mano_params[:, 1] = -0.3 + 0.2 * t  # Y translation (moving up)\n",
    "        mano_params[:, 2] = 0.1 * torch.cos(t * np.pi)  # Z translation\n",
    "        # Add some finger motion\n",
    "        mano_params[:, 3:8] = 0.5 * torch.sin(t.unsqueeze(1) * np.pi)\n",
    "        \n",
    "    elif sequence_type == 'pick_bottle':\n",
    "        # Simulate picking up bottle motion\n",
    "        t = torch.linspace(0, 1, num_frames)\n",
    "        mano_params[:, 0] = -0.1 + 0.2 * torch.cos(t * np.pi)\n",
    "        mano_params[:, 1] = -0.4 + 0.3 * t\n",
    "        mano_params[:, 2] = 0.15 * torch.sin(t * np.pi)\n",
    "        # Different finger pattern for bottle grasp\n",
    "        mano_params[:, 3:8] = 0.7 * torch.cos(t.unsqueeze(1) * np.pi * 0.5)\n",
    "        \n",
    "    elif sequence_type == 'typing':\n",
    "        # Simulate typing motion\n",
    "        t = torch.linspace(0, 4*np.pi, num_frames)\n",
    "        mano_params[:, 0] = 0.05 * torch.sin(t * 3)\n",
    "        mano_params[:, 1] = -0.2 + 0.02 * torch.sin(t * 5)\n",
    "        mano_params[:, 2] = 0.1\n",
    "        # Finger tapping motion\n",
    "        for i in range(5):\n",
    "            mano_params[:, 3+i*3:6+i*3] = 0.3 * torch.sin(t.unsqueeze(1) * (i+2))\n",
    "    \n",
    "    # Create full HOISDF outputs\n",
    "    return {\n",
    "        'mano_params': mano_params,\n",
    "        'hand_sdf': torch.randn(num_frames, 64, 64, 64) * 0.1,\n",
    "        'object_sdf': torch.randn(num_frames, 64, 64, 64) * 0.1,\n",
    "        'contact_points': torch.randn(num_frames, 10, 3) * 0.05,\n",
    "        'contact_frames': torch.randint(0, 2, (num_frames, 10)).float(),\n",
    "        'hand_vertices': torch.randn(num_frames, 778, 3) * 0.1,\n",
    "        'object_center': torch.randn(num_frames, 3) * 0.2,\n",
    "        'sequence_type': sequence_type\n",
    "    }\n",
    "\n",
    "# Create synthetic dataset\n",
    "data_dir = Path('data/synthetic_hoisdf')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate different types of sequences\n",
    "sequence_types = ['pick_phone', 'pick_bottle', 'typing']\n",
    "num_sequences_per_type = 10\n",
    "\n",
    "for seq_type in sequence_types:\n",
    "    for i in range(num_sequences_per_type):\n",
    "        seq_data = create_synthetic_hoisdf_sequence(seq_type)\n",
    "        filename = data_dir / f\"{seq_type}_{i:03d}.pkl\"\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(seq_data, f)\n",
    "            \n",
    "print(f\"Created {len(sequence_types) * num_sequences_per_type} synthetic sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "dataset_config = {\n",
    "    'sequence_length': 100,\n",
    "    'transition_length': 30,\n",
    "    'context_frames': 20,\n",
    "    'similarity_threshold': 0.3  # Lower threshold to get more pairs\n",
    "}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HOISDFTransitionDataset(\n",
    "    data_dir=str(data_dir),\n",
    "    mode='train',\n",
    "    **dataset_config\n",
    ")\n",
    "\n",
    "val_dataset = HOISDFTransitionDataset(\n",
    "    data_dir=str(data_dir),\n",
    "    mode='val',\n",
    "    **dataset_config\n",
    ")\n",
    "\n",
    "print(f\"Training pairs: {len(train_dataset)}\")\n",
    "print(f\"Validation pairs: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and training configuration\n",
    "config = {\n",
    "    # Model config\n",
    "    'tokenizer': {\n",
    "        'mano_dim': 51,\n",
    "        'sdf_resolution': 64,\n",
    "        'hidden_dim': 256,\n",
    "        'num_tokens': 256\n",
    "    },\n",
    "    'transformer': {\n",
    "        'input_dim': 256,\n",
    "        'hidden_dim': 512,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 6,\n",
    "        'mano_dim': 51,\n",
    "        'chunk_size': 50,\n",
    "        'dropout': 0.1\n",
    "    },\n",
    "    'diffuser': {\n",
    "        'mano_dim': 51,\n",
    "        'hidden_dim': 256,\n",
    "        'condition_dim': 512,\n",
    "        'num_timesteps': 100\n",
    "    },\n",
    "    \n",
    "    # Training config\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 0,  # Set to 0 for debugging\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 50,\n",
    "    'transition_length': 30,\n",
    "    'log_interval': 5,\n",
    "    'device': device,\n",
    "    \n",
    "    # Loss weights\n",
    "    'loss_weights': {\n",
    "        'mano_recon': 1.0,\n",
    "        'contact': 0.5,\n",
    "        'smooth': 0.1,\n",
    "        'boundary': 0.5,\n",
    "        'contrastive': 0.2,\n",
    "        'diffusion': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = TransitionMergerModel(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Dataset Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some dataset pairs\n",
    "sample = train_dataset[0]\n",
    "outputs1 = sample['outputs1']\n",
    "outputs2 = sample['outputs2']\n",
    "gt_transition = sample['gt_transition']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot MANO translations for both sequences\n",
    "ax = axes[0, 0]\n",
    "ax.plot(outputs1.mano_params[:, 0].numpy(), label='Seq1 X', alpha=0.7)\n",
    "ax.plot(outputs2.mano_params[:, 0].numpy(), label='Seq2 X', alpha=0.7)\n",
    "ax.axvline(x=80, color='red', linestyle='--', label='Transition region')\n",
    "ax.set_title('X Translation')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(outputs1.mano_params[:, 1].numpy(), label='Seq1 Y', alpha=0.7)\n",
    "ax.plot(outputs2.mano_params[:, 1].numpy(), label='Seq2 Y', alpha=0.7)\n",
    "ax.axvline(x=80, color='red', linestyle='--')\n",
    "ax.set_title('Y Translation')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot ground truth transition\n",
    "ax = axes[1, 0]\n",
    "ax.plot(gt_transition.mano_params[:, 0].numpy(), label='GT X', color='green')\n",
    "ax.plot(gt_transition.mano_params[:, 1].numpy(), label='GT Y', color='blue')\n",
    "ax.plot(gt_transition.mano_params[:, 2].numpy(), label='GT Z', color='red')\n",
    "ax.set_title('Ground Truth Transition')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Plot compatibility score\n",
    "ax = axes[1, 1]\n",
    "ax.text(0.5, 0.5, f\"Compatibility Score: {sample['compatibility']:.3f}\", \n",
    "        ha='center', va='center', fontsize=16)\n",
    "ax.text(0.5, 0.3, f\"Pair: {sample['names'][0]} → {sample['names'][1]}\", \n",
    "        ha='center', va='center', fontsize=12)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = TransitionTrainer(model, train_dataset, val_dataset, config)\n",
    "\n",
    "# Train for a few epochs (reduce for testing)\n",
    "num_epochs = 5\n",
    "trainer.train(num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load('checkpoint_best.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded checkpoint from epoch {checkpoint['epoch']} with val_loss {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Test on a validation pair\n",
    "model.eval()\n",
    "val_sample = val_dataset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Add batch dimension\n",
    "    outputs1 = HOISDFOutputs(\n",
    "        **{k: v.unsqueeze(0).to(device) for k, v in val_sample['outputs1'].__dict__.items()}\n",
    "    )\n",
    "    outputs2 = HOISDFOutputs(\n",
    "        **{k: v.unsqueeze(0).to(device) for k, v in val_sample['outputs2'].__dict__.items()}\n",
    "    )\n",
    "    \n",
    "    # Generate transition\n",
    "    model_outputs = model(outputs1, outputs2, transition_length=30, mode='inference')\n",
    "    \n",
    "# Extract predicted transition\n",
    "pred_transition = model_outputs['transformer']['refined_mano'][0].cpu().numpy()\n",
    "gt_transition = val_sample['gt_transition'].mano_params.numpy()\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot X translation\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(gt_transition[:, 0], label='GT', color='green', linewidth=2)\n",
    "plt.plot(pred_transition[:, 0], label='Predicted', color='red', linestyle='--', linewidth=2)\n",
    "plt.title('X Translation')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Translation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Y translation\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(gt_transition[:, 1], label='GT', color='green', linewidth=2)\n",
    "plt.plot(pred_transition[:, 1], label='Predicted', color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Y Translation')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('Translation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot error\n",
    "plt.subplot(1, 3, 3)\n",
    "error = np.linalg.norm(pred_transition - gt_transition, axis=1)\n",
    "plt.plot(error, color='blue', linewidth=2)\n",
    "plt.title('Prediction Error')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('L2 Error')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean prediction error: {error.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Novel Transitions\n",
    "\n",
    "Use the trained model to generate transitions between any two sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novel_transition(model, seq1_path, seq2_path, device):\n",
    "    \"\"\"Generate transition between two arbitrary sequences.\"\"\"\n",
    "    \n",
    "    # Load sequences\n",
    "    with open(seq1_path, 'rb') as f:\n",
    "        seq1_data = pickle.load(f)\n",
    "    with open(seq2_path, 'rb') as f:\n",
    "        seq2_data = pickle.load(f)\n",
    "        \n",
    "    # Convert to HOISDFOutputs\n",
    "    outputs1 = HOISDFOutputs(\n",
    "        **{k: torch.tensor(v).unsqueeze(0).to(device) \n",
    "           for k, v in seq1_data.items() if k != 'sequence_type'}\n",
    "    )\n",
    "    outputs2 = HOISDFOutputs(\n",
    "        **{k: torch.tensor(v).unsqueeze(0).to(device) \n",
    "           for k, v in seq2_data.items() if k != 'sequence_type'}\n",
    "    )\n",
    "    \n",
    "    # Generate transition\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(outputs1, outputs2, transition_length=30, mode='inference')\n",
    "        \n",
    "    transition_mano = model_outputs['transformer']['refined_mano'][0].cpu().numpy()\n",
    "    \n",
    "    return transition_mano, seq1_data, seq2_data\n",
    "\n",
    "# Test with different sequence types\n",
    "seq1_path = data_dir / 'pick_phone_000.pkl'\n",
    "seq2_path = data_dir / 'typing_000.pkl'\n",
    "\n",
    "transition, seq1, seq2 = generate_novel_transition(model, seq1_path, seq2_path, device)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq1['mano_params'][-20:, 0], 'b-', alpha=0.5, label='Phone (end)')\n",
    "plt.plot(np.arange(20, 50), transition[:, 0], 'r-', linewidth=2, label='Transition')\n",
    "plt.plot(np.arange(50, 70), seq2['mano_params'][:20, 0], 'g-', alpha=0.5, label='Typing (start)')\n",
    "plt.xlabel('Frame')\n",
    "plt.ylabel('X Translation')\n",
    "plt.title('Novel Transition: Pick Phone → Typing')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}