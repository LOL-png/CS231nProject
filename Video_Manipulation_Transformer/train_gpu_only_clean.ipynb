{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video-to-Manipulation Transformer: GPU-Only Stage 1 Training\n",
    "\n",
    "This notebook implements the fastest GPU-only training approach for maximum performance on H200.\n",
    "\n",
    "**Key Features:**\n",
    "- Entire dataset cached in GPU memory (zero CPU-GPU transfers)\n",
    "- Large batch sizes (1024+)\n",
    "- BFloat16 mixed precision\n",
    "- Compiled models for better performance\n",
    "- No DataLoader overhead\n",
    "\n",
    "**Requirements:**\n",
    "- H200 GPU with 140GiB memory\n",
    "- PyTorch 2.0+ for torch.compile\n",
    "- CUDA 12.0+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA H200\n",
      "Memory: 150.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Set environment\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# CUDA optimizations for H200\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('.')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import our modules\n",
    "from models.encoders.hand_encoder import HandPoseEncoder\n",
    "from models.encoders.object_encoder import ObjectPoseEncoder\n",
    "from models.encoders.contact_encoder import ContactDetectionEncoder\n",
    "from data.gpu_preprocessing import GPUVideoPreprocessor\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Will cache 300000 training samples in GPU memory\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    # GPU-only dataset settings\n",
    "    'max_samples_train': 300000,  # Adjust based on GPU memory\n",
    "    'max_samples_val': 20000,\n",
    "    'batch_size': 1024,  # Large batch for H200\n",
    "    'image_size': (224, 224),\n",
    "    'patch_size': 16,\n",
    "    'cache_path': 'gpu_cache',\n",
    "    'dtype': torch.bfloat16,  # Use bfloat16 to fit more samples\n",
    "    \n",
    "    # Model settings - scale up for H200\n",
    "    'hand_hidden_dim': 2048,\n",
    "    'object_hidden_dim': 2048,\n",
    "    'contact_hidden_dim': 1024,\n",
    "    'hand_layers': 12,\n",
    "    'object_layers': 12,\n",
    "    'contact_layers': 8,\n",
    "    \n",
    "    # Training settings\n",
    "    'learning_rate': 2e-3,\n",
    "    'num_epochs': 5,\n",
    "    'grad_clip': 1.0,\n",
    "    'log_interval': 10,\n",
    "    'val_interval': 50\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"Will cache {config['max_samples_train']} training samples in GPU memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed GPU-Only Dataset Implementation\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "class GPUOnlyDataset:\n",
    "    \"\"\"Dataset that lives entirely on GPU memory\"\"\"\n",
    "    \n",
    "    def __init__(self, split='s0_train', max_samples=50000, image_size=(224, 224),\n",
    "                 device='cuda', dtype=torch.float32, cache_path=None):\n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        self.image_size = image_size\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.cache_path = cache_path\n",
    "        \n",
    "        # Check cache\n",
    "        cache_file = f\"{cache_path}/{split}_gpu_cache.pt\" if cache_path else None\n",
    "        if cache_path and os.path.exists(cache_file):\n",
    "            print(f\"Loading cached GPU dataset from {cache_file}...\")\n",
    "            self.data = torch.load(cache_file, map_location=device)\n",
    "            self.num_samples = len(self.data['color'])\n",
    "        else:\n",
    "            print(f\"Building GPU dataset for {split}...\")\n",
    "            self._build_dataset()\n",
    "            if cache_path:\n",
    "                os.makedirs(cache_path, exist_ok=True)\n",
    "                torch.save(self.data, cache_file)\n",
    "                print(f\"Saved cache to {cache_file}\")\n",
    "        \n",
    "        print(f\"✓ GPU dataset ready with {self.num_samples} samples\")\n",
    "        print(f\"  Memory usage: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n",
    "    \n",
    "    def _build_dataset(self):\n",
    "        \"\"\"Build dataset directly on GPU\"\"\"\n",
    "        from dex_ycb_toolkit.factory import get_dataset\n",
    "        dex_dataset = get_dataset(self.split)\n",
    "        \n",
    "        num_samples = min(len(dex_dataset), self.max_samples)\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Pre-allocate GPU tensors\n",
    "        print(f\"Allocating GPU memory for {num_samples} samples...\")\n",
    "        self.data = {\n",
    "            'color': torch.zeros((num_samples, 3, *self.image_size), \n",
    "                               device=self.device, dtype=self.dtype),\n",
    "            'hand_joints_3d': torch.full((num_samples, 21, 3), -1.0,\n",
    "                                       device=self.device, dtype=self.dtype),\n",
    "            'hand_joints_2d': torch.full((num_samples, 21, 2), -1.0,\n",
    "                                       device=self.device, dtype=self.dtype),\n",
    "            'hand_pose': torch.zeros((num_samples, 51),  # Fixed: MANO pose is 51D\n",
    "                                   device=self.device, dtype=self.dtype),\n",
    "            'object_poses': torch.zeros((num_samples, 10, 3, 4),\n",
    "                                      device=self.device, dtype=self.dtype),\n",
    "            'ycb_ids': torch.zeros((num_samples, 10),\n",
    "                                 device=self.device, dtype=torch.long),\n",
    "            'num_objects': torch.zeros((num_samples,),\n",
    "                                     device=self.device, dtype=torch.long),\n",
    "            'has_hand': torch.zeros((num_samples,), device=self.device, dtype=torch.bool),\n",
    "        }\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        for i in tqdm(range(num_samples), desc=\"Loading samples\"):\n",
    "            try:\n",
    "                sample = dex_dataset[i]\n",
    "                \n",
    "                # Load image\n",
    "                img = cv2.imread(sample['color_file'])\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, self.image_size)\n",
    "                img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "                self.data['color'][i] = img_tensor.to(self.device, dtype=self.dtype)\n",
    "                \n",
    "                # Load labels\n",
    "                labels = np.load(sample['label_file'])\n",
    "                \n",
    "                # Hand data\n",
    "                if 'joint_3d' in labels and labels['joint_3d'].shape[0] > 0:\n",
    "                    joints_3d = torch.from_numpy(labels['joint_3d'][0])\n",
    "                    self.data['hand_joints_3d'][i] = joints_3d.to(self.device, dtype=self.dtype)\n",
    "                    self.data['has_hand'][i] = True\n",
    "                \n",
    "                if 'joint_2d' in labels and labels['joint_2d'].shape[0] > 0:\n",
    "                    joints_2d = torch.from_numpy(labels['joint_2d'][0])\n",
    "                    self.data['hand_joints_2d'][i] = joints_2d.to(self.device, dtype=self.dtype)\n",
    "                \n",
    "                if 'pose_m' in labels and labels['pose_m'].shape[0] > 0:\n",
    "                    pose = torch.from_numpy(labels['pose_m'][0])\n",
    "                    # Handle different pose dimensions\n",
    "                    if pose.shape[0] == 48:\n",
    "                        # Pad to 51 if needed\n",
    "                        pose = F.pad(pose, (0, 3), value=0)\n",
    "                    self.data['hand_pose'][i, :pose.shape[0]] = pose.to(self.device, dtype=self.dtype)\n",
    "                \n",
    "                # Object data\n",
    "                if 'pose_y' in labels and len(labels['pose_y']) > 0:\n",
    "                    obj_poses = labels['pose_y']\n",
    "                    num_objs = min(len(obj_poses), 10)\n",
    "                    if num_objs > 0:\n",
    "                        obj_tensor = torch.from_numpy(obj_poses[:num_objs])\n",
    "                        self.data['object_poses'][i, :num_objs] = obj_tensor.to(self.device, dtype=self.dtype)\n",
    "                    self.data['num_objects'][i] = num_objs\n",
    "                \n",
    "                # YCB IDs\n",
    "                ycb_ids = sample.get('ycb_ids', [])\n",
    "                if ycb_ids:\n",
    "                    num_ids = min(len(ycb_ids), 10)\n",
    "                    self.data['ycb_ids'][i, :num_ids] = torch.tensor(ycb_ids[:num_ids], \n",
    "                                                                    device=self.device, dtype=torch.long)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading sample {i}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a sample - already on GPU!\"\"\"\n",
    "        return {\n",
    "            'color': self.data['color'][idx],\n",
    "            'hand_joints_3d': self.data['hand_joints_3d'][idx],\n",
    "            'hand_joints_2d': self.data['hand_joints_2d'][idx],\n",
    "            'hand_pose': self.data['hand_pose'][idx],\n",
    "            'object_poses': self.data['object_poses'][idx],\n",
    "            'ycb_ids': self.data['ycb_ids'][idx],\n",
    "            'num_objects': self.data['num_objects'][idx],\n",
    "            'has_hand': self.data['has_hand'][idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class GPUBatchGenerator:\n",
    "    \"\"\"Generate batches directly from GPU memory - zero copy\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=256, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = len(dataset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        # Create indices\n",
    "        indices = torch.arange(self.num_samples, device='cuda')\n",
    "        if self.shuffle:\n",
    "            indices = indices[torch.randperm(self.num_samples, device='cuda')]\n",
    "        \n",
    "        # Generate batches\n",
    "        for start_idx in range(0, self.num_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, self.num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            # Create batch - everything stays on GPU\n",
    "            batch = {}\n",
    "            for key in self.dataset.data:\n",
    "                if isinstance(self.dataset.data[key], torch.Tensor):\n",
    "                    batch[key] = self.dataset.data[key][batch_indices]\n",
    "            \n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GPU-only datasets...\n",
      "First run will be slow (loading), subsequent runs will use cache\n",
      "Building GPU dataset for s0_train...\n",
      "Allocating GPU memory for 300000 samples...\n",
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0057481ae01546a99fe5d225393c0c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading samples:   0%|          | 0/300000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache to gpu_cache/s0_train_gpu_cache.pt\n",
      "✓ GPU dataset ready with 300000 samples\n",
      "  Memory usage: 90.5 GB\n",
      "Building GPU dataset for s0_val...\n",
      "Allocating GPU memory for 20000 samples...\n",
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd868725f7d463e8af6deb0b961f877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading samples:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cache to gpu_cache/s0_val_gpu_cache.pt\n",
      "✓ GPU dataset ready with 20000 samples\n",
      "  Memory usage: 96.5 GB\n",
      "\n",
      "✓ Datasets ready:\n",
      "  Train: 300000 samples, 293 batches\n",
      "  Val: 20000 samples, 40 batches\n",
      "  GPU Memory: 96.5 GB\n"
     ]
    }
   ],
   "source": [
    "# Create GPU-only datasets\n",
    "print(\"Creating GPU-only datasets...\")\n",
    "print(\"First run will be slow (loading), subsequent runs will use cache\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "train_dataset = GPUOnlyDataset(\n",
    "    split='s0_train',\n",
    "    max_samples=config['max_samples_train'],\n",
    "    image_size=config['image_size'],\n",
    "    device='cuda',\n",
    "    dtype=config['dtype'],\n",
    "    cache_path=config['cache_path']\n",
    ")\n",
    "\n",
    "val_dataset = GPUOnlyDataset(\n",
    "    split='s0_val',\n",
    "    max_samples=config['max_samples_val'],\n",
    "    image_size=config['image_size'],\n",
    "    device='cuda',\n",
    "    dtype=config['dtype'],\n",
    "    cache_path=config['cache_path']\n",
    ")\n",
    "\n",
    "# Create batch generators\n",
    "train_loader = GPUBatchGenerator(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = GPUBatchGenerator(val_dataset, batch_size=config['batch_size']//2, shuffle=False)\n",
    "\n",
    "print(f\"\\n✓ Datasets ready:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_dataset)} samples, {len(val_loader)} batches\")\n",
    "print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scaled-up models for H200...\n",
      "Compiling models with torch.compile...\n",
      "\n",
      "Model parameters:\n",
      "  Hand: 612.4M\n",
      "  Object: 610.5M\n",
      "  Contact: 108.4M\n",
      "  Total: 1331.3M\n"
     ]
    }
   ],
   "source": [
    "# Create models\n",
    "print(\"Creating scaled-up models for H200...\")\n",
    "\n",
    "patch_dim = 3 * config['patch_size'] * config['patch_size']\n",
    "\n",
    "# Large models for H200\n",
    "hand_encoder = HandPoseEncoder(\n",
    "    input_dim=patch_dim,\n",
    "    hidden_dim=config['hand_hidden_dim'],\n",
    "    num_layers=config['hand_layers'],\n",
    "    num_heads=32,\n",
    "    mlp_dim=8192,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "object_encoder = ObjectPoseEncoder(\n",
    "    input_dim=patch_dim,\n",
    "    hidden_dim=config['object_hidden_dim'],\n",
    "    num_layers=config['object_layers'],\n",
    "    num_heads=32,\n",
    "    mlp_dim=8192,\n",
    "    dropout=0.1,\n",
    "    max_objects=10\n",
    ").to(device)\n",
    "\n",
    "contact_encoder = ContactDetectionEncoder(\n",
    "    input_dim=patch_dim,\n",
    "    hidden_dim=config['contact_hidden_dim'],\n",
    "    num_layers=config['contact_layers'],\n",
    "    num_heads=32,\n",
    "    mlp_dim=4096,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Compile models for better performance\n",
    "if hasattr(torch, 'compile'):\n",
    "    print(\"Compiling models with torch.compile...\")\n",
    "    hand_encoder = torch.compile(hand_encoder, mode='max-autotune')\n",
    "    object_encoder = torch.compile(object_encoder, mode='max-autotune')\n",
    "    contact_encoder = torch.compile(contact_encoder, mode='max-autotune')\n",
    "\n",
    "# Count parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_params(hand_encoder) + count_params(object_encoder) + count_params(contact_encoder)\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Hand: {count_params(hand_encoder)/1e6:.1f}M\")\n",
    "print(f\"  Object: {count_params(object_encoder)/1e6:.1f}M\")\n",
    "print(f\"  Contact: {count_params(contact_encoder)/1e6:.1f}M\")\n",
    "print(f\"  Total: {total_params/1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessor and optimizers ready\n"
     ]
    }
   ],
   "source": [
    "# Create GPU preprocessor and optimizers\n",
    "gpu_preprocessor = GPUVideoPreprocessor(\n",
    "    image_size=config['image_size'],\n",
    "    patch_size=config['patch_size'],\n",
    "    normalize=True,\n",
    "    device='cuda'\n",
    ").to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_hand = optim.AdamW(hand_encoder.parameters(), lr=config['learning_rate'])\n",
    "optimizer_object = optim.AdamW(object_encoder.parameters(), lr=config['learning_rate'])\n",
    "optimizer_contact = optim.AdamW(contact_encoder.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "print(\"✓ Preprocessor and optimizers ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(epoch):\n",
    "    \"\"\"GPU-only training - everything stays on GPU\"\"\"\n",
    "    hand_encoder.train()\n",
    "    object_encoder.train()\n",
    "    contact_encoder.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Create patches on GPU\n",
    "        with torch.no_grad():\n",
    "            patches = gpu_preprocessor(batch['color'])\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer_hand.zero_grad(set_to_none=True)\n",
    "        optimizer_object.zero_grad(set_to_none=True)\n",
    "        optimizer_contact.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward passes with autocast\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            # Hand encoder\n",
    "            hand_output = hand_encoder(patches)\n",
    "            hand_gt = batch['hand_joints_3d']\n",
    "            valid_hands = batch['has_hand']\n",
    "            \n",
    "            if valid_hands.any():\n",
    "                hand_loss = F.mse_loss(hand_output['joints_3d'][valid_hands], hand_gt[valid_hands])\n",
    "            else:\n",
    "                hand_loss = torch.tensor(0.0, device='cuda')\n",
    "            \n",
    "            # Object encoder\n",
    "            object_output = object_encoder(patches, object_ids=batch['ycb_ids'])\n",
    "            object_loss = torch.tensor(0.0, device='cuda')\n",
    "            \n",
    "            valid_objects = batch['num_objects'] > 0\n",
    "            if valid_objects.any():\n",
    "                # Get object positions\n",
    "                object_positions_gt = batch['object_poses'][:, :, :3, 3]\n",
    "                num_pred = min(object_output['positions'].shape[1], 10)\n",
    "                \n",
    "                # Compute loss for each sample with objects\n",
    "                for i in torch.where(valid_objects)[0]:\n",
    "                    n_obj = batch['num_objects'][i].item()\n",
    "                    if n_obj > 0 and n_obj <= num_pred:\n",
    "                        pred = object_output['positions'][i, :n_obj]\n",
    "                        gt = object_positions_gt[i, :n_obj]\n",
    "                        object_loss = object_loss + F.mse_loss(pred, gt)\n",
    "                \n",
    "                if valid_objects.sum() > 0:\n",
    "                    object_loss = object_loss / valid_objects.sum()\n",
    "            \n",
    "            # Contact encoder (no supervision)\n",
    "            contact_output = contact_encoder(\n",
    "                hand_output['features'].detach(),\n",
    "                object_output['features'].detach()\n",
    "            )\n",
    "            \n",
    "            # Total loss\n",
    "            total_batch_loss = hand_loss + object_loss\n",
    "        \n",
    "        # Backward\n",
    "        total_batch_loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(hand_encoder.parameters(), config['grad_clip'])\n",
    "        torch.nn.utils.clip_grad_norm_(object_encoder.parameters(), config['grad_clip'])\n",
    "        torch.nn.utils.clip_grad_norm_(contact_encoder.parameters(), config['grad_clip'])\n",
    "        \n",
    "        # Update\n",
    "        optimizer_hand.step()\n",
    "        optimizer_object.step()\n",
    "        optimizer_contact.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += total_batch_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % 5 == 0:\n",
    "            gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "            elapsed = time.time() - epoch_start\n",
    "            samples_per_sec = (batch_idx + 1) * config['batch_size'] / elapsed\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{total_batch_loss.item():.4f}',\n",
    "                'gpu': f'{gpu_mem:.1f}GB',\n",
    "                'speed': f'{samples_per_sec:.0f}/s'\n",
    "            })\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def validate():\n",
    "    \"\"\"Fast validation\"\"\"\n",
    "    hand_encoder.eval()\n",
    "    object_encoder.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_mpjpe = 0\n",
    "    num_valid = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            if batch_idx >= 10:  # Quick validation\n",
    "                break\n",
    "            \n",
    "            # Preprocess\n",
    "            patches = gpu_preprocessor(batch['color'])\n",
    "            \n",
    "            # Forward\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                hand_output = hand_encoder(patches)\n",
    "            \n",
    "            # Compute metrics\n",
    "            valid_hands = batch['has_hand']\n",
    "            if valid_hands.any():\n",
    "                hand_gt = batch['hand_joints_3d'][valid_hands]\n",
    "                hand_pred = hand_output['joints_3d'][valid_hands]\n",
    "                \n",
    "                loss = F.mse_loss(hand_pred, hand_gt)\n",
    "                mpjpe = (hand_pred - hand_gt).norm(dim=-1).mean()\n",
    "                \n",
    "                total_loss += loss.item() * valid_hands.sum().item()\n",
    "                total_mpjpe += mpjpe.item() * valid_hands.sum().item()\n",
    "                num_valid += valid_hands.sum().item()\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / max(num_valid, 1),\n",
    "        'mpjpe': total_mpjpe / max(num_valid, 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU monitoring utilities\n",
    "def print_gpu_stats():\n",
    "    \"\"\"Print current GPU statistics\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"GPU Statistics:\")\n",
    "        print(f\"  Memory: {allocated:.1f} / {total:.1f} GB ({allocated/total*100:.1f}%)\")\n",
    "        print(f\"  Reserved: {reserved:.1f} GB\")\n",
    "        \n",
    "        # Get utilization and power\n",
    "        try:\n",
    "            result = subprocess.run([\n",
    "                'nvidia-smi', '--query-gpu=utilization.gpu,power.draw,power.limit', \n",
    "                '--format=csv,noheader,nounits'\n",
    "            ], capture_output=True, text=True)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                gpu_util, power_draw, power_limit = result.stdout.strip().split(', ')\n",
    "                print(f\"  Utilization: {gpu_util}%\")\n",
    "                print(f\"  Power: {power_draw}W / {power_limit}W\")\n",
    "                \n",
    "                # Performance indicator\n",
    "                if float(gpu_util) > 80:\n",
    "                    print(\"  Status: ✓ Excellent GPU utilization\")\n",
    "                elif float(gpu_util) > 50:\n",
    "                    print(\"  Status: ⚠️ Moderate GPU utilization\")\n",
    "                else:\n",
    "                    print(\"  Status: ✗ Low GPU utilization\")\n",
    "        except:\n",
    "            pass\n",
    "        print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPU-Only Training...\n",
      "Configuration:\n",
      "  Batch size: 1024\n",
      "  Epochs: 5\n",
      "  Samples cached in GPU: 300000\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "GPU Statistics:\n",
      "  Memory: 101.9 / 150.0 GB (67.9%)\n",
      "  Reserved: 107.2 GB\n",
      "  Utilization: 0%\n",
      "  Power: 114.21W / 700.00W\n",
      "  Status: ✗ Low GPU utilization\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1fd6e3c6a3406cbab57d987533ce66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_126421/4028550876.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(device_type='cuda', dtype=torch.bfloat16):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "autocast.__init__() got an unexpected keyword argument 'device_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m epoch_start = time.time()\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(epoch)\u001b[39m\n\u001b[32m     22\u001b[39m optimizer_contact.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Forward passes with autocast\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Hand encoder\u001b[39;00m\n\u001b[32m     27\u001b[39m     hand_output = hand_encoder(patches)\n\u001b[32m     28\u001b[39m     hand_gt = batch[\u001b[33m'\u001b[39m\u001b[33mhand_joints_3d\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/env2.0/lib/python3.12/site-packages/typing_extensions.py:2853\u001b[39m, in \u001b[36mdeprecated.__call__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   2850\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(arg)\n\u001b[32m   2851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   2852\u001b[39m     warnings.warn(msg, category=category, stacklevel=stacklevel + \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2853\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: autocast.__init__() got an unexpected keyword argument 'device_type'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Starting GPU-Only Training...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n",
    "print(f\"  Samples cached in GPU: {config['max_samples_train']}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# History\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_mpjpe': [],\n",
    "    'throughput': [],\n",
    "    'gpu_util': []\n",
    "}\n",
    "\n",
    "# Initial GPU stats\n",
    "print_gpu_stats()\n",
    "\n",
    "# Training\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(config['num_epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(epoch)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_metrics = validate()\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_mpjpe'].append(val_metrics['mpjpe'])\n",
    "    \n",
    "    # Calculate throughput\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    samples_processed = len(train_loader) * config['batch_size']\n",
    "    throughput = samples_processed / epoch_time\n",
    "    history['throughput'].append(throughput)\n",
    "    \n",
    "    # Get GPU utilization\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', \n",
    "                               '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            gpu_util = float(result.stdout.strip())\n",
    "            history['gpu_util'].append(gpu_util)\n",
    "    except:\n",
    "        history['gpu_util'].append(0)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{config['num_epochs']} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Val MPJPE: {val_metrics['mpjpe']*1000:.2f} mm\")\n",
    "    print(f\"  Throughput: {throughput:.0f} samples/s\")\n",
    "    print(f\"  Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        print(\"  ✓ New best validation loss!\")\n",
    "    \n",
    "    print_gpu_stats()\n",
    "\n",
    "print(\"\\n✓ Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Average throughput: {np.mean(history['throughput']):.0f} samples/s\")\n",
    "print(f\"Average GPU utilization: {np.mean(history['gpu_util']):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# MPJPE\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot([x*1000 for x in history['val_mpjpe']], label='Val MPJPE', marker='o', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MPJPE (mm)')\n",
    "plt.title('Hand Pose Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Throughput\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(history['throughput'], label='Throughput', marker='o', color='orange')\n",
    "plt.axhline(y=np.mean(history['throughput']), color='red', linestyle='--', label='Average')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Samples/s')\n",
    "plt.title('Training Speed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# GPU Utilization\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(history['gpu_util'], label='GPU Util %', marker='o', color='purple')\n",
    "plt.axhline(y=80, color='green', linestyle='--', label='Target (80%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('GPU Utilization %')\n",
    "plt.title('GPU Usage')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gpu_only_training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"  Peak throughput: {max(history['throughput']):.0f} samples/s\")\n",
    "print(f\"  Peak GPU utilization: {max(history['gpu_util']):.1f}%\")\n",
    "print(f\"  Final MPJPE: {history['val_mpjpe'][-1]*1000:.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "checkpoint_dir = 'checkpoints/gpu_only'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save with all training info\n",
    "torch.save({\n",
    "    'model_state_dict': hand_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_hand.state_dict(),\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'best_val_loss': best_val_loss\n",
    "}, os.path.join(checkpoint_dir, 'hand_encoder.pth'))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': object_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_object.state_dict(),\n",
    "}, os.path.join(checkpoint_dir, 'object_encoder.pth'))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': contact_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_contact.state_dict(),\n",
    "}, os.path.join(checkpoint_dir, 'contact_encoder.pth'))\n",
    "\n",
    "print(f\"✓ Models saved to {checkpoint_dir}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for Maximum Performance\n",
    "\n",
    "### 1. Increase Cache Size\n",
    "```python\n",
    "# For H200 with 140GB:\n",
    "'max_samples_train': 100000,  # ~100GB with bfloat16\n",
    "'max_samples_train': 150000,  # Use most of GPU memory\n",
    "```\n",
    "\n",
    "### 2. Larger Batch Sizes\n",
    "```python\n",
    "'batch_size': 2048,  # Or even 4096\n",
    "```\n",
    "\n",
    "### 3. Monitor in Real-Time\n",
    "```bash\n",
    "# In another terminal:\n",
    "watch -n 0.5 nvidia-smi\n",
    "```\n",
    "\n",
    "### 4. Expected Performance\n",
    "- **GPU Utilization**: 85-95%\n",
    "- **Memory Usage**: 50-120GB\n",
    "- **Throughput**: 10,000+ samples/s\n",
    "- **Power Draw**: 500-700W\n",
    "\n",
    "### 5. Troubleshooting\n",
    "- If GPU util is low: Increase batch size\n",
    "- If OOM: Reduce cache size or use bfloat16\n",
    "- If slow: Check torch.compile is working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
