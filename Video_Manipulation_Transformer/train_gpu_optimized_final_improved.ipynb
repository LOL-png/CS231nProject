{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video-to-Manipulation Transformer: GPU-Optimized Training (Mode Collapse Fix)\n",
    "\n",
    "This notebook implements GPU-optimized training with fixes for mode collapse:\n",
    "- **Reduced regularization** to allow learning input-specific features\n",
    "- **Standard MSE loss** instead of Smooth L1 with label smoothing\n",
    "- **Cosine annealing LR** instead of OneCycleLR\n",
    "- **Diversity loss** to encourage varied predictions\n",
    "- **Better initialization** for output layers\n",
    "\n",
    "Key changes from previous version:\n",
    "- Dropout: 0.3 → 0.1\n",
    "- Stochastic depth: 0.2 → 0.0 (disabled)\n",
    "- Label smoothing: 0.2 → 0.0 (disabled)\n",
    "- MixUp: 0.4 → 0.0 (disabled initially)\n",
    "- Loss: Smooth L1 → MSE\n",
    "- Added diversity encouragement loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import json\n",
    "import cv2\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# Set environment\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# CUDA optimizations for H200\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('.')\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our modules\n",
    "from models.encoders.hand_encoder import HandPoseEncoder\n",
    "from models.encoders.object_encoder import ObjectPoseEncoder\n",
    "from models.encoders.contact_encoder import ContactDetectionEncoder\n",
    "from data.gpu_preprocessing import GPUVideoPreprocessor\n",
    "\n",
    "print(\"✓ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Positional Embeddings (Standard ViT Implementation)\n",
    "\n",
    "class PositionalEmbedding2D(nn.Module):\n",
    "    \"\"\"\n",
    "    2D positional embeddings for Vision Transformer\n",
    "    Standard implementation that adds learnable position embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches: int = 196, hidden_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        # 14x14 = 196 patches for 224x224 image with 16x16 patches\n",
    "        self.num_patches = num_patches\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        self.num_patches_per_dim = int(num_patches ** 0.5)  # 14 for 196 patches\n",
    "        \n",
    "        # Standard learnable positional embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_dim))\n",
    "        \n",
    "        # Initialize with 2D sine-cosine positional encoding\n",
    "        pos_embed = self.get_2d_sincos_pos_embed(hidden_dim, self.num_patches_per_dim)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "    def get_2d_sincos_pos_embed(self, embed_dim, grid_size):\n",
    "        \"\"\"\n",
    "        Generate 2D sine-cosine positional embeddings\n",
    "        Standard method from MAE (Masked Autoencoders)\n",
    "        \"\"\"\n",
    "        grid_h = np.arange(grid_size, dtype=np.float32)\n",
    "        grid_w = np.arange(grid_size, dtype=np.float32)\n",
    "        grid = np.meshgrid(grid_w, grid_h)  # w goes first\n",
    "        grid = np.stack(grid, axis=0)\n",
    "        grid = grid.reshape([2, 1, grid_size, grid_size])\n",
    "        \n",
    "        # Generate positional embeddings\n",
    "        pos_embed = self.get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "        return pos_embed\n",
    "    \n",
    "    def get_2d_sincos_pos_embed_from_grid(self, embed_dim, grid):\n",
    "        assert embed_dim % 2 == 0\n",
    "        \n",
    "        # Use half of dimensions for each axis\n",
    "        emb_h = self.get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "        emb_w = self.get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "        \n",
    "        emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "        return emb\n",
    "    \n",
    "    def get_1d_sincos_pos_embed_from_grid(self, embed_dim, pos):\n",
    "        assert embed_dim % 2 == 0\n",
    "        omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "        omega /= embed_dim / 2.\n",
    "        omega = 1. / 10000**omega  # (D/2,)\n",
    "        \n",
    "        pos = pos.reshape(-1)  # (M,)\n",
    "        out = np.einsum('m,d->md', pos, omega)  # (M, D/2)\n",
    "        \n",
    "        emb_sin = np.sin(out)  # (M, D/2)\n",
    "        emb_cos = np.cos(out)  # (M, D/2)\n",
    "        \n",
    "        emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "        return emb\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Simply return the positional embeddings\"\"\"\n",
    "        return self.pos_embed\n",
    "\n",
    "\n",
    "# Alternative: Simple learnable embeddings (easier but less generalizable)\n",
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    \"\"\"Simple learnable positional embeddings\"\"\"\n",
    "    def __init__(self, num_patches: int = 196, hidden_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_dim))\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pos_embed\n",
    "\n",
    "\n",
    "print(\"✓ Fixed positional embedding implementation\")\n",
    "print(\"  - Standard 2D sine-cosine initialization (like MAE)\")\n",
    "print(\"  - Returns embeddings directly (not added to input)\")\n",
    "print(\"  - Encoders will handle the addition internally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional fixes for mode collapse and convergence issues\n",
    "print(\"📋 Summary of Critical Fixes Applied:\")\n",
    "print(\"\\n1. ✅ POSITIONAL EMBEDDINGS (was completely missing!)\")\n",
    "print(\"   - Without these, model had no spatial awareness\")\n",
    "print(\"   - Patches were just a bag of features with no location info\")\n",
    "print(\"   - Now using 2D positional embeddings that understand image grid\")\n",
    "\n",
    "print(\"\\n2. 🎯 Better Loss Functions:\")\n",
    "print(\"   - Smooth L1 instead of MSE (more robust to outliers)\")\n",
    "print(\"   - Diversity loss to prevent constant predictions\")\n",
    "print(\"   - Gradient norm monitoring to detect training issues\")\n",
    "\n",
    "print(\"\\n3. 🔧 Initialization Improvements:\")\n",
    "print(\"   - Xavier/Glorot init with higher variance\")\n",
    "print(\"   - Output layer initialized with random offsets\")\n",
    "print(\"   - Prevents all joints collapsing to same point\")\n",
    "\n",
    "print(\"\\n4. 📊 Additional Recommendations if still having issues:\")\n",
    "\n",
    "# More aggressive fixes if needed\n",
    "additional_config_fixes = {\n",
    "    # 1. Increase diversity loss weight\n",
    "    'diversity_weight': 0.1,  # was 0.01\n",
    "    \n",
    "    # 2. Add noise to features during training\n",
    "    'feature_noise_std': 0.1,  # Add Gaussian noise to encoder features\n",
    "    \n",
    "    # 3. Use learnable temperature scaling\n",
    "    'temperature_init': 1.0,  # Scale logits before output\n",
    "    \n",
    "    # 4. Auxiliary losses\n",
    "    'use_auxiliary_losses': True,\n",
    "    'auxiliary_weight': 0.1,\n",
    "    \n",
    "    # 5. Different optimizer for positional embeddings\n",
    "    'pos_embed_lr': 1e-3,  # Same as main LR\n",
    "    \n",
    "    # 6. Warmup for diversity loss\n",
    "    'diversity_warmup_epochs': 5,  # Gradually increase diversity weight\n",
    "}\n",
    "\n",
    "print(\"\\nIf model still converges to ~312mm MPJPE:\")\n",
    "print(\"1. Increase diversity_weight to 0.1 or even 0.5\")\n",
    "print(\"2. Add feature noise during training (dropout in feature space)\")\n",
    "print(\"3. Use auxiliary prediction tasks (e.g., predict hand center first)\")\n",
    "print(\"4. Check if ground truth has bias around 312mm\")\n",
    "print(\"5. Visualize actual predictions vs ground truth\")\n",
    "\n",
    "# Function to add feature noise (if needed)\n",
    "class FeatureNoise(nn.Module):\n",
    "    \"\"\"Add noise to features to prevent mode collapse\"\"\"\n",
    "    def __init__(self, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training and self.noise_std > 0:\n",
    "            noise = torch.randn_like(x) * self.noise_std\n",
    "            return x + noise\n",
    "        return x\n",
    "\n",
    "print(\"\\n💡 Most likely issue was missing positional embeddings!\")\n",
    "print(\"   This should dramatically improve results in next training run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Correct Vision Transformer Positional Embeddings\n",
    "\n",
    "print(\"📝 POSITIONAL EMBEDDINGS - WHAT WAS WRONG AND HOW IT'S FIXED:\\n\")\n",
    "\n",
    "print(\"❌ WRONG (My initial implementation):\")\n",
    "print(\"1. Projected patches OUTSIDE encoder: hand_patches = encoder.input_projection(patches)\")\n",
    "print(\"2. Added pos embeddings to projected patches: hand_patches_with_pos = pos_embed(hand_patches)\")\n",
    "print(\"3. Passed difference as spatial_encoding: encoder(patches, spatial_encoding=diff)\")\n",
    "print(\"4. Encoder projected patches AGAIN internally\")\n",
    "print(\"   → Double projection, wrong dimensions, unnecessary complexity!\\n\")\n",
    "\n",
    "print(\"✅ CORRECT (Standard ViT approach):\")\n",
    "print(\"1. Pass raw patches to encoder: encoder(patches)\")\n",
    "print(\"2. Pass positional embeddings separately: encoder(patches, spatial_encoding=pos_embed)\")\n",
    "print(\"3. Encoder internally does: x = projection(patches) + spatial_encoding\")\n",
    "print(\"   → Single projection, correct dimensions, standard approach!\\n\")\n",
    "\n",
    "print(\"🎯 KEY INSIGHT:\")\n",
    "print(\"- Vision Transformers need positional info because self-attention is permutation-invariant\")\n",
    "print(\"- Without positions, patches are just an unordered set\")\n",
    "print(\"- The encoder expects raw patches and handles projection+addition internally\")\n",
    "print(\"- This is why your model collapsed to constant predictions - no spatial awareness!\\n\")\n",
    "\n",
    "# Visual representation of the fix\n",
    "print(\"FLOW DIAGRAM:\")\n",
    "print(\"Before:  patches → project → add_pos → subtract → encoder(patches, diff) → project_again ❌\")\n",
    "print(\"After:   patches → encoder(patches, pos_embeddings) → project+add internally ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with reduced regularization to fix mode collapse\n",
    "config = {\n",
    "    # Dataset settings\n",
    "    'max_samples_train': 100000,\n",
    "    'max_samples_val': 10000,\n",
    "    'batch_size': 128,\n",
    "    'image_size': (224, 224),\n",
    "    'patch_size': 16,\n",
    "    'cache_path': 'gpu_cache_optimized',\n",
    "    'dtype': torch.bfloat16,\n",
    "    'use_gpu_decode': False,\n",
    "    \n",
    "    # Model settings - REDUCED REGULARIZATION\n",
    "    'hand_hidden_dim': 1024,\n",
    "    'object_hidden_dim': 1024,\n",
    "    'contact_hidden_dim': 512,\n",
    "    'hand_layers': 8,\n",
    "    'object_layers': 8,\n",
    "    'contact_layers': 6,\n",
    "    'num_heads': 16,\n",
    "    'dropout': 0.1,  # REDUCED from 0.3\n",
    "    'drop_path': 0.0,  # DISABLED (was 0.2)\n",
    "    \n",
    "    # Training settings - SIMPLIFIED\n",
    "    'learning_rate': 1e-3,  # Standard LR\n",
    "    'weight_decay': 0.01,  # Standard weight decay\n",
    "    'num_epochs': 20,\n",
    "    'grad_clip': 1.0,  # Standard gradient clipping\n",
    "    \n",
    "    # Augmentation settings - DISABLED FOR NOW\n",
    "    'mixup_alpha': 0.0,  # DISABLED (was 0.4)\n",
    "    'label_smoothing': 0.0,  # DISABLED (was 0.2)\n",
    "    'joint_noise_std': 0.005,  # Minimal noise (was 0.02)\n",
    "    'rotation_range': 5,  # Minimal rotation (was 30)\n",
    "    'color_jitter': 0.1,  # Reduced (was 0.3)\n",
    "    'random_erase_prob': 0.0,  # DISABLED\n",
    "    \n",
    "    # Anti-collapse measures - NEW\n",
    "    'diversity_weight': 0.01,  # Weight for diversity loss\n",
    "    'init_std': 0.1,  # Larger initialization (was 0.01)\n",
    "    \n",
    "    # Loss weights\n",
    "    'joint_weight_power': 1.1,  # Reduced from 1.2\n",
    "    'velocity_loss_weight': 0.05,  # Reduced from 0.1\n",
    "    \n",
    "    # Logging\n",
    "    'log_interval': 20,\n",
    "    'val_interval': 100,\n",
    "    'save_interval': 1000\n",
    "}\n",
    "\n",
    "print(\"Configuration updated to fix mode collapse:\")\n",
    "print(f\"  ✓ Dropout: {config['dropout']} (was 0.3)\")\n",
    "print(f\"  ✓ Stochastic depth: {config['drop_path']} (was 0.2)\")\n",
    "print(f\"  ✓ Label smoothing: {config['label_smoothing']} (was 0.2)\")\n",
    "print(f\"  ✓ MixUp: {config['mixup_alpha']} (was 0.4)\")\n",
    "print(f\"  ✓ Diversity weight: {config['diversity_weight']} (new)\")\n",
    "print(f\"  ✓ Init std: {config['init_std']} (was 0.01)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation utilities\n",
    "class HandPoseAugmentation:\n",
    "    \"\"\"Augmentation for hand pose data\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.joint_noise_std = config['joint_noise_std']\n",
    "        self.rotation_range = config['rotation_range']\n",
    "        \n",
    "    def __call__(self, joints_3d, image):\n",
    "        # Add Gaussian noise to joints\n",
    "        if self.joint_noise_std > 0:\n",
    "            noise = torch.randn_like(joints_3d) * self.joint_noise_std\n",
    "            joints_3d = joints_3d + noise\n",
    "        \n",
    "        # Random rotation (simplified - just rotate around Y axis)\n",
    "        if self.rotation_range > 0:\n",
    "            angle = (torch.rand(1).item() - 0.5) * 2 * self.rotation_range\n",
    "            angle_rad = np.radians(angle)\n",
    "            cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
    "            \n",
    "            # Rotation matrix around Y axis\n",
    "            rot_matrix = torch.tensor([\n",
    "                [cos_a, 0, sin_a],\n",
    "                [0, 1, 0],\n",
    "                [-sin_a, 0, cos_a]\n",
    "            ], device=joints_3d.device, dtype=joints_3d.dtype)\n",
    "            \n",
    "            # Apply rotation\n",
    "            joints_3d = torch.matmul(joints_3d, rot_matrix.T)\n",
    "        \n",
    "        # Color jittering for images\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            # Random brightness\n",
    "            brightness = 1 + (torch.rand(1).item() - 0.5) * 0.4\n",
    "            image = torch.clamp(image * brightness, 0, 1)\n",
    "            \n",
    "        return joints_3d, image\n",
    "\n",
    "\n",
    "class MixUp:\n",
    "    \"\"\"MixUp augmentation\"\"\"\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, images, labels):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "            \n",
    "        batch_size = images.size(0)\n",
    "        index = torch.randperm(batch_size).to(images.device)\n",
    "        \n",
    "        mixed_images = lam * images + (1 - lam) * images[index]\n",
    "        return mixed_images, labels, labels[index], lam\n",
    "\n",
    "\n",
    "# Initialize augmentation\n",
    "augmentation = HandPoseAugmentation(config)\n",
    "mixup = MixUp(alpha=config['mixup_alpha'])\n",
    "print(\"✓ Data augmentation initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified loss functions - using standard MSE\n",
    "def get_joint_weights(num_joints=21, power=1.1):\n",
    "    \"\"\"Get per-joint weights (fingertips weighted slightly more)\"\"\"\n",
    "    weights = torch.ones(num_joints)\n",
    "    \n",
    "    # Finger joint indices (assuming standard MANO ordering)\n",
    "    finger_starts = [1, 5, 9, 13, 17]  # Thumb, index, middle, ring, pinky\n",
    "    \n",
    "    for start in finger_starts:\n",
    "        for i in range(4):  # 4 joints per finger\n",
    "            if start + i < num_joints:\n",
    "                # Weight increases towards fingertip (reduced from before)\n",
    "                weights[start + i] = 1.0 + (i / 3) * (power - 1.0)\n",
    "    \n",
    "    return weights.to(device)\n",
    "\n",
    "\n",
    "# Diversity loss to prevent mode collapse\n",
    "def diversity_loss(predictions):\n",
    "    \"\"\"Encourage diverse predictions across the batch\"\"\"\n",
    "    # Calculate standard deviation across batch for each joint coordinate\n",
    "    batch_std = predictions.std(dim=0)  # [21, 3]\n",
    "    \n",
    "    # We want to maximize std, so we minimize negative log std\n",
    "    # Add small epsilon to prevent log(0)\n",
    "    diversity = -torch.log(batch_std.mean() + 1e-6)\n",
    "    \n",
    "    return diversity\n",
    "\n",
    "\n",
    "# Initialize loss components\n",
    "joint_weights = get_joint_weights(power=config['joint_weight_power'])\n",
    "print(f\"✓ Loss functions initialized (using MSE)\")\n",
    "print(f\"  Joint weights range: {joint_weights.min():.2f} - {joint_weights.max():.2f}\")\n",
    "print(f\"  Diversity loss weight: {config['diversity_weight']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stochastic Depth since we disabled it\n",
    "print(\"✓ Skipping Stochastic Depth implementation (disabled in config)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-Only Dataset (same as before but with augmentation support)\n",
    "class GPUOnlyDataset:\n",
    "    \"\"\"Dataset that lives entirely on GPU memory\"\"\"\n",
    "    \n",
    "    def __init__(self, split='s0_train', max_samples=50000, image_size=(224, 224),\n",
    "                 device='cuda', dtype=torch.float32, cache_path=None, use_gpu_decode=False):\n",
    "        self.split = split\n",
    "        self.max_samples = max_samples\n",
    "        self.image_size = image_size\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.cache_path = cache_path\n",
    "        self.use_gpu_decode = use_gpu_decode\n",
    "        \n",
    "        # Check cache\n",
    "        cache_file = f\"{cache_path}/{split}_gpu_cache_optimized.pt\" if cache_path else None\n",
    "        if cache_path and os.path.exists(cache_file):\n",
    "            print(f\"Loading cached GPU dataset from {cache_file}...\")\n",
    "            self.data = torch.load(cache_file, map_location=device, weights_only=False)\n",
    "            self.num_samples = len(self.data['color'])\n",
    "        else:\n",
    "            print(f\"Building GPU dataset for {split}...\")\n",
    "            self._build_dataset_cpu_decode()\n",
    "            \n",
    "            if cache_path:\n",
    "                os.makedirs(cache_path, exist_ok=True)\n",
    "                torch.save(self.data, cache_file)\n",
    "                print(f\"Saved cache to {cache_file}\")\n",
    "        \n",
    "        print(f\"✓ GPU dataset ready with {self.num_samples} samples\")\n",
    "        print(f\"  Memory usage: {torch.cuda.memory_allocated()/1e9:.1f} GB\")\n",
    "    \n",
    "    def _build_dataset_cpu_decode(self):\n",
    "        \"\"\"Original CPU-based loading\"\"\"\n",
    "        from dex_ycb_toolkit.factory import get_dataset\n",
    "        dex_dataset = get_dataset(self.split)\n",
    "        \n",
    "        num_samples = min(len(dex_dataset), self.max_samples)\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Pre-allocate GPU tensors\n",
    "        print(f\"Allocating GPU memory for {num_samples} samples...\")\n",
    "        self.data = {\n",
    "            'color': torch.zeros((num_samples, 3, *self.image_size), \n",
    "                               device=self.device, dtype=self.dtype),\n",
    "            'hand_joints_3d': torch.full((num_samples, 21, 3), -1.0,\n",
    "                                       device=self.device, dtype=self.dtype),\n",
    "            'hand_joints_2d': torch.full((num_samples, 21, 2), -1.0,\n",
    "                                       device=self.device, dtype=self.dtype),\n",
    "            'hand_pose': torch.zeros((num_samples, 51),\n",
    "                                   device=self.device, dtype=self.dtype),\n",
    "            'object_poses': torch.zeros((num_samples, 10, 3, 4),\n",
    "                                      device=self.device, dtype=self.dtype),\n",
    "            'ycb_ids': torch.zeros((num_samples, 10),\n",
    "                                 device=self.device, dtype=torch.long),\n",
    "            'num_objects': torch.zeros((num_samples,),\n",
    "                                     device=self.device, dtype=torch.long),\n",
    "            'has_hand': torch.zeros((num_samples,), device=self.device, dtype=torch.bool),\n",
    "        }\n",
    "        \n",
    "        # Load data with progress bar\n",
    "        print(\"Loading and preprocessing data...\")\n",
    "        for i in tqdm(range(num_samples), desc=\"Loading samples\"):\n",
    "            try:\n",
    "                sample = dex_dataset[i]\n",
    "                \n",
    "                # Load and preprocess image\n",
    "                img = cv2.imread(sample['color_file'])\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img = cv2.resize(img, self.image_size)\n",
    "                img_tensor = torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
    "                self.data['color'][i] = img_tensor.to(self.device, dtype=self.dtype)\n",
    "                \n",
    "                # Load labels\n",
    "                labels = np.load(sample['label_file'])\n",
    "                \n",
    "                # Hand data\n",
    "                if 'joint_3d' in labels and labels['joint_3d'].shape[0] > 0:\n",
    "                    joints_3d = torch.from_numpy(labels['joint_3d'][0])\n",
    "                    self.data['hand_joints_3d'][i] = joints_3d.to(self.device, dtype=self.dtype)\n",
    "                    self.data['has_hand'][i] = True\n",
    "                \n",
    "                if 'joint_2d' in labels and labels['joint_2d'].shape[0] > 0:\n",
    "                    joints_2d = torch.from_numpy(labels['joint_2d'][0])\n",
    "                    self.data['hand_joints_2d'][i] = joints_2d.to(self.device, dtype=self.dtype)\n",
    "                \n",
    "                if 'pose_m' in labels and labels['pose_m'].shape[0] > 0:\n",
    "                    pose = torch.from_numpy(labels['pose_m'][0])\n",
    "                    if pose.shape[0] == 48:\n",
    "                        pose = F.pad(pose, (0, 3), value=0)\n",
    "                    elif pose.shape[0] > 51:\n",
    "                        pose = pose[:51]\n",
    "                    self.data['hand_pose'][i, :pose.shape[0]] = pose.to(self.device, dtype=self.dtype)\n",
    "                \n",
    "                # Object data\n",
    "                if 'pose_y' in labels and len(labels['pose_y']) > 0:\n",
    "                    obj_poses = labels['pose_y']\n",
    "                    num_objs = min(len(obj_poses), 10)\n",
    "                    if num_objs > 0:\n",
    "                        obj_tensor = torch.from_numpy(obj_poses[:num_objs])\n",
    "                        self.data['object_poses'][i, :num_objs] = obj_tensor.to(self.device, dtype=self.dtype)\n",
    "                    self.data['num_objects'][i] = num_objs\n",
    "                \n",
    "                # YCB IDs\n",
    "                ycb_ids = sample.get('ycb_ids', [])\n",
    "                if ycb_ids:\n",
    "                    num_ids = min(len(ycb_ids), 10)\n",
    "                    self.data['ycb_ids'][i, :num_ids] = torch.tensor(ycb_ids[:num_ids], \n",
    "                                                                    device=self.device, dtype=torch.long)\n",
    "                                                                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError loading sample {i}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a sample - already on GPU!\"\"\"\n",
    "        return {\n",
    "            'color': self.data['color'][idx],\n",
    "            'hand_joints_3d': self.data['hand_joints_3d'][idx],\n",
    "            'hand_joints_2d': self.data['hand_joints_2d'][idx],\n",
    "            'hand_pose': self.data['hand_pose'][idx],\n",
    "            'object_poses': self.data['object_poses'][idx],\n",
    "            'ycb_ids': self.data['ycb_ids'][idx],\n",
    "            'num_objects': self.data['num_objects'][idx],\n",
    "            'has_hand': self.data['has_hand'][idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class GPUBatchGenerator:\n",
    "    \"\"\"Zero-copy batch generator with augmentation support\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=256, shuffle=True, augmentation=None):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = len(dataset)\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        indices = torch.arange(self.num_samples, device='cuda')\n",
    "        if self.shuffle:\n",
    "            indices = indices[torch.randperm(self.num_samples, device='cuda')]\n",
    "        \n",
    "        for start_idx in range(0, self.num_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, self.num_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            \n",
    "            batch = {}\n",
    "            for key in self.dataset.data:\n",
    "                if isinstance(self.dataset.data[key], torch.Tensor):\n",
    "                    batch[key] = self.dataset.data[key][batch_indices]\n",
    "            \n",
    "            # Apply augmentation if training\n",
    "            if self.augmentation is not None and self.shuffle:  # Only augment during training\n",
    "                for i in range(len(batch_indices)):\n",
    "                    if batch['has_hand'][i]:\n",
    "                        batch['hand_joints_3d'][i], batch['color'][i] = \\\n",
    "                            self.augmentation(batch['hand_joints_3d'][i], batch['color'][i])\n",
    "            \n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU-only datasets\n",
    "print(\"Creating GPU-only datasets with augmentation...\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Training dataset\n",
    "train_dataset = GPUOnlyDataset(\n",
    "    split='s0_train',\n",
    "    max_samples=config['max_samples_train'],\n",
    "    image_size=config['image_size'],\n",
    "    device='cuda',\n",
    "    dtype=config['dtype'],\n",
    "    cache_path=config['cache_path'],\n",
    "    use_gpu_decode=config['use_gpu_decode']\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val_dataset = GPUOnlyDataset(\n",
    "    split='s0_val',\n",
    "    max_samples=config['max_samples_val'],\n",
    "    image_size=config['image_size'],\n",
    "    device='cuda',\n",
    "    dtype=config['dtype'],\n",
    "    cache_path=config['cache_path'],\n",
    "    use_gpu_decode=config['use_gpu_decode']\n",
    ")\n",
    "\n",
    "# Create batch generators with augmentation\n",
    "train_loader = GPUBatchGenerator(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True,\n",
    "    augmentation=augmentation  # Enable augmentation for training\n",
    ")\n",
    "val_loader = GPUBatchGenerator(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size']//2, \n",
    "    shuffle=False,\n",
    "    augmentation=None  # No augmentation for validation\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Datasets ready:\")\n",
    "print(f\"  Train: {len(train_dataset):,} samples, {len(train_loader)} batches\")\n",
    "print(f\"  Val: {len(val_dataset):,} samples, {len(val_loader)} batches\")\n",
    "print(f\"  GPU Memory: {torch.cuda.memory_allocated()/1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models with CORRECT positional embeddings\n",
    "print(\"Creating models with STANDARD ViT positional embeddings...\\n\")\n",
    "\n",
    "patch_dim = 3 * config['patch_size'] * config['patch_size']\n",
    "num_patches = (config['image_size'][0] // config['patch_size']) * (config['image_size'][1] // config['patch_size'])\n",
    "\n",
    "# Create positional embeddings for each encoder\n",
    "hand_pos_embed = PositionalEmbedding2D(\n",
    "    num_patches=num_patches,\n",
    "    hidden_dim=config['hand_hidden_dim']\n",
    ").to(device)\n",
    "\n",
    "object_pos_embed = PositionalEmbedding2D(\n",
    "    num_patches=num_patches,\n",
    "    hidden_dim=config['object_hidden_dim']\n",
    ").to(device)\n",
    "\n",
    "# Note: Contact encoder doesn't need positional embeddings \n",
    "# because it works on features, not patches\n",
    "\n",
    "print(f\"✓ Created 2D positional embeddings for {num_patches} patches (14x14 grid)\")\n",
    "\n",
    "# Hand pose encoder\n",
    "hand_encoder = HandPoseEncoder(\n",
    "    input_dim=patch_dim,\n",
    "    hidden_dim=config['hand_hidden_dim'],\n",
    "    num_layers=config['hand_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_dim=4096,\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Object pose encoder\n",
    "object_encoder = ObjectPoseEncoder(\n",
    "    input_dim=patch_dim,\n",
    "    hidden_dim=config['object_hidden_dim'],\n",
    "    num_layers=config['object_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_dim=4096,\n",
    "    dropout=config['dropout'],\n",
    "    max_objects=10\n",
    ").to(device)\n",
    "\n",
    "# Contact detection encoder\n",
    "contact_encoder = ContactDetectionEncoder(\n",
    "    input_dim=patch_dim,\n",
    "    hidden_dim=config['contact_hidden_dim'],\n",
    "    num_layers=config['contact_layers'],\n",
    "    num_heads=config['num_heads'],\n",
    "    mlp_dim=2048,\n",
    "    dropout=config['dropout']\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Models created\")\n",
    "\n",
    "# Better initialization for diversity\n",
    "def initialize_encoder_with_diversity(encoder, name, std=0.02):\n",
    "    \"\"\"Initialize encoder to encourage diverse outputs\"\"\"\n",
    "    for param_name, param in encoder.named_parameters():\n",
    "        if 'weight' in param_name:\n",
    "            if 'norm' in param_name:\n",
    "                nn.init.constant_(param, 1.0)\n",
    "            elif len(param.shape) >= 2:\n",
    "                # Xavier/Glorot initialization\n",
    "                nn.init.xavier_uniform_(param)\n",
    "        elif 'bias' in param_name:\n",
    "            nn.init.constant_(param, 0.0)\n",
    "    \n",
    "    # Special initialization for output layers\n",
    "    if hasattr(encoder, 'joint_head'):  # Hand encoder\n",
    "        for module in encoder.joint_head.modules():\n",
    "            if isinstance(module, nn.Linear) and module.out_features == 63:\n",
    "                nn.init.xavier_normal_(module.weight, gain=2.0)  # Higher gain for output\n",
    "                # Initialize bias with small random values\n",
    "                nn.init.uniform_(module.bias, -0.1, 0.1)\n",
    "                # Add slight forward bias (positive Z)\n",
    "                with torch.no_grad():\n",
    "                    module.bias.view(21, 3)[:, 2] += 0.1\n",
    "    \n",
    "    print(f\"✓ Initialized {name} with Xavier initialization\")\n",
    "\n",
    "# Apply initialization\n",
    "initialize_encoder_with_diversity(hand_encoder, \"hand_encoder\")\n",
    "initialize_encoder_with_diversity(object_encoder, \"object_encoder\")\n",
    "initialize_encoder_with_diversity(contact_encoder, \"contact_encoder\")\n",
    "\n",
    "# Count parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = (count_params(hand_encoder) + count_params(object_encoder) + \n",
    "                count_params(contact_encoder) + count_params(hand_pos_embed) + \n",
    "                count_params(object_pos_embed))\n",
    "\n",
    "print(\"\\nModel parameters:\")\n",
    "print(f\"  Hand encoder: {count_params(hand_encoder)/1e6:.1f}M\")\n",
    "print(f\"  Object encoder: {count_params(object_encoder)/1e6:.1f}M\")\n",
    "print(f\"  Contact encoder: {count_params(contact_encoder)/1e6:.1f}M\")\n",
    "print(f\"  Positional embeddings: {(count_params(hand_pos_embed) + count_params(object_pos_embed))/1e6:.1f}M\")\n",
    "print(f\"  Total: {total_params/1e6:.1f}M\")\n",
    "\n",
    "print(f\"\\n✅ KEY FIXES APPLIED:\")\n",
    "print(f\"  1. Standard ViT positional embeddings (no dimension changes)\")\n",
    "print(f\"  2. Encoders handle projection + addition internally\")\n",
    "print(f\"  3. Xavier initialization for better gradient flow\")\n",
    "print(f\"  4. No double projection or complex operations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizers and schedulers\n",
    "gpu_preprocessor = GPUVideoPreprocessor(\n",
    "    image_size=config['image_size'],\n",
    "    patch_size=config['patch_size'],\n",
    "    normalize=True,\n",
    "    device='cuda'\n",
    ").to(device)\n",
    "\n",
    "# Create parameter groups for different learning rates if needed\n",
    "hand_params = [\n",
    "    {'params': hand_encoder.parameters(), 'lr': config['learning_rate']},\n",
    "    {'params': hand_pos_embed.parameters(), 'lr': config['learning_rate']}\n",
    "]\n",
    "\n",
    "object_params = [\n",
    "    {'params': object_encoder.parameters(), 'lr': config['learning_rate']},\n",
    "    {'params': object_pos_embed.parameters(), 'lr': config['learning_rate']}\n",
    "]\n",
    "\n",
    "# Optimizers with parameter groups\n",
    "optimizer_hand = optim.AdamW(\n",
    "    hand_params,\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "optimizer_object = optim.AdamW(\n",
    "    object_params,\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "optimizer_contact = optim.AdamW(\n",
    "    contact_encoder.parameters(), \n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# CosineAnnealingLR schedulers for stable training\n",
    "scheduler_hand = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_hand,\n",
    "    T_max=config['num_epochs'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "scheduler_object = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_object,\n",
    "    T_max=config['num_epochs'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "scheduler_contact = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_contact,\n",
    "    T_max=config['num_epochs'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(\"✓ Optimizers and schedulers ready\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Weight decay: {config['weight_decay']}\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR (min_lr=1e-6)\")\n",
    "print(f\"  Total epochs: {config['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED Training function with CORRECT positional embeddings usage\n",
    "def train_epoch(epoch):\n",
    "    \"\"\"Training with standard ViT positional embeddings\"\"\"\n",
    "    hand_encoder.train()\n",
    "    object_encoder.train()\n",
    "    contact_encoder.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_hand_loss = 0\n",
    "    total_object_loss = 0\n",
    "    total_diversity_loss = 0\n",
    "    num_batches = 0\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Diagnostic tracking\n",
    "    pred_stds = []\n",
    "    gt_stds = []\n",
    "    gradient_norms = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Clear cache periodically\n",
    "        if batch_idx % 10 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Create patches on GPU\n",
    "        with torch.no_grad():\n",
    "            patches = gpu_preprocessor(batch['color'])  # [B, 196, 768]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer_hand.zero_grad(set_to_none=True)\n",
    "        optimizer_object.zero_grad(set_to_none=True)\n",
    "        optimizer_contact.zero_grad(set_to_none=True)\n",
    "        \n",
    "        # Forward passes with mixed precision\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            # Get positional embeddings\n",
    "            hand_pos = hand_pos_embed(None)  # [1, 196, 1024]\n",
    "            object_pos = object_pos_embed(None)  # [1, 196, 1024]\n",
    "            \n",
    "            # STANDARD ViT: Pass patches and positional embeddings separately\n",
    "            # The encoder will: 1) project patches, 2) add positional embeddings\n",
    "            hand_output = hand_encoder(patches, spatial_encoding=hand_pos)\n",
    "            \n",
    "            hand_gt = batch['hand_joints_3d']\n",
    "            valid_hands = batch['has_hand']\n",
    "            \n",
    "            if valid_hands.any():\n",
    "                hand_pred = hand_output['joints_3d'][valid_hands]\n",
    "                hand_target = hand_gt[valid_hands]\n",
    "                \n",
    "                # Track prediction variance\n",
    "                pred_stds.append(hand_pred.std().item())\n",
    "                gt_stds.append(hand_target.std().item())\n",
    "                \n",
    "                # Main loss - use Smooth L1 for robustness\n",
    "                hand_loss = F.smooth_l1_loss(hand_pred, hand_target)\n",
    "                \n",
    "                # Per-joint weighting\n",
    "                joint_diff = F.smooth_l1_loss(hand_pred, hand_target, reduction='none')\n",
    "                weighted_diff = joint_diff * joint_weights.unsqueeze(0).unsqueeze(-1)\n",
    "                weighted_hand_loss = weighted_diff.mean()\n",
    "                \n",
    "                # Combine losses\n",
    "                hand_loss = 0.8 * hand_loss + 0.2 * weighted_hand_loss\n",
    "                \n",
    "                # Diversity loss\n",
    "                div_loss = diversity_loss(hand_pred)\n",
    "                total_diversity_loss += div_loss.item()\n",
    "                hand_loss = hand_loss + config['diversity_weight'] * div_loss\n",
    "                \n",
    "                # Temporal smoothness\n",
    "                if batch_idx > 0 and hasattr(train_epoch, 'prev_joints'):\n",
    "                    if train_epoch.prev_joints.shape[0] == hand_pred.shape[0]:\n",
    "                        velocity = hand_pred - train_epoch.prev_joints\n",
    "                        velocity_loss = velocity.norm(dim=-1).mean()\n",
    "                        hand_loss = hand_loss + config['velocity_loss_weight'] * velocity_loss\n",
    "                \n",
    "                train_epoch.prev_joints = hand_pred.detach()\n",
    "                \n",
    "                # L2 regularization on predictions\n",
    "                pred_norm = hand_pred.norm(dim=-1).mean()\n",
    "                if pred_norm > 1.0:\n",
    "                    hand_loss = hand_loss + 0.01 * pred_norm\n",
    "                    \n",
    "            else:\n",
    "                hand_loss = torch.tensor(0.0, device='cuda', dtype=torch.bfloat16)\n",
    "                div_loss = torch.tensor(0.0, device='cuda', dtype=torch.bfloat16)\n",
    "            \n",
    "            # Store features for contact encoder\n",
    "            hand_features = hand_output['features'].detach()\n",
    "            del hand_output\n",
    "            \n",
    "            # Object encoder with positional embeddings\n",
    "            object_output = object_encoder(patches, \n",
    "                                         object_ids=batch['ycb_ids'],\n",
    "                                         spatial_encoding=object_pos)\n",
    "            \n",
    "            object_loss = torch.tensor(0.0, device='cuda', dtype=torch.bfloat16)\n",
    "            \n",
    "            valid_objects = batch['num_objects'] > 0\n",
    "            if valid_objects.any():\n",
    "                object_positions_gt = batch['object_poses'][:, :, :3, 3]\n",
    "                num_pred = min(object_output['positions'].shape[1], 10)\n",
    "                \n",
    "                for i in torch.where(valid_objects)[0]:\n",
    "                    n_obj = batch['num_objects'][i].item()\n",
    "                    if n_obj > 0 and n_obj <= num_pred:\n",
    "                        pred = object_output['positions'][i, :n_obj]\n",
    "                        gt = object_positions_gt[i, :n_obj]\n",
    "                        object_loss = object_loss + F.smooth_l1_loss(pred, gt)\n",
    "                \n",
    "                if valid_objects.sum() > 0:\n",
    "                    object_loss = object_loss / valid_objects.sum()\n",
    "            \n",
    "            # Store features\n",
    "            object_features = object_output['features'].detach()\n",
    "            del object_output\n",
    "            \n",
    "            # Contact encoder - NO positional embeddings (works on features, not patches)\n",
    "            with torch.no_grad():\n",
    "                # Project features to contact hidden dim if needed\n",
    "                if hand_features.shape[-1] == 1024:\n",
    "                    hand_features_proj = hand_features[..., :512]\n",
    "                else:\n",
    "                    hand_features_proj = hand_features\n",
    "                    \n",
    "                if object_features.shape[-1] == 1024:\n",
    "                    object_features_proj = object_features[..., :512]\n",
    "                else:\n",
    "                    object_features_proj = object_features\n",
    "            \n",
    "            # Contact encoder expects hand and object features\n",
    "            contact_output = contact_encoder(\n",
    "                hand_features_proj,\n",
    "                object_features_proj\n",
    "            )\n",
    "            \n",
    "            # Clean up\n",
    "            del hand_features, object_features, hand_features_proj, object_features_proj, contact_output\n",
    "            \n",
    "            # Total loss\n",
    "            total_batch_loss = hand_loss + object_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_batch_loss.backward()\n",
    "        \n",
    "        # Track gradient norms\n",
    "        total_norm = 0\n",
    "        for p in hand_encoder.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2).item()\n",
    "                total_norm += param_norm ** 2\n",
    "        total_norm = total_norm ** 0.5\n",
    "        gradient_norms.append(total_norm)\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(hand_encoder.parameters(), config['grad_clip'])\n",
    "        torch.nn.utils.clip_grad_norm_(object_encoder.parameters(), config['grad_clip'])\n",
    "        torch.nn.utils.clip_grad_norm_(contact_encoder.parameters(), config['grad_clip'])\n",
    "        \n",
    "        # Optimizer steps\n",
    "        optimizer_hand.step()\n",
    "        optimizer_object.step()\n",
    "        optimizer_contact.step()\n",
    "        \n",
    "        # Metrics\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_hand_loss += hand_loss.item() if isinstance(hand_loss, torch.Tensor) else hand_loss\n",
    "        total_object_loss += object_loss.item() if isinstance(object_loss, torch.Tensor) else object_loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % 5 == 0:\n",
    "            current_lr = optimizer_hand.param_groups[0]['lr']\n",
    "            gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "            elapsed = time.time() - epoch_start\n",
    "            samples_per_sec = (batch_idx + 1) * config['batch_size'] / elapsed\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{total_batch_loss.item():.4f}',\n",
    "                'hand': f'{hand_loss.item():.4f}',\n",
    "                'div': f'{div_loss.item():.4f}' if valid_hands.any() else '0.0000',\n",
    "                'grad': f'{total_norm:.2f}',\n",
    "                'gpu': f'{gpu_mem:.1f}GB',\n",
    "                'speed': f'{samples_per_sec:.0f}/s',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "    \n",
    "    # Step schedulers\n",
    "    scheduler_hand.step()\n",
    "    scheduler_object.step()\n",
    "    scheduler_contact.step()\n",
    "    \n",
    "    # Print diagnostics\n",
    "    if len(pred_stds) > 0:\n",
    "        avg_pred_std = np.mean(pred_stds)\n",
    "        avg_gt_std = np.mean(gt_stds)\n",
    "        avg_grad_norm = np.mean(gradient_norms)\n",
    "        print(f\"\\n  Prediction std: {avg_pred_std:.4f} | GT std: {avg_gt_std:.4f} | Ratio: {avg_pred_std/avg_gt_std:.2f}\")\n",
    "        print(f\"  Average gradient norm: {avg_grad_norm:.2f}\")\n",
    "        \n",
    "        if avg_pred_std < 0.01:\n",
    "            print(\"  ⚠️  WARNING: Model still outputting near-constant predictions!\")\n",
    "        elif avg_pred_std > avg_gt_std * 0.5:\n",
    "            print(\"  ✓ Good prediction diversity!\")\n",
    "            \n",
    "        if avg_grad_norm < 0.01:\n",
    "            print(\"  ⚠️  WARNING: Vanishing gradients detected!\")\n",
    "        elif avg_grad_norm > 100:\n",
    "            print(\"  ⚠️  WARNING: Exploding gradients detected!\")\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss / max(num_batches, 1),\n",
    "        'hand_loss': total_hand_loss / max(num_batches, 1),\n",
    "        'object_loss': total_object_loss / max(num_batches, 1),\n",
    "        'diversity_loss': total_diversity_loss / max(num_batches, 1)\n",
    "    }\n",
    "\n",
    "\n",
    "def validate():\n",
    "    \"\"\"Validation with correct positional embeddings\"\"\"\n",
    "    hand_encoder.eval()\n",
    "    object_encoder.eval()\n",
    "    contact_encoder.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_mpjpe = 0\n",
    "    per_joint_errors = torch.zeros(21, device='cuda')\n",
    "    num_valid_hands = 0\n",
    "    num_valid_objects = 0\n",
    "    \n",
    "    # Diagnostic tracking\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Validation\")):\n",
    "            if batch_idx % 20 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            # Preprocess\n",
    "            patches = gpu_preprocessor(batch['color'])\n",
    "            \n",
    "            # Get positional embeddings\n",
    "            hand_pos = hand_pos_embed(None)\n",
    "            object_pos = object_pos_embed(None)\n",
    "            \n",
    "            # Forward with positional embeddings (standard ViT way)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                hand_output = hand_encoder(patches, spatial_encoding=hand_pos)\n",
    "                object_output = object_encoder(patches, \n",
    "                                             object_ids=batch['ycb_ids'],\n",
    "                                             spatial_encoding=object_pos)\n",
    "            \n",
    "            # Hand metrics\n",
    "            valid_hands = batch['has_hand']\n",
    "            if valid_hands.any():\n",
    "                hand_gt = batch['hand_joints_3d'][valid_hands]\n",
    "                hand_pred = hand_output['joints_3d'][valid_hands]\n",
    "                \n",
    "                # Store predictions\n",
    "                all_predictions.append(hand_pred.cpu())\n",
    "                \n",
    "                # Loss\n",
    "                loss = F.smooth_l1_loss(hand_pred, hand_gt)\n",
    "                \n",
    "                # MPJPE\n",
    "                joint_errors = (hand_pred - hand_gt).norm(dim=-1)  # [B, 21]\n",
    "                mpjpe = joint_errors.mean()\n",
    "                \n",
    "                # Per-joint accumulation\n",
    "                per_joint_errors += joint_errors.sum(dim=0)\n",
    "                \n",
    "                total_loss += loss.item() * valid_hands.sum().item()\n",
    "                total_mpjpe += mpjpe.item() * valid_hands.sum().item()\n",
    "                num_valid_hands += valid_hands.sum().item()\n",
    "            \n",
    "            # Object metrics\n",
    "            valid_objects = batch['num_objects'] > 0\n",
    "            if valid_objects.any():\n",
    "                num_valid_objects += valid_objects.sum().item()\n",
    "                \n",
    "            del hand_output, object_output\n",
    "    \n",
    "    # Analyze prediction diversity\n",
    "    if len(all_predictions) > 0:\n",
    "        all_preds = torch.cat(all_predictions, dim=0)\n",
    "        pred_std = all_preds.std(dim=0).mean().item()\n",
    "        print(f\"  Validation prediction diversity (std): {pred_std:.4f}\")\n",
    "        if pred_std < 0.01:\n",
    "            print(\"  ⚠️  Model is outputting nearly identical predictions!\")\n",
    "        elif pred_std > 0.1:\n",
    "            print(\"  ✓ Good prediction diversity in validation!\")\n",
    "    \n",
    "    # Calculate per-joint MPJPE\n",
    "    per_joint_mpjpe = per_joint_errors / max(num_valid_hands, 1)\n",
    "    \n",
    "    return {\n",
    "        'loss': total_loss / max(num_valid_hands, 1),\n",
    "        'mpjpe': total_mpjpe / max(num_valid_hands, 1),\n",
    "        'per_joint_mpjpe': per_joint_mpjpe.cpu().numpy(),\n",
    "        'hand_coverage': num_valid_hands / len(val_dataset),\n",
    "        'object_coverage': num_valid_objects / len(val_dataset)\n",
    "    }\n",
    "\n",
    "print(\"✓ Fixed training loop to use standard ViT positional embeddings\")\n",
    "print(\"  - Positional embeddings passed directly to encoders\")\n",
    "print(\"  - Encoders handle projection + addition internally\")\n",
    "print(\"  - Contact encoder doesn't use positional embeddings (works on features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU monitoring utilities (same as before)\n",
    "def get_gpu_stats():\n",
    "    \"\"\"Get comprehensive GPU statistics\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Memory stats\n",
    "    stats['mem_allocated'] = torch.cuda.memory_allocated() / 1e9\n",
    "    stats['mem_reserved'] = torch.cuda.memory_reserved() / 1e9\n",
    "    stats['mem_total'] = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    stats['mem_free'] = stats['mem_total'] - stats['mem_allocated']\n",
    "    \n",
    "    # GPU utilization\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            'nvidia-smi', '--query-gpu=utilization.gpu,power.draw,power.limit,temperature.gpu',\n",
    "            '--format=csv,noheader,nounits'\n",
    "        ], capture_output=True, text=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            values = result.stdout.strip().split(', ')\n",
    "            stats['gpu_util'] = float(values[0])\n",
    "            stats['power_draw'] = float(values[1])\n",
    "            stats['power_limit'] = float(values[2])\n",
    "            stats['temperature'] = float(values[3])\n",
    "    except:\n",
    "        stats['gpu_util'] = 0\n",
    "        stats['power_draw'] = 0\n",
    "        stats['power_limit'] = 700\n",
    "        stats['temperature'] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def print_gpu_stats():\n",
    "    \"\"\"Print formatted GPU statistics\"\"\"\n",
    "    stats = get_gpu_stats()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GPU Statistics:\")\n",
    "    print(f\"  Memory: {stats['mem_allocated']:.1f} / {stats['mem_total']:.1f} GB \"\n",
    "          f\"({stats['mem_allocated']/stats['mem_total']*100:.1f}%) | \"\n",
    "          f\"Free: {stats['mem_free']:.1f} GB\")\n",
    "    print(f\"  Utilization: {stats['gpu_util']:.0f}% | \"\n",
    "          f\"Temperature: {stats['temperature']:.0f}°C\")\n",
    "    print(f\"  Power: {stats['power_draw']:.0f}W / {stats['power_limit']:.0f}W \"\n",
    "          f\"({stats['power_draw']/stats['power_limit']*100:.1f}%)\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with mode collapse fixes\n",
    "print(\"Starting GPU-Optimized Training (Mode Collapse Fix)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n",
    "print(f\"  Train samples: {config['max_samples_train']:,}\")\n",
    "print(f\"\\nKey changes to fix mode collapse:\")\n",
    "print(f\"  ✓ MSE loss (no label smoothing)\")\n",
    "print(f\"  ✓ Minimal regularization (dropout={config['dropout']})\")\n",
    "print(f\"  ✓ Diversity loss (weight={config['diversity_weight']})\")\n",
    "print(f\"  ✓ Larger initialization (std={config['init_std']})\")\n",
    "print(f\"  ✓ CosineAnnealingLR (stable training)\")\n",
    "print(f\"  ✓ No stochastic depth\")\n",
    "print(f\"  ✓ No MixUp augmentation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_hand_loss': [],\n",
    "    'train_object_loss': [],\n",
    "    'train_diversity_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_mpjpe': [],\n",
    "    'per_joint_mpjpe': [],\n",
    "    'throughput': [],\n",
    "    'gpu_util': [],\n",
    "    'power_draw': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# Initial GPU stats\n",
    "print_gpu_stats()\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "best_mpjpe = float('inf')\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training\n",
    "    train_metrics = train_epoch(epoch)\n",
    "    history['train_loss'].append(train_metrics['total_loss'])\n",
    "    history['train_hand_loss'].append(train_metrics['hand_loss'])\n",
    "    history['train_object_loss'].append(train_metrics['object_loss'])\n",
    "    history['train_diversity_loss'].append(train_metrics['diversity_loss'])\n",
    "    history['learning_rate'].append(optimizer_hand.param_groups[0]['lr'])\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = validate()\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_mpjpe'].append(val_metrics['mpjpe'])\n",
    "    history['per_joint_mpjpe'].append(val_metrics['per_joint_mpjpe'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    samples_processed = len(train_loader) * config['batch_size']\n",
    "    throughput = samples_processed / epoch_time\n",
    "    history['throughput'].append(throughput)\n",
    "    \n",
    "    # Get GPU stats\n",
    "    stats = get_gpu_stats()\n",
    "    history['gpu_util'].append(stats['gpu_util'])\n",
    "    history['power_draw'].append(stats['power_draw'])\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Epoch {epoch+1}/{config['num_epochs']} Summary:\")\n",
    "    print(f\"  Train Loss: {train_metrics['total_loss']:.4f} \"\n",
    "          f\"(Hand: {train_metrics['hand_loss']:.4f}, Object: {train_metrics['object_loss']:.4f}, \"\n",
    "          f\"Diversity: {train_metrics['diversity_loss']:.4f})\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  Val MPJPE: {val_metrics['mpjpe']*1000:.2f} mm\")\n",
    "    print(f\"  Learning Rate: {history['learning_rate'][-1]:.2e}\")\n",
    "    print(f\"  Throughput: {throughput:.0f} samples/s\")\n",
    "    print(f\"  Epoch Time: {epoch_time/60:.1f} min\")\n",
    "    \n",
    "    # Check for improvements\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        print(\"  ✓ New best validation loss!\")\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'hand_state': hand_encoder.state_dict(),\n",
    "            'object_state': object_encoder.state_dict(),\n",
    "            'contact_state': contact_encoder.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "            'val_mpjpe': val_metrics['mpjpe'],\n",
    "            'config': config\n",
    "        }, 'best_model_fixed.pth')\n",
    "    \n",
    "    if val_metrics['mpjpe'] < best_mpjpe:\n",
    "        best_mpjpe = val_metrics['mpjpe']\n",
    "        print(f\"  ✓ New best MPJPE: {best_mpjpe*1000:.2f} mm\")\n",
    "    \n",
    "    # Print per-joint errors for worst joints\n",
    "    per_joint = val_metrics['per_joint_mpjpe']\n",
    "    worst_joints = np.argsort(per_joint)[-3:]  # Top 3 worst\n",
    "    print(f\"  Worst joints: \", end=\"\")\n",
    "    for j in worst_joints:\n",
    "        print(f\"J{j}:{per_joint[j]*1000:.1f}mm \", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    print_gpu_stats()\n",
    "\n",
    "# Training complete\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Training Complete!\")\n",
    "print(f\"  Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best MPJPE: {best_mpjpe*1000:.2f} mm\")\n",
    "print(f\"  Average throughput: {np.mean(history['throughput']):.0f} samples/s\")\n",
    "print(f\"  Average GPU utilization: {np.mean(history['gpu_util']):.1f}%\")\n",
    "print(f\"  Average power draw: {np.mean(history['power_draw']):.0f}W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization with diversity tracking\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 16))\n",
    "fig.suptitle('GPU-Optimized Training Results (Mode Collapse Fix)', fontsize=16)\n",
    "\n",
    "# 1. Loss curves with diversity\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['train_loss'], label='Train Total', marker='o', linewidth=2)\n",
    "ax.plot(history['train_hand_loss'], label='Train Hand', marker='s', linewidth=2, alpha=0.7)\n",
    "ax.plot(history['val_loss'], label='Val Loss', marker='^', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Progress (MSE Loss)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MPJPE with target\n",
    "ax = axes[0, 1]\n",
    "mpjpe_mm = [x*1000 for x in history['val_mpjpe']]\n",
    "ax.plot(mpjpe_mm, label='Val MPJPE', marker='o', color='green', linewidth=2)\n",
    "ax.axhline(y=20, color='red', linestyle='--', alpha=0.5, label='Target (20mm)')\n",
    "ax.axhline(y=100, color='orange', linestyle='--', alpha=0.5, label='Goal (100mm)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MPJPE (mm)')\n",
    "ax.set_title('Hand Pose Error')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Diversity loss tracking\n",
    "ax = axes[0, 2]\n",
    "ax.plot(history['train_diversity_loss'], label='Diversity Loss', marker='o', color='purple', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Diversity Loss')\n",
    "ax.set_title('Diversity Loss (Lower = More Diverse)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Per-joint MPJPE heatmap\n",
    "ax = axes[1, 0]\n",
    "joint_errors = np.array(history['per_joint_mpjpe']) * 1000  # Convert to mm\n",
    "im = ax.imshow(joint_errors.T, aspect='auto', cmap='hot', interpolation='nearest')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Joint Index')\n",
    "ax.set_title('Per-Joint MPJPE (mm)')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 5. Learning rate schedule\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['learning_rate'], label='Learning Rate', marker='o', color='blue', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('CosineAnnealingLR Schedule')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. GPU Utilization\n",
    "ax = axes[1, 2]\n",
    "ax.plot(history['gpu_util'], label='GPU Utilization', marker='o', color='orange', linewidth=2)\n",
    "ax.axhline(y=85, color='green', linestyle='--', alpha=0.5, label='Target (85%)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('GPU Utilization %')\n",
    "ax.set_title('GPU Usage')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Loss improvement rate\n",
    "ax = axes[2, 0]\n",
    "if len(history['train_loss']) > 1:\n",
    "    train_improvement = np.diff(history['train_loss']) / np.array(history['train_loss'][:-1]) * 100\n",
    "    val_improvement = np.diff(history['val_loss']) / np.array(history['val_loss'][:-1]) * 100\n",
    "    ax.plot(train_improvement, label='Train Loss Change %', marker='o')\n",
    "    ax.plot(val_improvement, label='Val Loss Change %', marker='s')\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Relative Change %')\n",
    "ax.set_title('Loss Improvement Rate')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Throughput\n",
    "ax = axes[2, 1]\n",
    "ax.plot(history['throughput'], label='Throughput', marker='o', color='green', linewidth=2)\n",
    "ax.axhline(y=np.mean(history['throughput']), color='red', linestyle='--', \n",
    "           label=f'Average ({np.mean(history[\"throughput\"]):.0f} samples/s)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Samples/second')\n",
    "ax.set_title('Training Speed')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Final joint error distribution\n",
    "ax = axes[2, 2]\n",
    "if len(history['per_joint_mpjpe']) > 0:\n",
    "    final_joint_errors = history['per_joint_mpjpe'][-1] * 1000\n",
    "    joint_names = ['Wrist'] + [f'F{i//4}J{i%4}' for i in range(20)]\n",
    "    bars = ax.bar(range(21), final_joint_errors)\n",
    "    # Color fingertips differently\n",
    "    fingertip_indices = [4, 8, 12, 16, 20]\n",
    "    for i in fingertip_indices:\n",
    "        if i < len(bars):\n",
    "            bars[i].set_color('red')\n",
    "ax.set_xlabel('Joint')\n",
    "ax.set_ylabel('MPJPE (mm)')\n",
    "ax.set_title('Final Per-Joint Errors (Red=Fingertips)')\n",
    "ax.set_xticks(range(0, 21, 4))\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_fixed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print analysis\n",
    "print(\"\\nTraining Analysis:\")\n",
    "if len(history['train_loss']) > 0:\n",
    "    print(f\"  Initial loss: {history['train_loss'][0]:.4f}\")\n",
    "    print(f\"  Final loss: {history['train_loss'][-1]:.4f}\")\n",
    "    if history['train_loss'][0] > 0:\n",
    "        print(f\"  Loss reduction: {(1 - history['train_loss'][-1]/history['train_loss'][0])*100:.1f}%\")\n",
    "\n",
    "if len(history['val_mpjpe']) > 0:\n",
    "    print(f\"\\n  Initial MPJPE: {history['val_mpjpe'][0]*1000:.2f} mm\")\n",
    "    print(f\"  Final MPJPE: {history['val_mpjpe'][-1]*1000:.2f} mm\")\n",
    "    if history['val_mpjpe'][0] > 0:\n",
    "        print(f\"  MPJPE reduction: {(1 - history['val_mpjpe'][-1]/history['val_mpjpe'][0])*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Best achieved MPJPE: {best_mpjpe*1000:.2f} mm\")\n",
    "\n",
    "if len(history['train_diversity_loss']) > 0:\n",
    "    print(f\"\\n  Initial diversity loss: {history['train_diversity_loss'][0]:.4f}\")\n",
    "    print(f\"  Final diversity loss: {history['train_diversity_loss'][-1]:.4f}\")\n",
    "    print(f\"  Diversity improvement: {abs(history['train_diversity_loss'][-1] - history['train_diversity_loss'][0]):.4f}\")\n",
    "\n",
    "# Identify problematic joints\n",
    "if len(history['per_joint_mpjpe']) > 0:\n",
    "    final_joint_errors = history['per_joint_mpjpe'][-1] * 1000\n",
    "    joint_names = ['Wrist'] + [f'F{i//4}J{i%4}' for i in range(20)]\n",
    "    print(\"\\nProblematic joints (>150mm error):\")\n",
    "    for i, error in enumerate(final_joint_errors):\n",
    "        if error > 150:\n",
    "            print(f\"  Joint {i} ({joint_names[i] if i < len(joint_names) else f'J{i}'}): {error:.1f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and training history with mode collapse fixes\n",
    "checkpoint_dir = 'checkpoints/gpu_optimized_fixed'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Save individual model checkpoints\n",
    "torch.save({\n",
    "    'model_state_dict': hand_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_hand.state_dict(),\n",
    "    'scheduler_state_dict': scheduler_hand.state_dict(),\n",
    "    'config': config,\n",
    "    'history': history,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'best_mpjpe': best_mpjpe\n",
    "}, os.path.join(checkpoint_dir, 'hand_encoder_fixed.pth'))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': object_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_object.state_dict(),\n",
    "    'scheduler_state_dict': scheduler_object.state_dict(),\n",
    "}, os.path.join(checkpoint_dir, 'object_encoder_fixed.pth'))\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': contact_encoder.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_contact.state_dict(),\n",
    "    'scheduler_state_dict': scheduler_contact.state_dict(),\n",
    "}, os.path.join(checkpoint_dir, 'contact_encoder_fixed.pth'))\n",
    "\n",
    "# Save training history\n",
    "with open(os.path.join(checkpoint_dir, 'training_history_fixed.json'), 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    json_history = {}\n",
    "    for key, value in history.items():\n",
    "        if key == 'per_joint_mpjpe':\n",
    "            json_history[key] = [v.tolist() for v in value]\n",
    "        else:\n",
    "            json_history[key] = value\n",
    "    json.dump(json_history, f, indent=2)\n",
    "\n",
    "print(f\"✓ Models and history saved to {checkpoint_dir}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best MPJPE: {best_mpjpe*1000:.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Results Logger - Saves all results to markdown (handles partial results)\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def log_training_results(config, history, best_val_loss=None, best_mpjpe=None, \n",
    "                        total_time=None, model_params=None, log_file='training_results_log.md'):\n",
    "    \"\"\"\n",
    "    Append training results to a markdown file for tracking experiments.\n",
    "    Handles partial results gracefully if training was stopped early.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary\n",
    "        history: Training history dictionary\n",
    "        best_val_loss: Best validation loss achieved (optional)\n",
    "        best_mpjpe: Best MPJPE achieved (optional)\n",
    "        total_time: Total training time in seconds (optional)\n",
    "        model_params: Dictionary with model parameter counts (optional)\n",
    "        log_file: Path to markdown file (will append)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Handle partial results - check what data we actually have\n",
    "    epochs_completed = len(history.get('train_loss', []))\n",
    "    if epochs_completed == 0:\n",
    "        print(\"⚠️ No training data to log yet\")\n",
    "        return\n",
    "    \n",
    "    # Calculate summary statistics with defaults for missing data\n",
    "    final_train_loss = history['train_loss'][-1] if history.get('train_loss') else float('inf')\n",
    "    final_val_loss = history['val_loss'][-1] if history.get('val_loss') else float('inf')\n",
    "    final_mpjpe = history['val_mpjpe'][-1] * 1000 if history.get('val_mpjpe') else float('inf')\n",
    "    \n",
    "    # Handle potentially missing metrics\n",
    "    avg_gpu_util = np.mean(history['gpu_util']) if history.get('gpu_util') else 0\n",
    "    avg_throughput = np.mean(history['throughput']) if history.get('throughput') else 0\n",
    "    avg_power = np.mean(history['power_draw']) if history.get('power_draw') else 0\n",
    "    \n",
    "    # Use provided values or calculate from history\n",
    "    if best_val_loss is None and history.get('val_loss'):\n",
    "        best_val_loss = min(history['val_loss'])\n",
    "    if best_mpjpe is None and history.get('val_mpjpe'):\n",
    "        best_mpjpe = min(history['val_mpjpe'])\n",
    "    \n",
    "    # Estimate time if not provided\n",
    "    if total_time is None:\n",
    "        # Rough estimate: 2.5 minutes per epoch\n",
    "        total_time = epochs_completed * 2.5 * 60\n",
    "    \n",
    "    # Get model params if not provided\n",
    "    if model_params is None:\n",
    "        try:\n",
    "            model_params = {\n",
    "                'hand_encoder': count_params(hand_encoder),\n",
    "                'object_encoder': count_params(object_encoder),\n",
    "                'contact_encoder': count_params(contact_encoder),\n",
    "            }\n",
    "            model_params['total'] = sum(model_params.values())\n",
    "        except:\n",
    "            model_params = {'total': 0}  # Default if models not available\n",
    "    \n",
    "    # Prepare markdown content\n",
    "    markdown_content = f\"\"\"\n",
    "## Training Run: {timestamp}\n",
    "\n",
    "### Status: {\"PARTIAL (Training Interrupted)\" if epochs_completed < config.get('num_epochs', 20) else \"COMPLETE\"}\n",
    "\n",
    "### Configuration\n",
    "```json\n",
    "{json.dumps(config, indent=2)}\n",
    "```\n",
    "\n",
    "### Model Architecture\n",
    "- **Hand Encoder**: {model_params.get('hand_encoder', 0)/1e6:.1f}M parameters\n",
    "- **Object Encoder**: {model_params.get('object_encoder', 0)/1e6:.1f}M parameters  \n",
    "- **Contact Encoder**: {model_params.get('contact_encoder', 0)/1e6:.1f}M parameters\n",
    "- **Total Parameters**: {model_params.get('total', 0)/1e6:.1f}M\n",
    "\n",
    "### Training Summary\n",
    "- **Duration**: ~{total_time/60:.1f} minutes\n",
    "- **Epochs Completed**: {epochs_completed} / {config.get('num_epochs', 'Unknown')}\n",
    "- **Average Throughput**: {avg_throughput:.0f} samples/s\n",
    "- **Average GPU Utilization**: {avg_gpu_util:.1f}%\n",
    "- **Average Power Draw**: {avg_power:.0f}W\n",
    "\n",
    "### Results (at epoch {epochs_completed})\n",
    "- **Final Train Loss**: {final_train_loss:.4f}\n",
    "- **Final Val Loss**: {final_val_loss:.4f if final_val_loss != float('inf') else 'N/A'}\n",
    "- **Final MPJPE**: {final_mpjpe:.2f if final_mpjpe != float('inf') else 'N/A'} mm\n",
    "- **Best Val Loss**: {best_val_loss:.4f if best_val_loss is not None else 'N/A'}\n",
    "- **Best MPJPE**: {best_mpjpe*1000:.2f if best_mpjpe is not None else 'N/A'} mm\n",
    "\n",
    "### Loss Breakdown (Final Available Epoch)\n",
    "\"\"\"\n",
    "    \n",
    "    # Add loss breakdown if available\n",
    "    if history.get('train_hand_loss'):\n",
    "        markdown_content += f\"- **Hand Loss**: {history['train_hand_loss'][-1]:.4f}\\n\"\n",
    "    if history.get('train_object_loss'):\n",
    "        markdown_content += f\"- **Object Loss**: {history['train_object_loss'][-1]:.4f}\\n\"\n",
    "    if history.get('train_diversity_loss'):\n",
    "        markdown_content += f\"- **Diversity Loss**: {history['train_diversity_loss'][-1]:.4f}\\n\"\n",
    "    \n",
    "    markdown_content += \"\\n### Per-Epoch Metrics\\n\"\n",
    "    markdown_content += \"| Epoch | Train Loss | Val Loss | MPJPE (mm) | GPU Util% | LR |\\n\"\n",
    "    markdown_content += \"|-------|------------|----------|------------|-----------|-----|\\n\"\n",
    "    \n",
    "    # Add per-epoch data\n",
    "    for i in range(epochs_completed):\n",
    "        markdown_content += f\"| {i+1} | \"\n",
    "        \n",
    "        # Train loss (should always exist)\n",
    "        markdown_content += f\"{history['train_loss'][i]:.4f} | \"\n",
    "        \n",
    "        # Val loss\n",
    "        if history.get('val_loss') and i < len(history['val_loss']):\n",
    "            markdown_content += f\"{history['val_loss'][i]:.4f} | \"\n",
    "        else:\n",
    "            markdown_content += \"- | \"\n",
    "            \n",
    "        # MPJPE\n",
    "        if history.get('val_mpjpe') and i < len(history['val_mpjpe']):\n",
    "            markdown_content += f\"{history['val_mpjpe'][i]*1000:.2f} | \"\n",
    "        else:\n",
    "            markdown_content += \"- | \"\n",
    "            \n",
    "        # GPU util\n",
    "        if history.get('gpu_util') and i < len(history['gpu_util']):\n",
    "            markdown_content += f\"{history['gpu_util'][i]:.0f} | \"\n",
    "        else:\n",
    "            markdown_content += \"- | \"\n",
    "            \n",
    "        # Learning rate\n",
    "        if history.get('learning_rate') and i < len(history['learning_rate']):\n",
    "            markdown_content += f\"{history['learning_rate'][i]:.2e} |\"\n",
    "        else:\n",
    "            markdown_content += \"- |\"\n",
    "            \n",
    "        markdown_content += \"\\n\"\n",
    "    \n",
    "    # Add per-joint analysis if available\n",
    "    if history.get('per_joint_mpjpe') and len(history['per_joint_mpjpe']) > 0:\n",
    "        final_joint_errors = history['per_joint_mpjpe'][-1] * 1000\n",
    "        markdown_content += \"\\n### Final Per-Joint MPJPE (mm)\\n\"\n",
    "        markdown_content += \"| Joint | Error (mm) | Type |\\n\"\n",
    "        markdown_content += \"|-------|------------|------|\\n\"\n",
    "        \n",
    "        joint_names = ['Wrist'] + [f'F{i//4}J{i%4}' for i in range(20)]\n",
    "        fingertip_indices = [4, 8, 12, 16, 20]\n",
    "        \n",
    "        for i, error in enumerate(final_joint_errors):\n",
    "            joint_type = \"Fingertip\" if i in fingertip_indices else \"Other\"\n",
    "            markdown_content += f\"| {joint_names[i] if i < len(joint_names) else f'J{i}'} | {error:.1f} | {joint_type} |\\n\"\n",
    "    \n",
    "    # Add notes about configuration\n",
    "    markdown_content += f\"\"\"\n",
    "### Configuration Notes\n",
    "- **Regularization**: Dropout={config.get('dropout', 'N/A')}, Drop Path={config.get('drop_path', 'N/A')}, Weight Decay={config.get('weight_decay', 'N/A')}\n",
    "- **Loss Function**: {\"MSE\" if config.get('label_smoothing', 0) == 0 else f\"Smooth L1 with {config.get('label_smoothing')} smoothing\"}\n",
    "- **Augmentation**: MixUp={config.get('mixup_alpha', 'N/A')}, Joint Noise={config.get('joint_noise_std', 'N/A')}, Rotation={config.get('rotation_range', 'N/A')}°\n",
    "- **Special Features**: Diversity Loss Weight={config.get('diversity_weight', 'N/A')}, Velocity Loss Weight={config.get('velocity_loss_weight', 'N/A')}\n",
    "\n",
    "### Analysis\n",
    "\"\"\"\n",
    "    \n",
    "    # Add automatic analysis\n",
    "    if len(history.get('train_loss', [])) > 1:\n",
    "        loss_reduction = (1 - history['train_loss'][-1]/history['train_loss'][0])*100\n",
    "        markdown_content += f\"- **Training Loss Reduction**: {loss_reduction:.1f}%\\n\"\n",
    "    \n",
    "    if len(history.get('val_mpjpe', [])) > 1:\n",
    "        mpjpe_reduction = (1 - history['val_mpjpe'][-1]/history['val_mpjpe'][0])*100\n",
    "        markdown_content += f\"- **MPJPE Reduction**: {mpjpe_reduction:.1f}%\\n\"\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if final_train_loss != float('inf') and final_val_loss != float('inf'):\n",
    "        if final_train_loss < final_val_loss * 0.5:\n",
    "            markdown_content += \"- ⚠️ **Potential Overfitting**: Train loss significantly lower than val loss\\n\"\n",
    "    \n",
    "    # Check for mode collapse\n",
    "    if history.get('train_diversity_loss') and len(history['train_diversity_loss']) > 0:\n",
    "        if history['train_diversity_loss'][-1] > 3.0:\n",
    "            markdown_content += \"- ⚠️ **Potential Mode Collapse**: High diversity loss indicates low prediction variance\\n\"\n",
    "    \n",
    "    # Check convergence\n",
    "    if len(history.get('train_loss', [])) > 5:\n",
    "        recent_losses = history['train_loss'][-5:]\n",
    "        if max(recent_losses) - min(recent_losses) < 0.001:\n",
    "            markdown_content += \"- ✓ **Converged**: Training loss has plateaued\\n\"\n",
    "    \n",
    "    if epochs_completed < config.get('num_epochs', 20):\n",
    "        markdown_content += f\"- ⚠️ **Incomplete**: Only {epochs_completed}/{config.get('num_epochs', 'Unknown')} epochs completed\\n\"\n",
    "    \n",
    "    markdown_content += \"\\n---\\n\"\n",
    "    \n",
    "    # Create file if it doesn't exist\n",
    "    if not os.path.exists(log_file):\n",
    "        with open(log_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"# Training Results Log\\n\\n\")\n",
    "            f.write(\"This file tracks all training experiments with configurations and results.\\n\\n\")\n",
    "    \n",
    "    # Append to file\n",
    "    with open(log_file, 'a', encoding='utf-8') as f:\n",
    "        f.write(markdown_content)\n",
    "    \n",
    "    print(f\"✓ Results logged to {log_file}\")\n",
    "    return log_file\n",
    "\n",
    "\n",
    "# Convenience function to save current state\n",
    "def save_training_log():\n",
    "    \"\"\"Save current training results to log file, handling partial results\"\"\"\n",
    "    \n",
    "    # Check if we have the necessary variables\n",
    "    try:\n",
    "        # Calculate model parameters if models exist\n",
    "        model_params = {\n",
    "            'hand_encoder': count_params(hand_encoder),\n",
    "            'object_encoder': count_params(object_encoder),\n",
    "            'contact_encoder': count_params(contact_encoder),\n",
    "            'total': count_params(hand_encoder) + count_params(object_encoder) + count_params(contact_encoder)\n",
    "        }\n",
    "    except NameError:\n",
    "        print(\"⚠️ Models not found, using default params\")\n",
    "        model_params = None\n",
    "    \n",
    "    # Get best metrics if they exist\n",
    "    try:\n",
    "        best_val_loss_value = best_val_loss if 'best_val_loss' in globals() else None\n",
    "        best_mpjpe_value = best_mpjpe if 'best_mpjpe' in globals() else None\n",
    "    except:\n",
    "        best_val_loss_value = None\n",
    "        best_mpjpe_value = None\n",
    "    \n",
    "    # Calculate total time if available\n",
    "    try:\n",
    "        if 'total_start' in globals() and 'history' in globals() and len(history.get('train_loss', [])) > 0:\n",
    "            total_time_value = (time.time() - total_start) if 'total_start' in globals() else None\n",
    "        else:\n",
    "            total_time_value = None\n",
    "    except:\n",
    "        total_time_value = None\n",
    "    \n",
    "    # Log the results\n",
    "    log_file = log_training_results(\n",
    "        config=config,\n",
    "        history=history,\n",
    "        best_val_loss=best_val_loss_value,\n",
    "        best_mpjpe=best_mpjpe_value,\n",
    "        total_time=total_time_value,\n",
    "        model_params=model_params,\n",
    "        log_file='training_results_log.md'\n",
    "    )\n",
    "    \n",
    "    return log_file\n",
    "\n",
    "print(\"✓ Training logger added with partial results support!\")\n",
    "print(\"  - Call save_training_log() anytime to save current progress\")\n",
    "print(\"  - Works even if training was interrupted\")\n",
    "print(\"  - Automatically detects available metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Save partial results (run this anytime during or after training)\n",
    "log_file = save_training_log()\n",
    "print(f\"\\nYou can view the log file at: {log_file}\")\n",
    "print(\"The log file will accumulate all your training runs with their configurations and results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Mode Collapse Fixes\n",
    "\n",
    "### Key Changes Implemented:\n",
    "1. **Reduced Regularization**:\n",
    "   - Dropout: 0.3 → 0.1\n",
    "   - Stochastic depth: 0.2 → 0.0 (disabled)\n",
    "   - Label smoothing: 0.2 → 0.0 (disabled)\n",
    "   - MixUp: 0.4 → 0.0 (disabled)\n",
    "\n",
    "2. **Loss Function Changes**:\n",
    "   - Smooth L1 → Standard MSE\n",
    "   - Added diversity loss to encourage varied predictions\n",
    "   - Reduced per-joint weighting importance\n",
    "\n",
    "3. **Optimization Changes**:\n",
    "   - OneCycleLR → CosineAnnealingLR (more stable)\n",
    "   - Learning rate: 5e-4 → 1e-3\n",
    "   - Weight decay: 0.05 → 0.01\n",
    "\n",
    "4. **Initialization**:\n",
    "   - Output layer std: 0.01 → 0.1 (10x larger)\n",
    "   - Using Kaiming initialization for hidden layers\n",
    "\n",
    "### Expected Results:\n",
    "- **Better diversity**: Predictions should vary based on input\n",
    "- **Improved convergence**: No early plateauing\n",
    "- **Lower MPJPE**: Better accuracy through learning input-specific features\n",
    "\n",
    "### Next Steps If Still Collapsed:\n",
    "1. Increase diversity loss weight (try 0.05 or 0.1)\n",
    "2. Add noise to encoder features during training\n",
    "3. Use temperature scaling on outputs\n",
    "4. Consider adversarial diversity loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}