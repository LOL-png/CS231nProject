{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Training from Checkpoint - Fixed Learning Rate\n",
    "\n",
    "This notebook loads the checkpoint from epoch 20 and continues training with a properly configured learning rate scheduler.\n",
    "\n",
    "## Key Fixes:\n",
    "1. **Properly Initialize Scheduler**: Set initial LRs in optimizer before creating scheduler\n",
    "2. **Load Optimizer State**: Restore optimizer state from checkpoint\n",
    "3. **Adjust Scheduler**: Account for already completed epochs\n",
    "4. **Monitor LR Changes**: Add detailed logging to verify LR is changing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set DEX_YCB_DIR environment variable\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set DEX_YCB_DIR environment variable\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set up matplotlib\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Auto-reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = \"outputs/full_featured/checkpoints/checkpoint_epoch_20.pth\"\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(f\"\\nCheckpoint contents:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# Extract configuration\n",
    "from omegaconf import OmegaConf\n",
    "config = checkpoint['config']\n",
    "start_epoch = checkpoint['epoch'] + 1  # Start from next epoch\n",
    "\n",
    "print(f\"\\nResuming from epoch: {start_epoch}\")\n",
    "print(f\"Previous training epochs: {checkpoint['epoch'] + 1}\")\n",
    "\n",
    "# Update output directory for continued training\n",
    "config.output_dir = 'outputs/continued_training'\n",
    "config.experiment_name = 'continued_from_epoch_20'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{config.output_dir}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{config.output_dir}/debug\", exist_ok=True)\n",
    "os.makedirs(f\"{config.output_dir}/visualizations\", exist_ok=True)\n",
    "\n",
    "# Initialize new history for continued training\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_mpjpe': [],\n",
    "    'val_mpjpe': [],\n",
    "    'learning_rates': [],\n",
    "    'gradient_norms': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since history is not saved in checkpoint, we'll skip the analysis\n",
    "# and focus on setting up proper learning rates\n",
    "\n",
    "print(\"Setting up continued training with proper learning rate configuration...\")\n",
    "print(f\"\\nOriginal configuration:\")\n",
    "print(f\"  Base learning rate: {config.training.learning_rate}\")\n",
    "print(f\"  Min learning rate (eta_min): {config.training.min_lr}\")\n",
    "print(f\"  T_0 (restart period): {config.training.T_0}\")\n",
    "print(f\"\\nThe issue was that the learning rate was stuck at eta_min (1e-6).\")\n",
    "print(\"We'll fix this by using a higher base learning rate to ensure proper oscillation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = \"outputs/full_featured/checkpoints/checkpoint_epoch_20.pth\"\n",
    "print(f\"Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(f\"\\nCheckpoint contents:\")\n",
    "for key in checkpoint.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# Extract configuration and history\n",
    "from omegaconf import OmegaConf\n",
    "config = checkpoint['config']\n",
    "history = checkpoint['history']\n",
    "start_epoch = checkpoint['epoch'] + 1  # Start from next epoch\n",
    "\n",
    "print(f\"\\nResuming from epoch: {start_epoch}\")\n",
    "print(f\"Previous training epochs: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Best loss so far: {min(history['val_loss']):.4f}\")\n",
    "print(f\"Best MPJPE so far: {min(history['val_mpjpe']):.2f} mm\")\n",
    "\n",
    "# Update output directory for continued training\n",
    "config.output_dir = 'outputs/continued_training'\n",
    "config.experiment_name = 'continued_from_epoch_20'\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{config.output_dir}/checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"{config.output_dir}/debug\", exist_ok=True)\n",
    "os.makedirs(f\"{config.output_dir}/visualizations\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Learning Rate History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot previous learning rate history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['learning_rates'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Previous Learning Rate Schedule')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add markers for key points\n",
    "plt.axhline(y=1e-6, color='r', linestyle='--', label='eta_min (1e-6)')\n",
    "plt.axhline(y=1e-4, color='g', linestyle='--', label='initial LR (1e-4)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Zoom in on the constant region\n",
    "plt.plot(history['learning_rates'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate (Linear Scale)')\n",
    "plt.ylim(0, 2e-6)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLearning rate analysis:\")\n",
    "print(f\"  Initial LR (config): {config.training.learning_rate}\")\n",
    "print(f\"  Min LR (eta_min): {config.training.min_lr}\")\n",
    "print(f\"  Current LR (epoch {checkpoint['epoch']}): {history['learning_rates'][-1]:.2e}\")\n",
    "print(f\"  Unique LR values: {set(history['learning_rates'])}\")\n",
    "print(f\"\\n⚠️  Problem: Learning rate is stuck at {history['learning_rates'][-1]:.2e}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all components\n",
    "from data.enhanced_dexycb import EnhancedDexYCBDataset\n",
    "from data.augmentation import DataAugmentor\n",
    "from models.unified_model import UnifiedManipulationTransformer\n",
    "from training.trainer import ManipulationTrainer\n",
    "from training.losses import ComprehensiveLoss\n",
    "from evaluation.evaluator import ComprehensiveEvaluator\n",
    "from debugging.model_debugger import ModelDebugger\n",
    "from solutions.mode_collapse import ModeCollapsePreventionModule\n",
    "from optimizations.data_loading import OptimizedDataLoader\n",
    "from data.gpu_cached_dataset import GPUCachedDataset, create_gpu_cached_dataloaders\n",
    "\n",
    "print(\"All components imported successfully!\")\n",
    "\n",
    "# Create GPU-cached dataloaders (same as original)\n",
    "print(\"\\nLoading GPU-cached datasets...\")\n",
    "gpu_config = {\n",
    "    'gpu_max_samples': 350000,\n",
    "    'gpu_max_samples_val': 20000,\n",
    "    'gpu_cache_path': './gpu_cache_advanced',\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'use_bfloat16': config.training.use_bf16,\n",
    "    'preload_dinov2': False\n",
    "}\n",
    "\n",
    "train_loader, val_loader = create_gpu_cached_dataloaders(gpu_config)\n",
    "print(f\"\\n✓ Datasets loaded:\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset):,}\")\n",
    "print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "# Import all components\n",
    "from data.enhanced_dexycb import EnhancedDexYCBDataset\n",
    "from data.augmentation import DataAugmentor\n",
    "from models.unified_model import UnifiedManipulationTransformer\n",
    "from training.trainer import ManipulationTrainer\n",
    "from training.losses import ComprehensiveLoss\n",
    "from evaluation.evaluator import ComprehensiveEvaluator\n",
    "from debugging.model_debugger import ModelDebugger\n",
    "from solutions.mode_collapse import ModeCollapsePreventionModule\n",
    "from optimizations.data_loading import OptimizedDataLoader\n",
    "from data.gpu_cached_dataset import GPUCachedDataset, create_gpu_cached_dataloaders\n",
    "\n",
    "print(\"All components imported successfully!\")\n",
    "\n",
    "# Create GPU-cached dataloaders (same as original)\n",
    "print(\"\\nLoading GPU-cached datasets...\")\n",
    "gpu_config = {\n",
    "    'gpu_max_samples': 350000,\n",
    "    'gpu_max_samples_val': 20000,\n",
    "    'gpu_cache_path': './gpu_cache_advanced',\n",
    "    'batch_size': config.training.batch_size,\n",
    "    'use_bfloat16': config.training.use_bf16,\n",
    "    'preload_dinov2': False\n",
    "}\n",
    "\n",
    "train_loader, val_loader = create_gpu_cached_dataloaders(gpu_config)\n",
    "print(f\"\\n✓ Datasets loaded:\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset):,}\")\n",
    "print(f\"  GPU Memory: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "from training.trainer import ManipulationTrainer\n",
    "trainer = ManipulationTrainer(\n",
    "    model=model,\n",
    "    config=config.training,\n",
    "    device=device,\n",
    "    distributed=False,\n",
    "    local_rank=0\n",
    ")\n",
    "\n",
    "# Use our configured optimizer and scheduler\n",
    "trainer.optimizer = optimizer\n",
    "trainer.scheduler = scheduler\n",
    "\n",
    "# Create loss function\n",
    "from training.losses import ComprehensiveLoss\n",
    "criterion = ComprehensiveLoss(config.loss)\n",
    "trainer.criterion = criterion\n",
    "\n",
    "# Create evaluator\n",
    "from evaluation.evaluator import ComprehensiveEvaluator\n",
    "evaluator = ComprehensiveEvaluator(config.evaluation)\n",
    "\n",
    "print(\"Training components created successfully\")\n",
    "\n",
    "# Initialize best metrics (we don't have history from checkpoint)\n",
    "best_val_loss = float('inf')\n",
    "best_val_mpjpe = float('inf')\n",
    "print(f\"\\nStarting fresh tracking of best metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Model and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model = UnifiedManipulationTransformer(config.model)\n",
    "\n",
    "# Apply mode collapse prevention\n",
    "if config.optimizations.use_mode_collapse_prevention:\n",
    "    print(\"Applying mode collapse prevention...\")\n",
    "    mode_collapse_config = {\n",
    "        'noise_std': 0.01,\n",
    "        'drop_path_rate': 0.1,\n",
    "        'mixup_alpha': 0.2\n",
    "    }\n",
    "    model = ModeCollapsePreventionModule.wrap_model(model, mode_collapse_config)\n",
    "\n",
    "# Load model weights from checkpoint\n",
    "print(\"\\nLoading model weights from checkpoint...\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel loaded:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final training history\n",
    "import json\n",
    "\n",
    "# Convert history to JSON-serializable format\n",
    "history_json = {}\n",
    "for key, values in history.items():\n",
    "    if isinstance(values, list):\n",
    "        history_json[key] = [float(v) if isinstance(v, (torch.Tensor, np.ndarray)) else v for v in values]\n",
    "    else:\n",
    "        history_json[key] = values\n",
    "\n",
    "with open(f\"{config.output_dir}/training_history.json\", 'w') as f:\n",
    "    json.dump(history_json, f, indent=2)\n",
    "\n",
    "# Create summary plot of continued training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss plot\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['val_loss'], label='Validation Loss')\n",
    "ax.set_xlabel('Epoch (from continuation)')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Validation Loss - Continued Training')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# MPJPE plot\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['val_mpjpe'], label='Validation MPJPE')\n",
    "ax.set_xlabel('Epoch (from continuation)')\n",
    "ax.set_ylabel('MPJPE (mm)')\n",
    "ax.set_title('MPJPE - Continued Training')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Learning rate schedule\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['learning_rates'], 'r-', linewidth=2)\n",
    "ax.set_xlabel('Epoch (from continuation)')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule - Fixed')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "\n",
    "# Training summary\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "Continued Training Summary\n",
    "\n",
    "Started from epoch: {start_epoch}\n",
    "Trained for: {len(history['train_loss'])} additional epochs\n",
    "Total epochs: {start_epoch + len(history['train_loss']) - 1}\n",
    "\n",
    "Results:\n",
    "  Best Val Loss: {min(history['val_loss']) if history['val_loss'] else 'N/A':.4f}\n",
    "  Best Val MPJPE: {min(history['val_mpjpe']) if history['val_mpjpe'] else 'N/A':.2f} mm\n",
    "  Final Learning Rate: {history['learning_rates'][-1] if history['learning_rates'] else 'N/A':.2e}\n",
    "\n",
    "Key Fixes Applied:\n",
    "  ✓ Base learning rate increased to 5e-4\n",
    "  ✓ Proper parameter group configuration\n",
    "  ✓ Learning rate oscillation restored\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "        fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.suptitle('Continued Training Results')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.output_dir}/training_summary.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING CONTINUATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOutputs saved to: {config.output_dir}\")\n",
    "print(f\"Best model: {config.output_dir}/checkpoints/best_model.pth\")\n",
    "print(f\"Best MPJPE model: {config.output_dir}/checkpoints/best_mpjpe_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter groups for multi-rate learning\n",
    "all_params = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        all_params[name] = param\n",
    "\n",
    "# Create mutually exclusive parameter groups\n",
    "dinov2_params = []\n",
    "encoder_params = []\n",
    "decoder_params = []\n",
    "other_params = []\n",
    "\n",
    "for name, param in all_params.items():\n",
    "    if 'dinov2' in name:\n",
    "        dinov2_params.append(param)\n",
    "    elif 'decoder' in name:\n",
    "        decoder_params.append(param)\n",
    "    elif 'encoder' in name:\n",
    "        encoder_params.append(param)\n",
    "    else:\n",
    "        other_params.append(param)\n",
    "\n",
    "# CRITICAL: Use higher base learning rate to avoid getting stuck at eta_min\n",
    "# The original config.training.learning_rate = 1e-4, but we'll use 5e-4 to ensure proper oscillation\n",
    "base_lr = 5e-4  # Increased from 1e-4\n",
    "\n",
    "# Create parameter groups with PROPER initial learning rates\n",
    "param_groups = []\n",
    "\n",
    "if dinov2_params:\n",
    "    param_groups.append({\n",
    "        'params': dinov2_params,\n",
    "        'lr': base_lr * config.training.multi_rate.pretrained,  # 5e-6\n",
    "        'weight_decay': config.training.weight_decay,\n",
    "        'name': 'dinov2'\n",
    "    })\n",
    "\n",
    "if encoder_params:\n",
    "    param_groups.append({\n",
    "        'params': encoder_params,\n",
    "        'lr': base_lr * config.training.multi_rate.new_encoders,  # 2.5e-4\n",
    "        'weight_decay': config.training.weight_decay,\n",
    "        'name': 'encoders'\n",
    "    })\n",
    "\n",
    "if decoder_params:\n",
    "    param_groups.append({\n",
    "        'params': decoder_params,\n",
    "        'lr': base_lr * config.training.multi_rate.decoders,  # 5e-4\n",
    "        'weight_decay': config.training.weight_decay,\n",
    "        'name': 'decoders'\n",
    "    })\n",
    "\n",
    "if other_params:\n",
    "    param_groups.append({\n",
    "        'params': other_params,\n",
    "        'lr': base_lr,  # 5e-4\n",
    "        'weight_decay': config.training.weight_decay,\n",
    "        'name': 'other'\n",
    "    })\n",
    "\n",
    "# Print parameter group summary\n",
    "print(\"Parameter groups with UPDATED learning rates:\")\n",
    "for group in param_groups:\n",
    "    print(f\"  {group['name']}: {len(group['params'])} parameters, lr={group['lr']:.2e}\")\n",
    "\n",
    "# Create optimizer with new learning rates\n",
    "optimizer = torch.optim.AdamW(param_groups, fused=True)\n",
    "\n",
    "# Load optimizer state from checkpoint\n",
    "print(\"\\nLoading optimizer state from checkpoint...\")\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# CRITICAL: Update the learning rates in the optimizer state\n",
    "# This ensures the optimizer uses our new rates, not the old stuck ones\n",
    "print(\"\\nUpdating learning rates in optimizer state...\")\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    old_lr = group['lr']\n",
    "    new_lr = param_groups[i]['lr']\n",
    "    group['lr'] = new_lr\n",
    "    print(f\"  Group {param_groups[i]['name']}: {old_lr:.2e} -> {new_lr:.2e}\")\n",
    "\n",
    "print(\"\\n✓ Optimizer configured with updated learning rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Scheduler with Proper Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduler that will actually oscillate\n",
    "print(\"Creating learning rate scheduler...\")\n",
    "\n",
    "# Option 1: Continue with CosineAnnealingWarmRestarts but adjust for completed epochs\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=config.training.T_0,  # 10 epochs\n",
    "    T_mult=2,\n",
    "    eta_min=config.training.min_lr  # 1e-6\n",
    ")\n",
    "\n",
    "# Fast-forward the scheduler to the correct epoch\n",
    "print(f\"\\nFast-forwarding scheduler to epoch {start_epoch}...\")\n",
    "for _ in range(start_epoch):\n",
    "    scheduler.step()\n",
    "\n",
    "# Option 2 (Alternative): Use OneCycleLR for guaranteed variation\n",
    "# remaining_epochs = config.training.num_epochs - start_epoch\n",
    "# steps_per_epoch = len(train_loader)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer,\n",
    "#     max_lr=[g['lr'] * 10 for g in param_groups],  # Peak at 10x base LR\n",
    "#     total_steps=remaining_epochs * steps_per_epoch,\n",
    "#     pct_start=0.3,  # 30% warmup\n",
    "#     anneal_strategy='cos',\n",
    "#     div_factor=10,  # Start at max_lr/10\n",
    "#     final_div_factor=100  # End at max_lr/100\n",
    "# )\n",
    "\n",
    "# Verify current learning rates\n",
    "print(\"\\nCurrent learning rates after scheduler setup:\")\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    print(f\"  Group {param_groups[i]['name']}: {group['lr']:.2e}\")\n",
    "\n",
    "# Test scheduler behavior\n",
    "print(\"\\nTesting scheduler behavior for next 10 epochs:\")\n",
    "test_optimizer = torch.optim.AdamW(param_groups, fused=True)\n",
    "test_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    test_optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Fast-forward test scheduler\n",
    "for _ in range(start_epoch):\n",
    "    test_scheduler.step()\n",
    "\n",
    "# Check next 10 epochs\n",
    "test_lrs = []\n",
    "for epoch in range(10):\n",
    "    test_lrs.append(test_optimizer.param_groups[0]['lr'])\n",
    "    test_scheduler.step()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(start_epoch, start_epoch + 10), test_lrs, 'o-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Expected Learning Rate for Next 10 Epochs')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Scheduler configured. LR will vary between {min(test_lrs):.2e} and {max(test_lrs):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "from training.trainer import ManipulationTrainer\n",
    "trainer = ManipulationTrainer(\n",
    "    model=model,\n",
    "    config=config.training,\n",
    "    device=device,\n",
    "    distributed=False,\n",
    "    local_rank=0\n",
    ")\n",
    "\n",
    "# Use our configured optimizer and scheduler\n",
    "trainer.optimizer = optimizer\n",
    "trainer.scheduler = scheduler\n",
    "\n",
    "# Create loss function\n",
    "from training.losses import ComprehensiveLoss\n",
    "criterion = ComprehensiveLoss(config.loss)\n",
    "trainer.criterion = criterion\n",
    "\n",
    "# Create evaluator\n",
    "from evaluation.evaluator import ComprehensiveEvaluator\n",
    "evaluator = ComprehensiveEvaluator(config.evaluation)\n",
    "\n",
    "print(\"Training components created successfully\")\n",
    "\n",
    "# Continue with previous best metrics\n",
    "best_val_loss = min(history['val_loss'])\n",
    "best_val_mpjpe = min(history['val_mpjpe'])\n",
    "print(f\"\\nContinuing with best metrics:\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Best val MPJPE: {best_val_mpjpe:.2f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop with LR Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot training progress\n",
    "def plot_training_progress(history, epoch):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(history['train_loss'], label='Train Loss')\n",
    "    ax.plot(history['val_loss'], label='Val Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # MPJPE plot\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(history['train_mpjpe'], label='Train MPJPE')\n",
    "    ax.plot(history['val_mpjpe'], label='Val MPJPE')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MPJPE (mm)')\n",
    "    ax.set_title('Hand Pose Error')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Learning rate plot with emphasis on recent epochs\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(history['learning_rates'], 'b-', alpha=0.5, label='All epochs')\n",
    "    # Highlight recent epochs\n",
    "    if len(history['learning_rates']) > 20:\n",
    "        recent_start = len(history['learning_rates']) - 20\n",
    "        ax.plot(range(recent_start, len(history['learning_rates'])), \n",
    "                history['learning_rates'][recent_start:], 'r-', linewidth=2, label='Recent 20 epochs')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Gradient norm plot\n",
    "    ax = axes[1, 1]\n",
    "    if history['gradient_norms']:\n",
    "        ax.plot(history['gradient_norms'])\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel('Gradient Norm')\n",
    "        ax.set_title('Gradient Norm History')\n",
    "        ax.set_yscale('log')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Training Progress - Epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"Starting training from epoch {start_epoch}...\")\n",
    "print(f\"Training until epoch {config.training.num_epochs}\")\n",
    "print(f\"Remaining epochs: {config.training.num_epochs - start_epoch}\\n\")\n",
    "\n",
    "for epoch in range(start_epoch, config.training.num_epochs):\n",
    "    # Update loss function epoch\n",
    "    criterion.set_epoch(epoch)\n",
    "    \n",
    "    # Log current learning rates\n",
    "    current_lrs = {}\n",
    "    for i, group in enumerate(optimizer.param_groups):\n",
    "        group_name = param_groups[i]['name']\n",
    "        current_lrs[group_name] = group['lr']\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{config.training.num_epochs} - Learning Rates:\")\n",
    "    for name, lr in current_lrs.items():\n",
    "        print(f\"  {name}: {lr:.2e}\")\n",
    "    \n",
    "    # Training epoch\n",
    "    train_metrics = {'loss': 0, 'hand_mpjpe': 0, 'samples': 0}\n",
    "    model.train()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.training.num_epochs} [Train]\")\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # Convert BFloat16 to Float32 for DINOv2\n",
    "        if config.training.use_bf16 and 'image' in batch:\n",
    "            if batch['image'].dtype == torch.bfloat16:\n",
    "                batch['image'] = batch['image'].float()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16) if config.training.use_bf16 else torch.no_grad():\n",
    "            outputs = model(batch)\n",
    "            losses = criterion(outputs, batch)\n",
    "            loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.training.grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Extract metrics\n",
    "        loss_value = loss.item()\n",
    "        mpjpe_value = 0\n",
    "        if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "            with torch.no_grad():\n",
    "                mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                mpjpe_value = mpjpe.item() * 1000  # Convert to mm\n",
    "        \n",
    "        # Update metrics\n",
    "        batch_size = batch['image'].shape[0]\n",
    "        train_metrics['samples'] += batch_size\n",
    "        train_metrics['loss'] += loss_value * batch_size\n",
    "        train_metrics['hand_mpjpe'] += mpjpe_value * batch_size\n",
    "        \n",
    "        # Log gradient norms periodically\n",
    "        if batch_idx % 100 == 0:\n",
    "            grad_norm = 0\n",
    "            for p in model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "            grad_norm = grad_norm ** 0.5\n",
    "            history['gradient_norms'].append(grad_norm)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss_value:.4f}',\n",
    "            'mpjpe': f'{mpjpe_value:.1f}mm',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "    \n",
    "    # Average training metrics\n",
    "    train_metrics['loss'] /= train_metrics['samples']\n",
    "    train_metrics['hand_mpjpe'] /= train_metrics['samples']\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = {'loss': 0, 'hand_mpjpe': 0, 'samples': 0}\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config.training.num_epochs} [Val]\"):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            # Convert BFloat16 to Float32\n",
    "            if config.training.use_bf16 and 'image' in batch:\n",
    "                if batch['image'].dtype == torch.bfloat16:\n",
    "                    batch['image'] = batch['image'].float()\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.amp.autocast('cuda', dtype=torch.bfloat16) if config.training.use_bf16 else torch.no_grad():\n",
    "                outputs = model(batch)\n",
    "                losses = criterion(outputs, batch)\n",
    "                loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "            \n",
    "            batch_size = batch['image'].shape[0]\n",
    "            val_metrics['samples'] += batch_size\n",
    "            val_metrics['loss'] += loss.item() * batch_size\n",
    "            \n",
    "            if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                val_metrics['hand_mpjpe'] += mpjpe.item() * 1000 * batch_size\n",
    "    \n",
    "    # Average validation metrics\n",
    "    val_metrics['loss'] /= val_metrics['samples']\n",
    "    val_metrics['hand_mpjpe'] /= val_metrics['samples']\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['train_mpjpe'].append(train_metrics['hand_mpjpe'])\n",
    "    history['val_mpjpe'].append(val_metrics['hand_mpjpe'])\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_loss': best_val_loss,\n",
    "            'config': config\n",
    "        }, f\"{config.output_dir}/checkpoints/best_model.pth\")\n",
    "        print(f\"  ✓ New best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    if val_metrics['hand_mpjpe'] < best_val_mpjpe:\n",
    "        best_val_mpjpe = val_metrics['hand_mpjpe']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_mpjpe': best_val_mpjpe,\n",
    "            'config': config\n",
    "        }, f\"{config.output_dir}/checkpoints/best_mpjpe_model.pth\")\n",
    "        print(f\"  ✓ New best MPJPE: {best_val_mpjpe:.2f} mm\")\n",
    "    \n",
    "    # Regular checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'history': history,\n",
    "            'config': config\n",
    "        }, f\"{config.output_dir}/checkpoints/checkpoint_epoch_{epoch+1}.pth\")\n",
    "    \n",
    "    # Step scheduler AFTER validation\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.training.num_epochs} Summary:\")\n",
    "    print(f\"  Train - Loss: {train_metrics['loss']:.4f}, MPJPE: {train_metrics['hand_mpjpe']:.2f}mm\")\n",
    "    print(f\"  Val   - Loss: {val_metrics['loss']:.4f}, MPJPE: {val_metrics['hand_mpjpe']:.2f}mm\")\n",
    "    print(f\"  Best  - Loss: {best_val_loss:.4f}, MPJPE: {best_val_mpjpe:.2f}mm\")\n",
    "    \n",
    "    # Verify learning rate is changing\n",
    "    lr_change = history['learning_rates'][-1] - history['learning_rates'][-2] if len(history['learning_rates']) > 1 else 0\n",
    "    print(f\"  LR Change: {lr_change:.2e} (current: {history['learning_rates'][-1]:.2e})\")\n",
    "    \n",
    "    # Update live plot\n",
    "    clear_output(wait=True)\n",
    "    fig = plot_training_progress(history, epoch + 1)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save plot\n",
    "    fig.savefig(f\"{config.output_dir}/training_progress.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final best MPJPE: {best_val_mpjpe:.2f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final training history\n",
    "import json\n",
    "\n",
    "# Convert history to JSON-serializable format\n",
    "history_json = {}\n",
    "for key, values in history.items():\n",
    "    if isinstance(values, list):\n",
    "        history_json[key] = [float(v) if isinstance(v, (torch.Tensor, np.ndarray)) else v for v in values]\n",
    "    else:\n",
    "        history_json[key] = values\n",
    "\n",
    "with open(f\"{config.output_dir}/training_history.json\", 'w') as f:\n",
    "    json.dump(history_json, f, indent=2)\n",
    "\n",
    "# Create comparison plot of old vs new training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss comparison\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['val_loss'], label='Validation Loss')\n",
    "ax.axvline(x=start_epoch-1, color='r', linestyle='--', label='Resume Point')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Validation Loss Progress')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# MPJPE comparison\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['val_mpjpe'], label='Validation MPJPE')\n",
    "ax.axvline(x=start_epoch-1, color='r', linestyle='--', label='Resume Point')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MPJPE (mm)')\n",
    "ax.set_title('MPJPE Progress')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Learning rate - full history\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['learning_rates'][:start_epoch], 'b-', alpha=0.5, label='Original Training')\n",
    "ax.plot(range(start_epoch-1, len(history['learning_rates'])), \n",
    "        history['learning_rates'][start_epoch-1:], 'r-', linewidth=2, label='Continued Training')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule Comparison')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Improvement summary\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "# Calculate improvements\n",
    "pre_resume_loss = history['val_loss'][start_epoch-1]\n",
    "post_resume_loss = min(history['val_loss'][start_epoch:])\n",
    "loss_improvement = (pre_resume_loss - post_resume_loss) / pre_resume_loss * 100\n",
    "\n",
    "pre_resume_mpjpe = history['val_mpjpe'][start_epoch-1]\n",
    "post_resume_mpjpe = min(history['val_mpjpe'][start_epoch:])\n",
    "mpjpe_improvement = (pre_resume_mpjpe - post_resume_mpjpe) / pre_resume_mpjpe * 100\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "Training Continuation Summary\n",
    "\n",
    "Resumed from epoch: {start_epoch}\n",
    "Trained until epoch: {len(history['train_loss'])}\n",
    "\n",
    "Before Resume (Epoch {start_epoch}):\n",
    "  Val Loss: {pre_resume_loss:.4f}\n",
    "  Val MPJPE: {pre_resume_mpjpe:.2f} mm\n",
    "  Learning Rate: {history['learning_rates'][start_epoch-1]:.2e}\n",
    "\n",
    "After Continued Training:\n",
    "  Best Val Loss: {post_resume_loss:.4f} ({loss_improvement:.1f}% improvement)\n",
    "  Best Val MPJPE: {post_resume_mpjpe:.2f} mm ({mpjpe_improvement:.1f}% improvement)\n",
    "  Final Learning Rate: {history['learning_rates'][-1]:.2e}\n",
    "\n",
    "Key Fix Applied:\n",
    "  ✓ Updated base learning rate from 1e-4 to 5e-4\n",
    "  ✓ Properly initialized scheduler with new LRs\n",
    "  ✓ Learning rate now oscillates as expected\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "        fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.suptitle('Training Continuation Analysis')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.output_dir}/continuation_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING CONTINUATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOutputs saved to: {config.output_dir}\")\n",
    "print(f\"Best model: {config.output_dir}/checkpoints/best_model.pth\")\n",
    "print(f\"Best MPJPE model: {config.output_dir}/checkpoints/best_mpjpe_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}