{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Manipulation Transformer - Barebones Debug Version\n",
    "\n",
    "This notebook provides a minimal implementation without advanced features for easier debugging:\n",
    "- No FlashAttention, FP8, or memory optimizations\n",
    "- No mode collapse prevention modules\n",
    "- Simple data loading without prefetching\n",
    "- Basic loss functions only\n",
    "- Small dataset subset\n",
    "- CPU-friendly options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set DEX_YCB_DIR environment variable\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Simple matplotlib setup\n",
    "%matplotlib inline\n",
    "\n",
    "# Check environment\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimal Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple configuration dictionary\n",
    "config = {\n",
    "    # Data\n",
    "    'data_root': '../../dex-ycb-toolkit',\n",
    "    'batch_size': 4,  # Small batch size\n",
    "    'num_samples': 1000,  # Use only 1000 samples for debugging\n",
    "    'num_workers': 0,  # No multiprocessing for debugging\n",
    "    \n",
    "    # Model - smaller sizes for debugging\n",
    "    'hidden_dim': 256,  # Much smaller than production\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 4,  # Fewer layers\n",
    "    'dropout': 0.1,\n",
    "    'num_refinement_steps': 1,  # Minimal refinement\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 5,  # Just a few epochs\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 0.01,\n",
    "    'print_freq': 10,\n",
    "    \n",
    "    # No optimizations\n",
    "    'use_amp': False,\n",
    "    'use_ema': False,\n",
    "    'gradient_checkpointing': False,\n",
    "    \n",
    "    # Output\n",
    "    'output_dir': 'outputs/debug_run'\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config['output_dir'], exist_ok=True)\n",
    "print(f\"Output directory: {config['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Dataset (No Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import minimal dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from data.enhanced_dexycb import EnhancedDexYCBDataset\n",
    "\n",
    "# Create simple dataset wrapper that limits samples\n",
    "class DebugDataset(Dataset):\n",
    "    def __init__(self, base_dataset, max_samples=1000):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.max_samples = min(max_samples, len(base_dataset))\n",
    "        print(f\"Using {self.max_samples} samples from {len(base_dataset)} total\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.max_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.base_dataset[idx]\n",
    "\n",
    "# Create datasets without augmentation\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset_full = EnhancedDexYCBDataset(\n",
    "    dexycb_root=config['data_root'],\n",
    "    split='train',  # Changed from 's0_train' to 'train'\n",
    "    sequence_length=1,\n",
    "    augment=False  # No augmentation for debugging\n",
    ")\n",
    "\n",
    "val_dataset_full = EnhancedDexYCBDataset(\n",
    "    dexycb_root=config['data_root'],\n",
    "    split='val',  # Changed from 's0_val' to 'val'\n",
    "    sequence_length=1,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Wrap with debug dataset\n",
    "train_dataset = DebugDataset(train_dataset_full, config['num_samples'])\n",
    "val_dataset = DebugDataset(val_dataset_full, config['num_samples'] // 10)\n",
    "\n",
    "# Simple dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=False  # Disable for debugging\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data format\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"Sample batch contents:\")\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} ({value.dtype}) - device: {value.device}\")\n",
    "        # Only compute statistics for floating point tensors\n",
    "        if value.dtype in [torch.float32, torch.float64, torch.float16, torch.bfloat16]:\n",
    "            print(f\"    min: {value.min():.3f}, max: {value.max():.3f}, mean: {value.mean():.3f}\")\n",
    "        else:\n",
    "            # For integer tensors, just show min/max\n",
    "            print(f\"    min: {value.min()}, max: {value.max()}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simplified Model (No Advanced Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified version of the model for debugging\n",
    "class SimpleManipulationModel(nn.Module):\n",
    "    \"\"\"Simplified model without advanced features for debugging\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = config['hidden_dim']\n",
    "        self.num_heads = config['num_heads']\n",
    "        self.num_layers = config['num_layers']\n",
    "        \n",
    "        # Simple patch embedding (no DINOv2)\n",
    "        self.patch_embed = nn.Conv2d(3, self.hidden_dim, kernel_size=16, stride=16)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 196, self.hidden_dim) * 0.02)\n",
    "        \n",
    "        # Simple transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.hidden_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=self.hidden_dim * 4,\n",
    "            dropout=config['dropout'],\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=self.num_layers)\n",
    "        \n",
    "        # Simple output heads\n",
    "        self.hand_head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config['dropout']),\n",
    "            nn.Linear(self.hidden_dim, 21 * 3)  # 21 joints x 3D\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Get image\n",
    "        x = batch['image']  # [B, 3, H, W]\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)  # [B, hidden_dim, H/16, W/16]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, hidden_dim]\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.encoder(x)  # [B, num_patches, hidden_dim]\n",
    "        \n",
    "        # Global pooling\n",
    "        x = x.mean(dim=1)  # [B, hidden_dim]\n",
    "        \n",
    "        # Predict hand joints\n",
    "        hand_joints = self.hand_head(x)  # [B, 21*3]\n",
    "        hand_joints = hand_joints.view(B, 21, 3)  # [B, 21, 3]\n",
    "        \n",
    "        # Simple output dictionary\n",
    "        outputs = {\n",
    "            'hand_joints': hand_joints,\n",
    "            'features': x  # For debugging\n",
    "        }\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Create model\n",
    "print(\"Creating simplified model...\")\n",
    "model = SimpleManipulationModel(config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "print(\"Testing forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch_gpu = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                       for k, v in sample_batch.items()}\n",
    "    outputs = model(sample_batch_gpu)\n",
    "    \n",
    "print(\"Output shapes:\")\n",
    "for key, value in outputs.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape}\")\n",
    "        print(f\"    min: {value.min():.3f}, max: {value.max():.3f}, mean: {value.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MPJPE loss\n",
    "class SimpleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        losses = {}\n",
    "        \n",
    "        # Initialize pred_joints to None\n",
    "        pred_joints = None\n",
    "        \n",
    "        # Hand joint loss (MPJPE)\n",
    "        if 'hand_joints' in outputs and 'hand_joints' in targets:\n",
    "            pred_joints = outputs['hand_joints']\n",
    "            gt_joints = targets['hand_joints']\n",
    "            \n",
    "            # Simple L2 loss\n",
    "            joint_loss = F.mse_loss(pred_joints, gt_joints)\n",
    "            losses['joint_mse'] = joint_loss\n",
    "            \n",
    "            # MPJPE for monitoring\n",
    "            with torch.no_grad():\n",
    "                mpjpe = torch.norm(pred_joints - gt_joints, dim=-1).mean()\n",
    "                losses['mpjpe'] = mpjpe\n",
    "        \n",
    "        # Simple diversity loss to prevent collapse\n",
    "        if pred_joints is not None and pred_joints.shape[0] > 1:\n",
    "            # Variance of predictions\n",
    "            pred_std = pred_joints.std(dim=0).mean()\n",
    "            diversity_loss = torch.relu(0.01 - pred_std)  # Penalize if std < 0.01\n",
    "            losses['diversity'] = diversity_loss * 0.1  # Small weight\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = sum(losses.values())\n",
    "        losses['total'] = total_loss\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# Create loss function\n",
    "criterion = SimpleLoss()\n",
    "print(\"Loss function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simple Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                            lr=config['learning_rate'],\n",
    "                            weight_decay=config['weight_decay'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['num_epochs']\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: Adam with lr={config['learning_rate']}\")\n",
    "print(f\"Scheduler: CosineAnnealingLR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_mpjpe': [],\n",
    "    'val_loss': [],\n",
    "    'val_mpjpe': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mpjpe = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(loader, desc=\"Training\")):\n",
    "        # Move to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        losses = criterion(outputs, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses['total'].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += losses['total'].item()\n",
    "        if 'mpjpe' in losses:\n",
    "            total_mpjpe += losses['mpjpe'].item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % config['print_freq'] == 0:\n",
    "            print(f\"  Batch {batch_idx}/{len(loader)}: \"\n",
    "                  f\"Loss={losses['total'].item():.4f}, \"\n",
    "                  f\"MPJPE={losses.get('mpjpe', 0).item():.2f}mm\")\n",
    "    \n",
    "    return total_loss / num_batches, total_mpjpe / num_batches\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mpjpe = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(batch)\n",
    "            losses = criterion(outputs, batch)\n",
    "            \n",
    "            total_loss += losses['total'].item()\n",
    "            if 'mpjpe' in losses:\n",
    "                total_mpjpe += losses['mpjpe'].item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_mpjpe / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"\\nStarting training for {config['num_epochs']} epochs...\\n\")\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_mpjpe = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mpjpe = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mpjpe'].append(train_mpjpe)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mpjpe'].append(val_mpjpe)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, MPJPE: {train_mpjpe:.2f}mm\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, MPJPE: {val_mpjpe:.2f}mm\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'config': config\n",
    "    }\n",
    "    torch.save(checkpoint, f\"{config['output_dir']}/checkpoint_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss plot\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train')\n",
    "ax.plot(history['val_loss'], label='Val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# MPJPE plot\n",
    "ax = axes[1]\n",
    "ax.plot(history['train_mpjpe'], label='Train')\n",
    "ax.plot(history['val_mpjpe'], label='Val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MPJPE (mm)')\n",
    "ax.set_title('Mean Per Joint Position Error')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Learning rate plot\n",
    "ax = axes[2]\n",
    "ax.plot(history['lr'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/training_curves.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best Train MPJPE: {min(history['train_mpjpe']):.2f}mm\")\n",
    "print(f\"  Best Val MPJPE: {min(history['val_mpjpe']):.2f}mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Debug Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check prediction diversity\n",
    "print(\"Checking prediction diversity...\")\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "num_batches = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        \n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        all_predictions.append(outputs['hand_joints'].cpu())\n",
    "\n",
    "# Concatenate predictions\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "print(f\"Total predictions: {all_predictions.shape}\")\n",
    "\n",
    "# Compute statistics\n",
    "pred_mean = all_predictions.mean(dim=0)\n",
    "pred_std = all_predictions.std(dim=0)\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"  Mean std across joints: {pred_std.mean():.6f}\")\n",
    "print(f\"  Min std: {pred_std.min():.6f}\")\n",
    "print(f\"  Max std: {pred_std.max():.6f}\")\n",
    "\n",
    "# Visualize prediction diversity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Std per joint\n",
    "ax = axes[0]\n",
    "joint_std = pred_std.mean(dim=1)  # Average over xyz\n",
    "ax.bar(range(21), joint_std)\n",
    "ax.set_xlabel('Joint Index')\n",
    "ax.set_ylabel('Std Dev')\n",
    "ax.set_title('Prediction Diversity per Joint')\n",
    "ax.grid(True, axis='y')\n",
    "\n",
    "# Distribution of predictions\n",
    "ax = axes[1]\n",
    "ax.hist(all_predictions.flatten().numpy(), bins=50, alpha=0.7, density=True)\n",
    "ax.set_xlabel('Predicted Value')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of All Predictions')\n",
    "ax.grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for mode collapse\n",
    "if pred_std.mean() < 0.001:\n",
    "    print(\"\\n⚠️ WARNING: Possible mode collapse detected! Predictions have very low diversity.\")\n",
    "else:\n",
    "    print(\"\\n✓ Good prediction diversity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "print(\"Visualizing predictions...\")\n",
    "model.eval()\n",
    "\n",
    "# Get one batch\n",
    "vis_batch = next(iter(val_loader))\n",
    "vis_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "            for k, v in vis_batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(vis_batch)\n",
    "\n",
    "# Visualize first 4 samples\n",
    "num_vis = min(4, vis_batch['image'].shape[0])\n",
    "fig, axes = plt.subplots(num_vis, 2, figsize=(8, 4*num_vis))\n",
    "if num_vis == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(num_vis):\n",
    "    # Input image\n",
    "    ax = axes[i, 0]\n",
    "    img = vis_batch['image'][i].cpu().numpy().transpose(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Sample {i+1} - Input\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Predictions vs GT\n",
    "    ax = axes[i, 1]\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Plot predicted joints (red)\n",
    "    pred_joints = outputs['hand_joints'][i].cpu().numpy()\n",
    "    # Simple projection assuming normalized coordinates\n",
    "    pred_2d = pred_joints[:, :2] * 112 + 112\n",
    "    ax.scatter(pred_2d[:, 0], pred_2d[:, 1], c='red', s=30, alpha=0.8, label='Pred')\n",
    "    \n",
    "    # Plot GT joints (green)\n",
    "    if 'hand_joints' in vis_batch:\n",
    "        gt_joints = vis_batch['hand_joints'][i].cpu().numpy()\n",
    "        gt_2d = gt_joints[:, :2] * 112 + 112\n",
    "        ax.scatter(gt_2d[:, 0], gt_2d[:, 1], c='green', s=30, alpha=0.8, label='GT')\n",
    "        \n",
    "        # Compute error\n",
    "        mpjpe = np.mean(np.linalg.norm(pred_joints - gt_joints, axis=1))\n",
    "        ax.set_title(f\"Sample {i+1} - MPJPE: {mpjpe:.1f}mm\")\n",
    "    else:\n",
    "        ax.set_title(f\"Sample {i+1} - Predictions\")\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/predictions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Debug Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gradient flow\n",
    "print(\"Analyzing gradient flow...\")\n",
    "\n",
    "# Do one forward-backward pass\n",
    "model.train()\n",
    "batch = next(iter(train_loader))\n",
    "batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "        for k, v in batch.items()}\n",
    "\n",
    "outputs = model(batch)\n",
    "losses = criterion(outputs, batch)\n",
    "losses['total'].backward()\n",
    "\n",
    "# Check gradients\n",
    "grad_norms = {}\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.norm().item()\n",
    "        grad_norms[name] = grad_norm\n",
    "\n",
    "# Print gradient statistics\n",
    "print(f\"\\nGradient statistics:\")\n",
    "grad_values = list(grad_norms.values())\n",
    "print(f\"  Mean gradient norm: {np.mean(grad_values):.6f}\")\n",
    "print(f\"  Max gradient norm: {np.max(grad_values):.6f}\")\n",
    "print(f\"  Min gradient norm: {np.min(grad_values):.6f}\")\n",
    "\n",
    "# Check for vanishing/exploding gradients\n",
    "vanishing = sum(1 for v in grad_values if v < 1e-6)\n",
    "exploding = sum(1 for v in grad_values if v > 100)\n",
    "print(f\"\\n  Parameters with vanishing gradients (<1e-6): {vanishing}/{len(grad_values)}\")\n",
    "print(f\"  Parameters with exploding gradients (>100): {exploding}/{len(grad_values)}\")\n",
    "\n",
    "# Plot gradient norms\n",
    "plt.figure(figsize=(12, 4))\n",
    "names = list(grad_norms.keys())\n",
    "values = list(grad_norms.values())\n",
    "plt.bar(range(len(names)), values)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Gradient Norm (log scale)')\n",
    "plt.title('Gradient Norms by Layer')\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clear gradients\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dead neurons\n",
    "print(\"\\nChecking for dead neurons...\")\n",
    "\n",
    "# Get activations\n",
    "model.eval()\n",
    "activations = {}\n",
    "\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on key layers\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        hook = module.register_forward_hook(hook_fn(name))\n",
    "        hooks.append(hook)\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    _ = model(batch)\n",
    "\n",
    "# Analyze activations\n",
    "for name, act in activations.items():\n",
    "    if act.dim() > 1:\n",
    "        # Check for dead neurons (always zero)\n",
    "        dead_neurons = (act.abs().max(dim=0)[0] < 1e-6).float().mean().item()\n",
    "        print(f\"{name}: {dead_neurons*100:.1f}% dead neurons\")\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "print(\"\\nDebug analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate debug report\n",
    "report = f\"\"\"\n",
    "# Debug Training Report\n",
    "\n",
    "## Configuration\n",
    "- Model: Simplified transformer\n",
    "- Hidden dim: {config['hidden_dim']}\n",
    "- Layers: {config['num_layers']}\n",
    "- Batch size: {config['batch_size']}\n",
    "- Learning rate: {config['learning_rate']}\n",
    "- Epochs: {config['num_epochs']}\n",
    "- Training samples: {len(train_dataset)}\n",
    "- Validation samples: {len(val_dataset)}\n",
    "\n",
    "## Results\n",
    "- Final train MPJPE: {history['train_mpjpe'][-1]:.2f}mm\n",
    "- Final val MPJPE: {history['val_mpjpe'][-1]:.2f}mm\n",
    "- Best val MPJPE: {min(history['val_mpjpe']):.2f}mm\n",
    "- Prediction diversity (std): {pred_std.mean():.6f}\n",
    "\n",
    "## Gradient Analysis\n",
    "- Mean gradient norm: {np.mean(grad_values):.6f}\n",
    "- Parameters with vanishing gradients: {vanishing}\n",
    "- Parameters with exploding gradients: {exploding}\n",
    "\n",
    "## Issues Found\n",
    "\"\"\"\n",
    "\n",
    "# Add any issues\n",
    "if pred_std.mean() < 0.001:\n",
    "    report += \"- ⚠️ Mode collapse detected\\n\"\n",
    "if vanishing > len(grad_values) * 0.5:\n",
    "    report += \"- ⚠️ Many vanishing gradients\\n\"\n",
    "if exploding > 0:\n",
    "    report += \"- ⚠️ Some exploding gradients\\n\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(f\"{config['output_dir']}/debug_report.txt\", 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {config['output_dir']}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
