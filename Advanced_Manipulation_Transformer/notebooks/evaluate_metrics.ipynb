{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DexYCB Model Evaluation with Standard Metrics\n",
    "\n",
    "This notebook evaluates a trained model on the DexYCB test set and computes:\n",
    "- **MPJPE**: Mean Per Joint Position Error in millimeters\n",
    "- **PA-MPJPE**: Procrustes-aligned MPJPE after optimal rigid alignment\n",
    "- **PCK**: Percentage of Correct Keypoints within various thresholds\n",
    "- **AUC**: Area Under Curve for PCK from 0-50mm\n",
    "- **Diversity**: Standard deviation of predictions to measure mode collapse\n",
    "\n",
    "## Key Updates Applied:\n",
    "1. **Proper checkpoint loading**: Extracts model config from checkpoint and handles mode collapse wrapper\n",
    "2. **BFloat16 handling**: Converts BFloat16 images to Float32 for DINOv2 compatibility\n",
    "3. **Camera parameter fixes**: Ensures camera_params dictionary is properly formatted\n",
    "4. **Robust model loading**: Handles both wrapped and unwrapped model architectures\n",
    "5. **Checkpoint info display**: Shows training history and best metrics from checkpoint\n",
    "\n",
    "Based on the train_full_featured notebook implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to Python path\n",
    "parent_dir = os.path.dirname(os.path.dirname(os.path.abspath('./')))\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Import our modules - use full paths from project root\n",
    "from Advanced_Manipulation_Transformer.data.enhanced_dexycb import EnhancedDexYCBDataset\n",
    "from Advanced_Manipulation_Transformer.models.unified_model import UnifiedManipulationTransformer\n",
    "from Advanced_Manipulation_Transformer.evaluation.evaluator import ComprehensiveEvaluator\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    # Dataset settings\n",
    "    'dex_ycb_dir': os.environ.get('DEX_YCB_DIR', '/home/n231/231nProjectV2/dex-ycb'),\n",
    "    'test_split': 's0_val',  # DexYCB doesn't have 'test', use 'val' for evaluation\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    \n",
    "    # Model checkpoint\n",
    "    'checkpoint_path': '/home/n231/231nProjectV2/Advanced_Manipulation_Transformer/notebooks/outputs/full_featured/checkpoints/checkpoint_epoch_50.pth',  # Update with your model path\n",
    "    \n",
    "    # Evaluation settings\n",
    "    'save_results': True,\n",
    "    'results_dir': 'evaluation_results',\n",
    "    'visualize_samples': 10,  # Number of samples to visualize\n",
    "    \n",
    "    # Metric thresholds\n",
    "    'pck_thresholds': [5, 10, 15, 20, 30, 40, 50],  # mm\n",
    "    'auc_min': 0,\n",
    "    'auc_max': 50,\n",
    "}\n",
    "\n",
    "# Create results directory\n",
    "if config['save_results']:\n",
    "    os.makedirs(config['results_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "print(f\"Loading DexYCB test split: {config['test_split']}\")\n",
    "\n",
    "test_dataset = EnhancedDexYCBDataset(\n",
    "    split=config['test_split'],\n",
    "    dexycb_root=config['dex_ycb_dir'],  # Changed from dex_ycb_dir to dexycb_root\n",
    "    use_cache=True,\n",
    "    augment=False,  # No augmentation for testing\n",
    "    sequence_length=1  # Single frame evaluation\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test set size: {len(test_dataset)} samples\")\n",
    "print(f\"Number of batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "\n",
    "# Load checkpoint first to extract config\n",
    "if os.path.exists(config['checkpoint_path']):\n",
    "    checkpoint = torch.load(config['checkpoint_path'], map_location=device)\n",
    "    \n",
    "    # Extract model config from checkpoint\n",
    "    if 'config' in checkpoint:\n",
    "        # Full config available\n",
    "        full_config = checkpoint['config']\n",
    "        if 'model' in full_config:\n",
    "            model_config = full_config['model']\n",
    "        else:\n",
    "            model_config = full_config\n",
    "        print(\"Using model config from checkpoint\")\n",
    "    else:\n",
    "        # Use default config\n",
    "        print(\"No config found in checkpoint, using default config\")\n",
    "        model_config = {\n",
    "            'hidden_dim': 1024,\n",
    "            'contact_hidden_dim': 512,\n",
    "            'use_mano_vertices': True,\n",
    "            'use_sigma_reparam': True,\n",
    "            'use_attention_fusion': True,\n",
    "            'num_refinement_steps': 2,\n",
    "            'max_objects': 10,\n",
    "            'num_object_classes': 100,\n",
    "            'num_contact_points': 10,\n",
    "            'dropout': 0.15,\n",
    "            'freeze_layers': 12\n",
    "        }\n",
    "    \n",
    "    # Initialize model with correct config\n",
    "    model = UnifiedManipulationTransformer(model_config)\n",
    "    \n",
    "    # Check if model was wrapped with mode collapse prevention\n",
    "    mode_collapse_wrapped = False\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        # Check if state dict has mode collapse wrapper prefix\n",
    "        state_dict_keys = list(checkpoint['model_state_dict'].keys())\n",
    "        if state_dict_keys and ('base_model.' in state_dict_keys[0] or '_orig_mod.base_model.' in state_dict_keys[0]):\n",
    "            mode_collapse_wrapped = True\n",
    "    \n",
    "    if mode_collapse_wrapped or 'mode_collapse_wrapped' in checkpoint:\n",
    "        from Advanced_Manipulation_Transformer.solutions.mode_collapse import ModeCollapsePreventionModule\n",
    "        mode_collapse_config = checkpoint.get('mode_collapse_config', {\n",
    "            'noise_std': 0.01,\n",
    "            'drop_path_rate': 0.1,\n",
    "            'mixup_alpha': 0.2\n",
    "        })\n",
    "        model = ModeCollapsePreventionModule.wrap_model(model, mode_collapse_config)\n",
    "        print(\"Applied mode collapse prevention wrapper\")\n",
    "    \n",
    "    # Load model weights\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Handle compiled model state dict (_orig_mod prefix)\n",
    "        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "            print(\"Detected compiled model checkpoint, removing _orig_mod prefix...\")\n",
    "            # Remove _orig_mod prefix\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if k.startswith('_orig_mod.'):\n",
    "                    new_state_dict[k[10:]] = v  # Remove '_orig_mod.' prefix\n",
    "                else:\n",
    "                    new_state_dict[k] = v\n",
    "            state_dict = new_state_dict\n",
    "        \n",
    "        # Now try to load the state dict\n",
    "        try:\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "            print(\"Loaded model weights successfully (strict mode)\")\n",
    "        except RuntimeError as e:\n",
    "            # Try non-strict loading\n",
    "            missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "            print(f\"Loaded model weights (non-strict mode)\")\n",
    "            if missing_keys:\n",
    "                print(f\"  Missing keys: {len(missing_keys)} keys\")\n",
    "                print(f\"  First 5: {missing_keys[:5]}\")\n",
    "            if unexpected_keys:\n",
    "                print(f\"  Unexpected keys: {len(unexpected_keys)} keys\")\n",
    "                print(f\"  First 5: {unexpected_keys[:5]}\")\n",
    "                    \n",
    "    elif 'ema_state_dict' in checkpoint:\n",
    "        # Use EMA weights if available (usually better for evaluation)\n",
    "        print(\"Using EMA weights for evaluation\")\n",
    "        state_dict = checkpoint['ema_state_dict']\n",
    "        \n",
    "        # Handle compiled model state dict\n",
    "        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "            print(\"Detected compiled model checkpoint, removing _orig_mod prefix...\")\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if k.startswith('_orig_mod.'):\n",
    "                    new_state_dict[k[10:]] = v\n",
    "                else:\n",
    "                    new_state_dict[k] = v\n",
    "            state_dict = new_state_dict\n",
    "        \n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        # Direct checkpoint (just state dict)\n",
    "        state_dict = checkpoint\n",
    "        \n",
    "        # Handle compiled model state dict\n",
    "        if any(k.startswith('_orig_mod.') for k in state_dict.keys()):\n",
    "            print(\"Detected compiled model checkpoint, removing _orig_mod prefix...\")\n",
    "            new_state_dict = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if k.startswith('_orig_mod.'):\n",
    "                    new_state_dict[k[10:]] = v\n",
    "                else:\n",
    "                    new_state_dict[k] = v\n",
    "            state_dict = new_state_dict\n",
    "            \n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    print(f\"Loaded checkpoint from: {config['checkpoint_path']}\")\n",
    "    \n",
    "    # Print checkpoint info if available\n",
    "    if 'epoch' in checkpoint:\n",
    "        print(f\"Checkpoint epoch: {checkpoint['epoch']}\")\n",
    "    if 'history' in checkpoint:\n",
    "        history = checkpoint['history']\n",
    "        if 'val_mpjpe' in history and history['val_mpjpe']:\n",
    "            print(f\"Checkpoint best val MPJPE: {min(history['val_mpjpe']):.2f} mm\")\n",
    "    if 'best_val_mpjpe' in checkpoint:\n",
    "        print(f\"Best validation MPJPE: {checkpoint['best_val_mpjpe']:.2f} mm\")\n",
    "    if 'best_val_loss' in checkpoint:\n",
    "        print(f\"Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {config['checkpoint_path']}\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Mode collapse wrapped: {mode_collapse_wrapped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluation functions for detailed metrics\n",
    "\n",
    "def compute_mpjpe(pred_joints, gt_joints):\n",
    "    \"\"\"Compute Mean Per Joint Position Error\"\"\"\n",
    "    # pred_joints, gt_joints: [B, 21, 3]\n",
    "    errors = torch.norm(pred_joints - gt_joints, dim=-1)  # [B, 21]\n",
    "    return errors\n",
    "\n",
    "def procrustes_align(pred, gt):\n",
    "    \"\"\"Align predicted pose to ground truth using Procrustes analysis\"\"\"\n",
    "    # Center the points\n",
    "    pred_centered = pred - pred.mean(axis=0)\n",
    "    gt_centered = gt - gt.mean(axis=0)\n",
    "    \n",
    "    # Compute optimal rotation\n",
    "    H = pred_centered.T @ gt_centered\n",
    "    U, _, Vt = np.linalg.svd(H)\n",
    "    R_opt = Vt.T @ U.T\n",
    "    \n",
    "    # Handle reflection\n",
    "    if np.linalg.det(R_opt) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R_opt = Vt.T @ U.T\n",
    "    \n",
    "    # Apply transformation\n",
    "    aligned = pred_centered @ R_opt.T\n",
    "    aligned += gt.mean(axis=0)\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "def compute_pa_mpjpe(pred_joints, gt_joints):\n",
    "    \"\"\"Compute Procrustes-aligned MPJPE\"\"\"\n",
    "    pa_errors = []\n",
    "    \n",
    "    for i in range(pred_joints.shape[0]):\n",
    "        pred_np = pred_joints[i].cpu().numpy()\n",
    "        gt_np = gt_joints[i].cpu().numpy()\n",
    "        \n",
    "        # Align prediction to ground truth\n",
    "        aligned_pred = procrustes_align(pred_np, gt_np)\n",
    "        \n",
    "        # Compute error\n",
    "        error = np.mean(np.linalg.norm(aligned_pred - gt_np, axis=-1))\n",
    "        pa_errors.append(error)\n",
    "    \n",
    "    return np.array(pa_errors)\n",
    "\n",
    "def compute_pck(errors, thresholds):\n",
    "    \"\"\"Compute Percentage of Correct Keypoints for different thresholds\"\"\"\n",
    "    pck_values = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Check how many predictions are within threshold\n",
    "        correct = (errors < threshold).float().mean()\n",
    "        pck_values[f'PCK@{threshold}mm'] = correct.item() * 100\n",
    "    \n",
    "    return pck_values\n",
    "\n",
    "def compute_auc(errors, min_threshold=0, max_threshold=50, num_steps=100):\n",
    "    \"\"\"Compute Area Under Curve for PCK metric\"\"\"\n",
    "    thresholds = np.linspace(min_threshold, max_threshold, num_steps)\n",
    "    pck_values = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pck = (errors < threshold).float().mean().item()\n",
    "        pck_values.append(pck)\n",
    "    \n",
    "    # Compute AUC using trapezoidal rule\n",
    "    auc = np.trapz(pck_values, thresholds) / (max_threshold - min_threshold)\n",
    "    \n",
    "    return auc, thresholds, pck_values\n",
    "\n",
    "def compute_diversity(predictions):\n",
    "    \"\"\"Compute prediction diversity (std deviation)\"\"\"\n",
    "    # predictions: List of [B, 21, 3] tensors\n",
    "    all_preds = torch.cat(predictions, dim=0)  # [N, 21, 3]\n",
    "    \n",
    "    # Compute std deviation per joint and average (convert to mm)\n",
    "    std_per_joint = torch.std(all_preds, dim=0) * 1000  # [21, 3] in mm\n",
    "    mean_std = std_per_joint.mean().item()\n",
    "    \n",
    "    # Also compute overall std\n",
    "    overall_std = torch.std(all_preds.reshape(-1, 63), dim=0).mean().item() * 1000\n",
    "    \n",
    "    return {\n",
    "        'mean_std_per_joint': mean_std,\n",
    "        'overall_std': overall_std,\n",
    "        'std_per_joint': std_per_joint.cpu().numpy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "print(\"Starting evaluation...\")\n",
    "\n",
    "# Storage for metrics\n",
    "all_mpjpe = []\n",
    "all_pa_mpjpe = []\n",
    "all_predictions = []\n",
    "per_joint_errors = {i: [] for i in range(21)}\n",
    "\n",
    "# Joint names for visualization\n",
    "joint_names = [\n",
    "    'Wrist', 'Thumb_1', 'Thumb_2', 'Thumb_3', 'Thumb_tip',\n",
    "    'Index_1', 'Index_2', 'Index_3', 'Index_tip',\n",
    "    'Middle_1', 'Middle_2', 'Middle_3', 'Middle_tip',\n",
    "    'Ring_1', 'Ring_2', 'Ring_3', 'Ring_tip',\n",
    "    'Pinky_1', 'Pinky_2', 'Pinky_3', 'Pinky_tip'\n",
    "]\n",
    "\n",
    "# Helper function to fix batch data types (from train_full_featured)\n",
    "def fix_batch_for_model(batch, model):\n",
    "    '''Fix dtype issues in batch, especially for camera parameters'''\n",
    "    # Get model dtype\n",
    "    model_dtype = next(model.parameters()).dtype\n",
    "    \n",
    "    # Fix camera intrinsics if present\n",
    "    if 'camera_intrinsics' in batch and isinstance(batch['camera_intrinsics'], torch.Tensor):\n",
    "        batch['camera_intrinsics'] = batch['camera_intrinsics'].to(model_dtype)\n",
    "    \n",
    "    # Create camera_params dict if needed\n",
    "    if 'camera_params' not in batch and 'camera_intrinsics' in batch:\n",
    "        batch['camera_params'] = {'intrinsics': batch['camera_intrinsics']}\n",
    "    \n",
    "    # Fix camera_params dictionary\n",
    "    if 'camera_params' in batch and isinstance(batch['camera_params'], dict):\n",
    "        fixed_params = {}\n",
    "        for key, value in batch['camera_params'].items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                fixed_params[key] = value.to(model_dtype)\n",
    "            else:\n",
    "                fixed_params[key] = value\n",
    "        batch['camera_params'] = fixed_params\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                for k, v in batch.items()}\n",
    "        \n",
    "        # CRITICAL FIX: Convert BFloat16 images to Float32 for DINOv2 (from train_full_featured)\n",
    "        if 'image' in batch and batch['image'].dtype == torch.bfloat16:\n",
    "            batch['image'] = batch['image'].float()\n",
    "        \n",
    "        # Fix camera parameters\n",
    "        batch = fix_batch_for_model(batch, model)\n",
    "        \n",
    "        # Forward pass\n",
    "        try:\n",
    "            outputs = model(batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in forward pass: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract predictions and ground truth\n",
    "        # Use refined joints if available, otherwise use coarse predictions\n",
    "        if 'hand_joints' in outputs:\n",
    "            pred_joints = outputs['hand_joints']  # [B, 21, 3]\n",
    "        elif 'hand' in outputs and 'joints_3d_refined' in outputs['hand']:\n",
    "            pred_joints = outputs['hand']['joints_3d_refined']\n",
    "        elif 'hand' in outputs and 'joints_3d' in outputs['hand']:\n",
    "            pred_joints = outputs['hand']['joints_3d']\n",
    "        else:\n",
    "            print(\"Warning: No hand joint predictions found in model output\")\n",
    "            continue\n",
    "            \n",
    "        # Get ground truth - handle different possible keys\n",
    "        if 'hand_joints_3d' in batch:\n",
    "            gt_joints = batch['hand_joints_3d']   # [B, 21, 3]\n",
    "        elif 'hand_joints' in batch:\n",
    "            gt_joints = batch['hand_joints']\n",
    "        else:\n",
    "            print(\"Warning: No ground truth hand joints found in batch\")\n",
    "            continue\n",
    "        \n",
    "        # Store predictions for diversity\n",
    "        all_predictions.append(pred_joints.cpu())\n",
    "        \n",
    "        # Compute MPJPE (convert to mm)\n",
    "        mpjpe_errors = compute_mpjpe(pred_joints, gt_joints) * 1000  # [B, 21] in mm\n",
    "        all_mpjpe.extend(mpjpe_errors.mean(dim=1).cpu().numpy())  # Mean over joints\n",
    "        \n",
    "        # Store per-joint errors\n",
    "        for joint_idx in range(21):\n",
    "            per_joint_errors[joint_idx].extend(\n",
    "                mpjpe_errors[:, joint_idx].cpu().numpy()\n",
    "            )\n",
    "        \n",
    "        # Compute PA-MPJPE (convert to mm)\n",
    "        pa_mpjpe = compute_pa_mpjpe(pred_joints, gt_joints) * 1000\n",
    "        all_pa_mpjpe.extend(pa_mpjpe)\n",
    "\n",
    "# Convert to arrays\n",
    "all_mpjpe = np.array(all_mpjpe)\n",
    "all_pa_mpjpe = np.array(all_pa_mpjpe)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "print(f\"Evaluated {len(all_mpjpe)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute all metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. MPJPE\n",
    "mpjpe_mean = np.mean(all_mpjpe)\n",
    "mpjpe_std = np.std(all_mpjpe)\n",
    "print(f\"\\nMPJPE: {mpjpe_mean:.2f} ± {mpjpe_std:.2f} mm\")\n",
    "\n",
    "# 2. PA-MPJPE\n",
    "pa_mpjpe_mean = np.mean(all_pa_mpjpe)\n",
    "pa_mpjpe_std = np.std(all_pa_mpjpe)\n",
    "print(f\"PA-MPJPE: {pa_mpjpe_mean:.2f} ± {pa_mpjpe_std:.2f} mm\")\n",
    "\n",
    "# 3. PCK at different thresholds\n",
    "all_errors_tensor = torch.tensor(all_mpjpe)\n",
    "pck_results = compute_pck(all_errors_tensor, config['pck_thresholds'])\n",
    "print(\"\\nPCK Results:\")\n",
    "for threshold, pck in pck_results.items():\n",
    "    print(f\"  {threshold}: {pck:.2f}%\")\n",
    "\n",
    "# 4. AUC\n",
    "auc, auc_thresholds, auc_pck_values = compute_auc(\n",
    "    all_errors_tensor, \n",
    "    config['auc_min'], \n",
    "    config['auc_max']\n",
    ")\n",
    "print(f\"\\nAUC (0-50mm): {auc:.3f}\")\n",
    "\n",
    "# 5. Diversity\n",
    "diversity_metrics = compute_diversity(all_predictions)\n",
    "print(f\"\\nDiversity Metrics:\")\n",
    "print(f\"  Mean STD per joint: {diversity_metrics['mean_std_per_joint']:.4f} mm\")\n",
    "print(f\"  Overall STD: {diversity_metrics['overall_std']:.4f} mm\")\n",
    "\n",
    "# Check for mode collapse\n",
    "if diversity_metrics['mean_std_per_joint'] < 0.5:\n",
    "    print(\"  ⚠️ WARNING: Low diversity detected - possible mode collapse!\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-joint analysis\n",
    "print(\"\\nPer-Joint MPJPE Analysis:\")\n",
    "joint_mpjpe = {}\n",
    "for joint_idx in range(21):\n",
    "    joint_errors = np.array(per_joint_errors[joint_idx])\n",
    "    joint_mpjpe[joint_idx] = np.mean(joint_errors)\n",
    "    print(f\"  {joint_names[joint_idx]:12s}: {joint_mpjpe[joint_idx]:.2f} mm\")\n",
    "\n",
    "# Find worst performing joints\n",
    "worst_joints = sorted(joint_mpjpe.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\nWorst performing joints:\")\n",
    "for joint_idx, error in worst_joints:\n",
    "    print(f\"  {joint_names[joint_idx]:12s}: {error:.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: PCK Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(auc_thresholds, np.array(auc_pck_values) * 100, 'b-', linewidth=2)\n",
    "plt.fill_between(auc_thresholds, 0, np.array(auc_pck_values) * 100, alpha=0.3)\n",
    "plt.xlabel('Distance Threshold (mm)', fontsize=12)\n",
    "plt.ylabel('PCK (%)', fontsize=12)\n",
    "plt.title(f'PCK Curve (AUC = {auc:.3f})', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(config['auc_min'], config['auc_max'])\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add specific threshold markers\n",
    "for threshold in config['pck_thresholds']:\n",
    "    if threshold <= config['auc_max']:\n",
    "        pck_val = pck_results[f'PCK@{threshold}mm']\n",
    "        plt.plot(threshold, pck_val, 'ro', markersize=8)\n",
    "        plt.text(threshold, pck_val + 2, f'{pck_val:.1f}%', \n",
    "                ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "if config['save_results']:\n",
    "    plt.savefig(os.path.join(config['results_dir'], 'pck_curve.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Per-Joint Error Heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot of per-joint errors\n",
    "joint_errors_list = [joint_mpjpe[i] for i in range(21)]\n",
    "colors = ['red' if i in [4, 8, 12, 16, 20] else 'blue' for i in range(21)]  # Fingertips in red\n",
    "\n",
    "bars = ax1.bar(range(21), joint_errors_list, color=colors)\n",
    "ax1.set_xticks(range(21))\n",
    "ax1.set_xticklabels(joint_names, rotation=45, ha='right')\n",
    "ax1.set_ylabel('MPJPE (mm)', fontsize=12)\n",
    "ax1.set_title('Per-Joint MPJPE', fontsize=14)\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add mean line\n",
    "ax1.axhline(y=mpjpe_mean, color='green', linestyle='--', label=f'Mean: {mpjpe_mean:.2f}mm')\n",
    "ax1.legend()\n",
    "\n",
    "# Diversity visualization\n",
    "std_per_joint = diversity_metrics['std_per_joint']\n",
    "std_magnitude = np.linalg.norm(std_per_joint, axis=1)\n",
    "\n",
    "bars2 = ax2.bar(range(21), std_magnitude, color='green')\n",
    "ax2.set_xticks(range(21))\n",
    "ax2.set_xticklabels(joint_names, rotation=45, ha='right')\n",
    "ax2.set_ylabel('Prediction STD (mm)', fontsize=12)\n",
    "ax2.set_title('Per-Joint Prediction Diversity', fontsize=14)\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Add warning line for low diversity\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', label='Low diversity threshold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "if config['save_results']:\n",
    "    plt.savefig(os.path.join(config['results_dir'], 'per_joint_analysis.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Error Distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# MPJPE distribution\n",
    "axes[0, 0].hist(all_mpjpe, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0, 0].axvline(mpjpe_mean, color='red', linestyle='--', label=f'Mean: {mpjpe_mean:.2f}')\n",
    "axes[0, 0].set_xlabel('MPJPE (mm)')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('MPJPE Distribution')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PA-MPJPE distribution\n",
    "axes[0, 1].hist(all_pa_mpjpe, bins=50, density=True, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].axvline(pa_mpjpe_mean, color='red', linestyle='--', label=f'Mean: {pa_mpjpe_mean:.2f}')\n",
    "axes[0, 1].set_xlabel('PA-MPJPE (mm)')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('PA-MPJPE Distribution')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# MPJPE vs PA-MPJPE scatter\n",
    "axes[1, 0].scatter(all_mpjpe, all_pa_mpjpe, alpha=0.5, s=1)\n",
    "axes[1, 0].plot([0, max(all_mpjpe)], [0, max(all_mpjpe)], 'r--', label='y=x')\n",
    "axes[1, 0].set_xlabel('MPJPE (mm)')\n",
    "axes[1, 0].set_ylabel('PA-MPJPE (mm)')\n",
    "axes[1, 0].set_title('MPJPE vs PA-MPJPE')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative error distribution\n",
    "sorted_mpjpe = np.sort(all_mpjpe)\n",
    "cumulative = np.arange(1, len(sorted_mpjpe) + 1) / len(sorted_mpjpe) * 100\n",
    "\n",
    "axes[1, 1].plot(sorted_mpjpe, cumulative, 'b-', linewidth=2)\n",
    "axes[1, 1].set_xlabel('MPJPE (mm)')\n",
    "axes[1, 1].set_ylabel('Cumulative Percentage (%)')\n",
    "axes[1, 1].set_title('Cumulative Error Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentile markers\n",
    "percentiles = [50, 75, 90, 95]\n",
    "for p in percentiles:\n",
    "    val = np.percentile(all_mpjpe, p)\n",
    "    axes[1, 1].axvline(val, color='red', linestyle=':', alpha=0.5)\n",
    "    axes[1, 1].text(val + 1, p - 5, f'{p}%: {val:.1f}mm', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "if config['save_results']:\n",
    "    plt.savefig(os.path.join(config['results_dir'], 'error_distributions.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions (optional)\n",
    "if config['visualize_samples'] > 0:\n",
    "    print(f\"\\nVisualizing {config['visualize_samples']} sample predictions...\")\n",
    "    \n",
    "    # Get a few samples\n",
    "    vis_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=True\n",
    "    )\n",
    "    \n",
    "    fig, axes = plt.subplots(2, min(5, config['visualize_samples']), \n",
    "                            figsize=(4*min(5, config['visualize_samples']), 8))\n",
    "    \n",
    "    if config['visualize_samples'] == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    elif min(5, config['visualize_samples']) == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(vis_loader):\n",
    "            if idx >= config['visualize_samples']:\n",
    "                break\n",
    "            \n",
    "            # Get predictions\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in batch.items()}\n",
    "            \n",
    "            # Fix batch for model (BFloat16 conversion and camera params)\n",
    "            batch = fix_batch_for_model(batch, model)\n",
    "            \n",
    "            outputs = model(batch)\n",
    "            \n",
    "            # Extract data - use 'image' key\n",
    "            if 'image' in batch:\n",
    "                image = batch['image'][0].cpu()\n",
    "            else:\n",
    "                print(f\"Warning: No image found in batch, skipping visualization {idx}\")\n",
    "                continue\n",
    "            \n",
    "            # Handle image format (could be CHW or HWC)\n",
    "            if image.dim() == 3 and image.shape[0] in [3, 4]:  # CHW format\n",
    "                image = image.permute(1, 2, 0).numpy()\n",
    "            else:\n",
    "                image = image.numpy()\n",
    "                \n",
    "            # Extract predictions\n",
    "            if 'hand_joints' in outputs:\n",
    "                pred_joints = outputs['hand_joints'][0].cpu().numpy()\n",
    "            elif 'hand' in outputs and 'joints_3d_refined' in outputs['hand']:\n",
    "                pred_joints = outputs['hand']['joints_3d_refined'][0].cpu().numpy()\n",
    "            elif 'hand' in outputs and 'joints_3d' in outputs['hand']:\n",
    "                pred_joints = outputs['hand']['joints_3d'][0].cpu().numpy()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Get ground truth 3D joints first\n",
    "            if 'hand_joints_3d' in batch:\n",
    "                gt_3d = batch['hand_joints_3d'][0].cpu().numpy()\n",
    "            elif 'hand_joints' in batch:\n",
    "                gt_3d = batch['hand_joints'][0].cpu().numpy()\n",
    "            else:\n",
    "                print(f\"Warning: No ground truth joints found, skipping visualization {idx}\")\n",
    "                continue\n",
    "                \n",
    "            # Project to 2D for visualization\n",
    "            if 'camera_intrinsics' in batch:\n",
    "                K = batch['camera_intrinsics'][0].cpu().numpy()\n",
    "                pred_2d = pred_joints @ K.T\n",
    "                pred_2d = pred_2d[:, :2] / pred_2d[:, 2:3]\n",
    "            else:\n",
    "                pred_2d = pred_joints[:, :2] * 1000  # Just use x,y, scale to image space\n",
    "                \n",
    "            # Get ground truth 2D joints\n",
    "            if 'hand_joints_2d' in batch:\n",
    "                gt_2d = batch['hand_joints_2d'][0].cpu().numpy()\n",
    "            else:\n",
    "                # Project 3D to 2D\n",
    "                if 'camera_intrinsics' in batch:\n",
    "                    gt_2d = gt_3d @ K.T\n",
    "                    gt_2d = gt_2d[:, :2] / gt_2d[:, 2:3]\n",
    "                else:\n",
    "                    gt_2d = gt_3d[:, :2] * 1000\n",
    "            \n",
    "            # Denormalize image if needed\n",
    "            if image.max() <= 1.0:\n",
    "                image = np.clip(image, 0, 1)\n",
    "            else:\n",
    "                image = np.clip(image / 255.0, 0, 1)\n",
    "            \n",
    "            # Plot\n",
    "            col = idx % 5\n",
    "            \n",
    "            # Original image with GT\n",
    "            axes[0, col].imshow(image)\n",
    "            axes[0, col].scatter(gt_2d[:, 0], gt_2d[:, 1], c='green', s=20, label='GT')\n",
    "            axes[0, col].set_title(f'Ground Truth')\n",
    "            axes[0, col].axis('off')\n",
    "            \n",
    "            # Prediction\n",
    "            axes[1, col].imshow(image)\n",
    "            axes[1, col].scatter(pred_2d[:, 0], pred_2d[:, 1], c='red', s=20, label='Pred')\n",
    "            \n",
    "            # Compute error for this sample\n",
    "            mpjpe_3d = np.mean(np.linalg.norm(pred_joints - gt_3d, axis=-1)) * 1000\n",
    "            axes[1, col].set_title(f'Pred (MPJPE: {mpjpe_3d:.1f}mm)')\n",
    "            axes[1, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if config['save_results']:\n",
    "        plt.savefig(os.path.join(config['results_dir'], 'sample_predictions.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {config['checkpoint_path']}\")\n",
    "print(f\"Test Set: {config['test_split']} ({len(test_dataset)} samples)\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  - MPJPE: {mpjpe_mean:.2f} ± {mpjpe_std:.2f} mm\")\n",
    "print(f\"  - PA-MPJPE: {pa_mpjpe_mean:.2f} ± {pa_mpjpe_std:.2f} mm\")\n",
    "print(f\"  - PCK@20mm: {pck_results['PCK@20mm']:.2f}%\")\n",
    "print(f\"  - AUC (0-50mm): {auc:.3f}\")\n",
    "print(f\"  - Diversity: {diversity_metrics['mean_std_per_joint']:.4f} mm\")\n",
    "\n",
    "# Performance assessment\n",
    "print(\"\\nPerformance Assessment:\")\n",
    "if mpjpe_mean < 20:\n",
    "    print(\"  ✅ Excellent performance (MPJPE < 20mm)\")\n",
    "elif mpjpe_mean < 50:\n",
    "    print(\"  ✓ Good performance (MPJPE < 50mm)\")\n",
    "elif mpjpe_mean < 100:\n",
    "    print(\"  ⚠️ Moderate performance (MPJPE < 100mm)\")\n",
    "else:\n",
    "    print(\"  ❌ Poor performance (MPJPE > 100mm)\")\n",
    "\n",
    "if diversity_metrics['mean_std_per_joint'] < 0.5:\n",
    "    print(\"  ❌ Mode collapse detected (low diversity)\")\n",
    "else:\n",
    "    print(\"  ✅ Good prediction diversity\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}