{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Manipulation Transformer - W&B Hyperparameter Sweep\n",
    "\n",
    "This notebook implements a Weights & Biases sweep for hyperparameter optimization, focusing on:\n",
    "- Augmentation parameters\n",
    "- Dropout\n",
    "- Learning rate scheduler\n",
    "- Loss weights\n",
    "- Diversity margin\n",
    "- Per-joint weighting\n",
    "- Fingertip weight\n",
    "- Learning rate\n",
    "\n",
    "Configuration:\n",
    "- 7 epochs per run\n",
    "- 20,000 training samples\n",
    "- 2,000 validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'val/hand_mpjpe',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        # Batch size\n",
    "        'batch_size': {\n",
    "            'values': [32,64,128,256]\n",
    "        },\n",
    "        \n",
    "        # Dropout\n",
    "        'dropout': {\n",
    "            'values': [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "        },\n",
    "        \n",
    "        # Scheduler type\n",
    "        'scheduler_type': {\n",
    "            'values': ['cosine', 'cosine_warmup', 'step', 'exponential']\n",
    "        },\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        'aug_rotation_range': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 5.0,\n",
    "            'max': 15.0\n",
    "        },\n",
    "        'aug_scale_min': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.7,\n",
    "            'max': 0.9\n",
    "        },\n",
    "        'aug_scale_max': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1.1,\n",
    "            'max': 1.3\n",
    "        },\n",
    "        'aug_translation_std': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.02,\n",
    "            'max': 0.1\n",
    "        },\n",
    "        'aug_color_jitter': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.1,\n",
    "            'max': 0.3\n",
    "        },\n",
    "        'aug_joint_noise_std': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.002,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        \n",
    "        # Loss weights\n",
    "        'loss_weight_hand_coarse': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.8,\n",
    "            'max': 1.2\n",
    "        },\n",
    "        'loss_weight_hand_refined': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1.0,\n",
    "            'max': 1.5\n",
    "        },\n",
    "        'loss_weight_object_position': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.8,\n",
    "            'max': 1.2\n",
    "        },\n",
    "        'loss_weight_object_rotation': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.3,\n",
    "            'max': 0.7\n",
    "        },\n",
    "        'loss_weight_contact': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.2,\n",
    "            'max': 0.5\n",
    "        },\n",
    "        'loss_weight_physics': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.05,\n",
    "            'max': 0.2\n",
    "        },\n",
    "        'loss_weight_diversity': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 0.005,\n",
    "            'max': 0.05\n",
    "        },\n",
    "        'loss_weight_reprojection': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.3,\n",
    "            'max': 0.7\n",
    "        },\n",
    "        \n",
    "        # Loss configuration\n",
    "        'diversity_margin': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.005,\n",
    "            'max': 0.02\n",
    "        },\n",
    "        'per_joint_weighting': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'fingertip_weight': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 1.2,\n",
    "            'max': 2.0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sweep configuration defined:\")\n",
    "print(f\"Method: {sweep_config['method']}\")\n",
    "print(f\"Metric: {sweep_config['metric']['name']} ({sweep_config['metric']['goal']})\")\n",
    "print(f\"Number of parameters: {len(sweep_config['parameters'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Initialize wandb\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # Load base configuration\n",
    "    config = OmegaConf.load('../configs/default_config.yaml')\n",
    "    \n",
    "    # Override with sweep parameters\n",
    "    # Learning rate, batch size, and dropout\n",
    "    config.training.batch_size = wandb.config.batch_size  # Now from sweep\n",
    "    config.model.dropout = wandb.config.dropout\n",
    "    \n",
    "    # Augmentation parameters\n",
    "    config.data.augmentation.rotation_range = wandb.config.aug_rotation_range\n",
    "    config.data.augmentation.scale_range = [\n",
    "        wandb.config.aug_scale_min,\n",
    "        wandb.config.aug_scale_max\n",
    "    ]\n",
    "    config.data.augmentation.translation_std = wandb.config.aug_translation_std\n",
    "    config.data.augmentation.color_jitter = wandb.config.aug_color_jitter\n",
    "    config.data.augmentation.joint_noise_std = wandb.config.aug_joint_noise_std\n",
    "    \n",
    "    # Loss weights\n",
    "    config.loss.loss_weights.hand_coarse = wandb.config.loss_weight_hand_coarse\n",
    "    config.loss.loss_weights.hand_refined = wandb.config.loss_weight_hand_refined\n",
    "    config.loss.loss_weights.object_position = wandb.config.loss_weight_object_position\n",
    "    config.loss.loss_weights.object_rotation = wandb.config.loss_weight_object_rotation\n",
    "    config.loss.loss_weights.contact = wandb.config.loss_weight_contact\n",
    "    config.loss.loss_weights.physics = wandb.config.loss_weight_physics\n",
    "    config.loss.loss_weights.diversity = wandb.config.loss_weight_diversity\n",
    "    config.loss.loss_weights.reprojection = wandb.config.loss_weight_reprojection\n",
    "    \n",
    "    # Loss configuration\n",
    "    config.loss.diversity_margin = wandb.config.diversity_margin\n",
    "    config.loss.per_joint_weighting = wandb.config.per_joint_weighting\n",
    "    config.loss.fingertip_weight = wandb.config.fingertip_weight\n",
    "    \n",
    "    # Fixed parameters for sweep\n",
    "    config.training.num_epochs = 7  # Fixed at 7 epochs\n",
    "    config.training.use_wandb = True\n",
    "    config.training.use_amp = True\n",
    "    config.training.use_bf16 = True\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Import components\n",
    "    from models.unified_model import UnifiedManipulationTransformer\n",
    "    from training.losses import ComprehensiveLoss\n",
    "    from data.gpu_cached_dataset import create_gpu_cached_dataloaders\n",
    "    from solutions.mode_collapse import ModeCollapsePreventionModule\n",
    "    from optimizations.pytorch_native_optimization import PyTorchNativeOptimizer\n",
    "    \n",
    "    try:\n",
    "        # Create GPU-cached dataloaders with fixed sizes\n",
    "        print(f\"Creating dataloaders with 20k train, 2k val samples, batch_size={config.training.batch_size}...\")\n",
    "        gpu_config = {\n",
    "            'gpu_max_samples': 20000,      # Fixed at 20k\n",
    "            'gpu_max_samples_val': 2000,   # Fixed at 2k\n",
    "            'gpu_cache_path': './gpu_cache_sweep',\n",
    "            'batch_size': config.training.batch_size,  # Now using sweep parameter\n",
    "            'use_bfloat16': config.training.use_bf16,\n",
    "            'preload_dinov2': False\n",
    "        }\n",
    "        \n",
    "        train_loader, val_loader = create_gpu_cached_dataloaders(gpu_config)\n",
    "        print(f\"Dataloaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n",
    "        \n",
    "        # Create model\n",
    "        model = UnifiedManipulationTransformer(config.model)\n",
    "        \n",
    "        # Apply mode collapse prevention\n",
    "        mode_collapse_config = {\n",
    "            'noise_std': 0.01,\n",
    "            'drop_path_rate': 0.1,\n",
    "            'mixup_alpha': 0.2\n",
    "        }\n",
    "        model = ModeCollapsePreventionModule.wrap_model(model, mode_collapse_config)\n",
    "        \n",
    "        # Initialize weights\n",
    "        def init_weights(module):\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                if hasattr(module, 'weight') and module.weight is not None:\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.01)\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if 'dinov2' not in name or 'encoder.layer.' not in name:\n",
    "                init_weights(module)\n",
    "        \n",
    "        # Apply optimizations (DISABLE torch.compile to avoid errors)\n",
    "        native_optimizer = PyTorchNativeOptimizer()\n",
    "        model = native_optimizer.optimize_model(model, {'use_compile': False})\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer with parameter groups\n",
    "        dinov2_params = []\n",
    "        encoder_params = []\n",
    "        decoder_params = []\n",
    "        other_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if 'dinov2' in name:\n",
    "                dinov2_params.append(param)\n",
    "            elif 'decoder' in name:\n",
    "                decoder_params.append(param)\n",
    "            elif 'encoder' in name:\n",
    "                encoder_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        param_groups = []\n",
    "        if dinov2_params:\n",
    "            param_groups.append({\n",
    "                'params': dinov2_params,\n",
    "                'lr': config.training.learning_rate * 0.01,\n",
    "                'name': 'dinov2'\n",
    "            })\n",
    "        if encoder_params:\n",
    "            param_groups.append({\n",
    "                'params': encoder_params,\n",
    "                'lr': config.training.learning_rate * 0.5,\n",
    "                'name': 'encoders'\n",
    "            })\n",
    "        if decoder_params:\n",
    "            param_groups.append({\n",
    "                'params': decoder_params,\n",
    "                'lr': config.training.learning_rate,\n",
    "                'name': 'decoders'\n",
    "            })\n",
    "        if other_params:\n",
    "            param_groups.append({\n",
    "                'params': other_params,\n",
    "                'lr': config.training.learning_rate,\n",
    "                'name': 'other'\n",
    "            })\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=0.01, fused=True)\n",
    "        \n",
    "        # Create scheduler based on sweep parameter\n",
    "        scheduler_type = wandb.config.scheduler_type\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=config.training.num_epochs, eta_min=1e-6\n",
    "            )\n",
    "        elif scheduler_type == 'cosine_warmup':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "        elif scheduler_type == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=3, gamma=0.5\n",
    "            )\n",
    "        elif scheduler_type == 'exponential':\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=0.9\n",
    "            )\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = ComprehensiveLoss(config.loss)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_mpjpe = float('inf')\n",
    "        \n",
    "        for epoch in range(config.training.num_epochs):\n",
    "            # Update loss epoch\n",
    "            criterion.set_epoch(epoch)\n",
    "            \n",
    "            # Train\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_mpjpe = 0\n",
    "            train_samples = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                # Convert BFloat16 images to Float32 for DINOv2\n",
    "                if batch['image'].dtype == torch.bfloat16:\n",
    "                    batch['image'] = batch['image'].float()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    outputs = model(batch)\n",
    "                    losses = criterion(outputs, batch)\n",
    "                    loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_size = batch['image'].shape[0]\n",
    "                train_loss += loss.item() * batch_size\n",
    "                train_samples += batch_size\n",
    "                \n",
    "                # Calculate MPJPE\n",
    "                if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                    with torch.no_grad():\n",
    "                        mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                        train_mpjpe += mpjpe.item() * 1000 * batch_size\n",
    "                \n",
    "                # Log batch metrics\n",
    "                if batch_idx % 20 == 0:\n",
    "                    wandb.log({\n",
    "                        'train/batch_loss': loss.item(),\n",
    "                        'train/batch_mpjpe': mpjpe.item() * 1000 if 'hand_joints' in outputs else 0,\n",
    "                        'train/lr': optimizer.param_groups[0]['lr'],\n",
    "                        'system/gpu_memory_gb': torch.cuda.memory_allocated() / 1e9\n",
    "                    })\n",
    "            \n",
    "            # Average training metrics\n",
    "            train_loss /= train_samples\n",
    "            train_mpjpe /= train_samples\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_mpjpe = 0\n",
    "            val_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    if batch['image'].dtype == torch.bfloat16:\n",
    "                        batch['image'] = batch['image'].float()\n",
    "                    \n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        outputs = model(batch)\n",
    "                        losses = criterion(outputs, batch)\n",
    "                        loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "                    \n",
    "                    batch_size = batch['image'].shape[0]\n",
    "                    val_samples += batch_size\n",
    "                    val_loss += loss.item() * batch_size\n",
    "                    \n",
    "                    if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                        mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                        val_mpjpe += mpjpe.item() * 1000 * batch_size\n",
    "            \n",
    "            # Average validation metrics\n",
    "            val_loss /= val_samples\n",
    "            val_mpjpe /= val_samples\n",
    "            \n",
    "            # Update best\n",
    "            if val_mpjpe < best_val_mpjpe:\n",
    "                best_val_mpjpe = val_mpjpe\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train/loss': train_loss,\n",
    "                'train/hand_mpjpe': train_mpjpe,\n",
    "                'val/loss': val_loss,\n",
    "                'val/hand_mpjpe': val_mpjpe,\n",
    "                'val/best_mpjpe': best_val_mpjpe\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{config.training.num_epochs}: \"\n",
    "                  f\"train_loss={train_loss:.4f}, train_mpjpe={train_mpjpe:.2f}mm, \"\n",
    "                  f\"val_loss={val_loss:.4f}, val_mpjpe={val_mpjpe:.2f}mm\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        wandb.log({\n",
    "            'final/best_val_mpjpe': best_val_mpjpe,\n",
    "            'final/epochs_trained': config.training.num_epochs\n",
    "        })\n",
    "        \n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        wandb.log({'error': str(e), 'val/hand_mpjpe': 1000.0})\n",
    "        raise\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize and Run Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sweep\n",
    "project_name = 'amt-hyperparameter-sweep'\n",
    "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
    "\n",
    "print(f\"Sweep initialized!\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(f\"View at: https://wandb.ai/{wandb.api.default_entity}/{project_name}/sweeps/{sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sweep agent\n",
    "# You can adjust count to run more or fewer experiments\n",
    "sweep_count = 50  # Number of experiments to run\n",
    "\n",
    "print(f\"Starting sweep agent to run {sweep_count} experiments...\")\n",
    "print(\"Each experiment will run for 7 epochs with 20k training samples\")\n",
    "print(\"This will take approximately 10-15 minutes per experiment\")\n",
    "\n",
    "wandb.agent(sweep_id, function=train, count=sweep_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After sweep completes, analyze results\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"{wandb.api.default_entity}/{project_name}/sweeps/{sweep_id}\")\n",
    "\n",
    "# Get best run\n",
    "best_run = sweep.best_run()\n",
    "print(f\"Best run: {best_run.name}\")\n",
    "print(f\"Best validation MPJPE: {best_run.summary['val/hand_mpjpe']:.2f} mm\")\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for param, value in best_run.config.items():\n",
    "    if param.startswith('_'):\n",
    "        continue\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Get all runs\n",
    "runs = sweep.runs\n",
    "data = []\n",
    "\n",
    "for run in runs:\n",
    "    if 'val/hand_mpjpe' in run.summary:\n",
    "        config = dict(run.config)\n",
    "        config['val_mpjpe'] = run.summary['val/hand_mpjpe']\n",
    "        data.append(config)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot correlation of key parameters with performance\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "key_params = ['learning_rate', 'batch_size', 'dropout', 'loss_weight_diversity', \n",
    "              'aug_rotation_range', 'diversity_margin', 'fingertip_weight',\n",
    "              'scheduler_type', 'per_joint_weighting']\n",
    "\n",
    "for i, param in enumerate(key_params):\n",
    "    if param in df.columns:\n",
    "        ax = axes[i]\n",
    "        if param in ['scheduler_type', 'per_joint_weighting']:\n",
    "            # Categorical parameters\n",
    "            param_values = df[param].unique()\n",
    "            mpjpe_by_param = []\n",
    "            for val in param_values:\n",
    "                mpjpe_by_param.append(df[df[param] == val]['val_mpjpe'].values)\n",
    "            ax.boxplot(mpjpe_by_param, labels=param_values)\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_ylabel('Validation MPJPE (mm)')\n",
    "            ax.set_title(f'{param} vs Performance')\n",
    "        else:\n",
    "            # Continuous parameters\n",
    "            ax.scatter(df[param], df['val_mpjpe'], alpha=0.6)\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_ylabel('Validation MPJPE (mm)')\n",
    "            ax.set_title(f'{param} vs Performance')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Hyperparameter Impact on Performance', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total runs: {len(df)}\")\n",
    "print(f\"Best MPJPE: {df['val_mpjpe'].min():.2f} mm\")\n",
    "print(f\"Worst MPJPE: {df['val_mpjpe'].max():.2f} mm\")\n",
    "print(f\"Mean MPJPE: {df['val_mpjpe'].mean():.2f} mm\")\n",
    "print(f\"Std MPJPE: {df['val_mpjpe'].std():.2f} mm\")\n",
    "\n",
    "# Print best batch size analysis\n",
    "if 'batch_size' in df.columns:\n",
    "    print(\"\\nBatch Size Analysis:\")\n",
    "    batch_size_stats = df.groupby('batch_size')['val_mpjpe'].agg(['mean', 'std', 'min', 'count'])\n",
    "    print(batch_size_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Best Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export best configuration for future use\n",
    "best_config = dict(best_run.config)\n",
    "\n",
    "# Create optimized configuration file\n",
    "optimized_config = OmegaConf.load('../configs/default_config.yaml')\n",
    "\n",
    "# Update with best parameters\n",
    "optimized_config.training.learning_rate = best_config['learning_rate']\n",
    "optimized_config.model.dropout = best_config['dropout']\n",
    "\n",
    "# Update augmentation\n",
    "optimized_config.data.augmentation.rotation_range = best_config['aug_rotation_range']\n",
    "optimized_config.data.augmentation.scale_range = [\n",
    "    best_config['aug_scale_min'],\n",
    "    best_config['aug_scale_max']\n",
    "]\n",
    "optimized_config.data.augmentation.translation_std = best_config['aug_translation_std']\n",
    "optimized_config.data.augmentation.color_jitter = best_config['aug_color_jitter']\n",
    "optimized_config.data.augmentation.joint_noise_std = best_config['aug_joint_noise_std']\n",
    "\n",
    "# Update loss weights\n",
    "for key in best_config:\n",
    "    if key.startswith('loss_weight_'):\n",
    "        loss_key = key.replace('loss_weight_', '')\n",
    "        optimized_config.loss.loss_weights[loss_key] = best_config[key]\n",
    "\n",
    "# Update other loss configs\n",
    "optimized_config.loss.diversity_margin = best_config['diversity_margin']\n",
    "optimized_config.loss.per_joint_weighting = best_config['per_joint_weighting']\n",
    "optimized_config.loss.fingertip_weight = best_config['fingertip_weight']\n",
    "\n",
    "# Save optimized configuration\n",
    "OmegaConf.save(optimized_config, '../configs/optimized_config.yaml')\n",
    "\n",
    "print(\"Optimized configuration saved to configs/optimized_config.yaml\")\n",
    "print(\"\\nYou can now use this configuration for training:\")\n",
    "print(\"python train_advanced.py --config configs/optimized_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a comprehensive W&B sweep for hyperparameter optimization of the Advanced Manipulation Transformer, focusing on:\n",
    "\n",
    "1. **Augmentation parameters**: rotation, scale, translation, color jitter, joint noise\n",
    "2. **Model parameters**: dropout, learning rate\n",
    "3. **Scheduler options**: cosine, cosine with warmup, step, exponential\n",
    "4. **Loss weights**: all major loss components\n",
    "5. **Loss configuration**: diversity margin, per-joint weighting, fingertip weight\n",
    "\n",
    "The sweep was configured to run each experiment for:\n",
    "- 7 epochs\n",
    "- 20,000 training samples\n",
    "- 2,000 validation samples\n",
    "\n",
    "Results are automatically tracked in W&B, and the best configuration is exported for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}