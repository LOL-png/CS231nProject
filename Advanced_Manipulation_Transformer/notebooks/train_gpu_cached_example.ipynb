{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Cached Training Example\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Disable torch.compile to avoid graph break warnings\n",
    "2. Use GPU-cached datasets to achieve 100GB+ memory usage\n",
    "3. Maximize training performance on H200 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Disable torch.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# IMPORTANT: Disable torch.compile for debugging\n",
    "from disable_compile_for_debug import disable_torch_compile\n",
    "disable_torch_compile()\n",
    "print(\"✓ Debugging mode enabled - no more graph break warnings!\")\n",
    "\n",
    "# Set environment\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for GPU-cached training\n",
    "config = {\n",
    "    # GPU caching settings\n",
    "    'gpu_max_samples': 100000,      # Load 100k samples (~100GB memory)\n",
    "    'gpu_max_samples_val': 10000,   # 10k validation samples (~10GB)\n",
    "    'gpu_cache_path': './gpu_cache', # Cache directory\n",
    "    'use_bfloat16': True,           # Use bfloat16 to fit more data\n",
    "    'preload_dinov2': False,        # Pre-extract DINOv2 features\n",
    "    \n",
    "    # Model settings\n",
    "    'hidden_dim': 512,              # Model hidden dimension\n",
    "    'num_heads': 16,\n",
    "    'num_layers': 8,\n",
    "    'dropout': 0.1,\n",
    "    'num_refinement_steps': 2,\n",
    "    \n",
    "    # Training settings\n",
    "    'batch_size': 256,              # Start with this, will optimize\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    \n",
    "    # Output\n",
    "    'output_dir': 'outputs/gpu_cached_run'\n",
    "}\n",
    "\n",
    "os.makedirs(config['output_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create GPU-Cached Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GPU-cached dataset\n",
    "from data.gpu_cached_dataset import GPUCachedDataset, GPUDataLoader\n",
    "\n",
    "print(\"Creating GPU-cached datasets...\")\n",
    "print(f\"Target memory usage: {config['gpu_max_samples'] / 1000:.0f} GB\")\n",
    "print(\"Note: First run will be slow (preprocessing), subsequent runs will be instant\\n\")\n",
    "\n",
    "# Create train dataset\n",
    "train_dataset = GPUCachedDataset(\n",
    "    split='train',\n",
    "    max_samples=config['gpu_max_samples'],\n",
    "    image_size=(224, 224),\n",
    "    device='cuda',\n",
    "    dtype=torch.bfloat16 if config['use_bfloat16'] else torch.float32,\n",
    "    cache_path=config['gpu_cache_path'],\n",
    "    normalize=True,\n",
    "    load_dinov2_features=config['preload_dinov2']\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "val_dataset = GPUCachedDataset(\n",
    "    split='val',\n",
    "    max_samples=config['gpu_max_samples_val'],\n",
    "    image_size=(224, 224),\n",
    "    device='cuda',\n",
    "    dtype=torch.bfloat16 if config['use_bfloat16'] else torch.float32,\n",
    "    cache_path=config['gpu_cache_path'],\n",
    "    normalize=True,\n",
    "    load_dinov2_features=config['preload_dinov2']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Datasets loaded!\")\n",
    "print(f\"  Train: {len(train_dataset)} samples ({train_dataset._get_memory_usage():.1f} GB)\")\n",
    "print(f\"  Val: {len(val_dataset)} samples ({val_dataset._get_memory_usage():.1f} GB)\")\n",
    "print(f\"  Total GPU memory used: {train_dataset._get_memory_usage() + val_dataset._get_memory_usage():.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find Optimal Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test model for batch size optimization\n",
    "class SimpleTestModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, hidden_dim, 16, 16)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(hidden_dim, 21 * 3)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = batch['image']\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x).squeeze(-1).squeeze(-1)\n",
    "        joints = self.fc(x).view(-1, 21, 3)\n",
    "        return {'hand_joints': joints}\n",
    "\n",
    "# Create test model\n",
    "test_model = SimpleTestModel(config['hidden_dim']).to(device)\n",
    "if config['use_bfloat16']:\n",
    "    test_model = test_model.to(dtype=torch.bfloat16)\n",
    "\n",
    "# Find optimal batch size\n",
    "def find_optimal_batch_size(model, dataset, initial=64, maximum=2048):\n",
    "    print(\"Finding optimal batch size...\")\n",
    "    batch_size = initial\n",
    "    best_batch_size = batch_size\n",
    "    \n",
    "    while batch_size <= maximum:\n",
    "        try:\n",
    "            print(f\"Testing batch size: {batch_size}\", end=\"\")\n",
    "            loader = GPUDataLoader(dataset, batch_size=batch_size)\n",
    "            batch = next(iter(loader))\n",
    "            \n",
    "            # Forward and backward\n",
    "            outputs = model(batch)\n",
    "            loss = outputs['hand_joints'].mean()\n",
    "            loss.backward()\n",
    "            \n",
    "            model.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            best_batch_size = batch_size\n",
    "            print(f\" ✓ ({torch.cuda.memory_allocated() / 1e9:.1f} GB used)\")\n",
    "            batch_size *= 2\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(\" ✗ OOM\")\n",
    "                break\n",
    "            raise e\n",
    "    \n",
    "    return best_batch_size\n",
    "\n",
    "optimal_batch_size = find_optimal_batch_size(test_model, train_dataset)\n",
    "print(f\"\\nOptimal batch size: {optimal_batch_size}\")\n",
    "config['batch_size'] = optimal_batch_size\n",
    "\n",
    "# Clean up test model\n",
    "del test_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU dataloaders with optimal batch size\n",
    "train_loader = GPUDataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True, \n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = GPUDataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False, \n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(f\"Dataloaders created:\")\n",
    "print(f\"  Batch size: {config['batch_size']}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_dataloader(loader, num_batches=50):\n",
    "    \"\"\"Benchmark dataloader performance\"\"\"\n",
    "    print(f\"Benchmarking {num_batches} batches...\")\n",
    "    \n",
    "    # Warmup\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        _ = batch['image'].mean()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        # Simulate computation\n",
    "        _ = batch['image'].mean()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    samples_per_sec = (num_batches * loader.batch_size) / elapsed\n",
    "    batches_per_sec = num_batches / elapsed\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s\")\n",
    "    print(f\"  Throughput: {samples_per_sec:,.0f} samples/sec\")\n",
    "    print(f\"  Batches/sec: {batches_per_sec:.1f}\")\n",
    "    \n",
    "    return samples_per_sec\n",
    "\n",
    "# Benchmark\n",
    "throughput = benchmark_dataloader(train_loader)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"GPU Memory Usage: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "print(f\"Expected training speed: {throughput:,.0f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model - choose based on your needs\n",
    "USE_SIMPLE_MODEL = True  # Set False to use full UnifiedManipulationModel\n",
    "\n",
    "if USE_SIMPLE_MODEL:\n",
    "    # Simple model for testing\n",
    "    class SimpleManipulationModel(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.hidden_dim = config['hidden_dim']\n",
    "            \n",
    "            # Patch embedding\n",
    "            self.patch_embed = nn.Conv2d(3, self.hidden_dim, 16, 16)\n",
    "            self.pos_embed = nn.Parameter(torch.randn(1, 196, self.hidden_dim) * 0.02)\n",
    "            \n",
    "            # Transformer\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=self.hidden_dim,\n",
    "                nhead=config['num_heads'],\n",
    "                dim_feedforward=self.hidden_dim * 4,\n",
    "                dropout=config['dropout'],\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.encoder = nn.TransformerEncoder(encoder_layer, config['num_layers'])\n",
    "            \n",
    "            # Output head\n",
    "            self.hand_head = nn.Linear(self.hidden_dim, 21 * 3)\n",
    "        \n",
    "        def forward(self, batch):\n",
    "            x = batch['image']\n",
    "            B = x.shape[0]\n",
    "            \n",
    "            # Patch embedding\n",
    "            x = self.patch_embed(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = x + self.pos_embed\n",
    "            \n",
    "            # Transformer\n",
    "            x = self.encoder(x)\n",
    "            x = x.mean(dim=1)\n",
    "            \n",
    "            # Predict joints\n",
    "            joints = self.hand_head(x).view(B, 21, 3)\n",
    "            \n",
    "            return {\n",
    "                'hand_joints': joints,\n",
    "                'hand_joints_refined': joints  # Same for simple model\n",
    "            }\n",
    "    \n",
    "    model = SimpleManipulationModel(config)\n",
    "    print(\"Using simple model for testing\")\n",
    "    \n",
    "else:\n",
    "    # Full model\n",
    "    from models.unified_model import UnifiedManipulationModel\n",
    "    model = UnifiedManipulationModel(\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_heads=config['num_heads'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        num_refinement_steps=config['num_refinement_steps'],\n",
    "        use_sigma_reparam=False  # Disable for debugging\n",
    "    )\n",
    "    print(\"Using full UnifiedManipulationModel\")\n",
    "\n",
    "# Move to GPU with appropriate dtype\n",
    "model = model.to(device)\n",
    "if config['use_bfloat16']:\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "\n",
    "# Enable gradient checkpointing if available\n",
    "if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"✓ Gradient checkpointing enabled\")\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "class SimpleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        losses = {}\n",
    "        \n",
    "        # Hand joint loss\n",
    "        if 'hand_joints' in outputs and 'hand_joints' in targets:\n",
    "            pred = outputs['hand_joints']\n",
    "            gt = targets['hand_joints']\n",
    "            \n",
    "            # Filter valid joints\n",
    "            if 'hand_joints_valid' in targets:\n",
    "                valid = targets['hand_joints_valid'].unsqueeze(-1)\n",
    "                pred = pred * valid\n",
    "                gt = gt * valid\n",
    "            \n",
    "            # MSE loss\n",
    "            losses['joint_loss'] = F.mse_loss(pred, gt)\n",
    "            \n",
    "            # MPJPE\n",
    "            with torch.no_grad():\n",
    "                losses['mpjpe'] = torch.norm(pred - gt, dim=-1).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        losses['total'] = sum(v for k, v in losses.items() if k != 'mpjpe')\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "criterion = SimpleLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config['num_epochs']\n",
    ")\n",
    "\n",
    "print(\"Training setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_mpjpe': [],\n",
    "    'val_loss': [],\n",
    "    'val_mpjpe': [],\n",
    "    'gpu_memory': [],\n",
    "    'throughput': []\n",
    "}\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mpjpe = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        losses = criterion(outputs, batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        losses['total'].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += losses['total'].item()\n",
    "        if 'mpjpe' in losses:\n",
    "            total_mpjpe += losses['mpjpe'].item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{losses['total'].item():.4f}\",\n",
    "            'mpjpe': f\"{losses.get('mpjpe', 0).item():.2f}mm\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / num_batches, total_mpjpe / num_batches\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mpjpe = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Validation\"):\n",
    "        outputs = model(batch)\n",
    "        losses = criterion(outputs, batch)\n",
    "        \n",
    "        total_loss += losses['total'].item()\n",
    "        if 'mpjpe' in losses:\n",
    "            total_mpjpe += losses['mpjpe'].item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_mpjpe / num_batches\n",
    "\n",
    "# Main training loop\n",
    "print(f\"\\nStarting training for {config['num_epochs']} epochs...\\n\")\n",
    "\n",
    "best_val_mpjpe = float('inf')\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Record GPU memory\n",
    "    gpu_mem = torch.cuda.memory_allocated() / 1e9\n",
    "    history['gpu_memory'].append(gpu_mem)\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "    \n",
    "    # Train\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    train_loss, train_mpjpe = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate throughput\n",
    "    samples_per_sec = len(train_loader) * config['batch_size'] / train_time\n",
    "    history['throughput'].append(samples_per_sec)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_mpjpe = validate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mpjpe'].append(train_mpjpe)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mpjpe'].append(val_mpjpe)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, MPJPE: {train_mpjpe:.2f}mm\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, MPJPE: {val_mpjpe:.2f}mm\")\n",
    "    print(f\"  Throughput: {samples_per_sec:,.0f} samples/sec\")\n",
    "    print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_mpjpe < best_val_mpjpe:\n",
    "        best_val_mpjpe = val_mpjpe\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_mpjpe': val_mpjpe,\n",
    "            'config': config\n",
    "        }, f\"{config['output_dir']}/best_model.pth\")\n",
    "        print(\"  ✓ Saved best model\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation MPJPE: {best_val_mpjpe:.2f}mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss plot\n",
    "ax = axes[0, 0]\n",
    "ax.plot(history['train_loss'], label='Train')\n",
    "ax.plot(history['val_loss'], label='Val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# MPJPE plot\n",
    "ax = axes[0, 1]\n",
    "ax.plot(history['train_mpjpe'], label='Train')\n",
    "ax.plot(history['val_mpjpe'], label='Val')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MPJPE (mm)')\n",
    "ax.set_title('Mean Per Joint Position Error')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# GPU Memory plot\n",
    "ax = axes[1, 0]\n",
    "ax.plot(history['gpu_memory'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('GPU Memory (GB)')\n",
    "ax.set_title('GPU Memory Usage')\n",
    "ax.grid(True)\n",
    "ax.axhline(y=100, color='r', linestyle='--', label='Target: 100GB')\n",
    "ax.legend()\n",
    "\n",
    "# Throughput plot\n",
    "ax = axes[1, 1]\n",
    "ax.plot(history['throughput'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Samples/sec')\n",
    "ax.set_title('Training Throughput')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config['output_dir']}/training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"  Best Train MPJPE: {min(history['train_mpjpe']):.2f}mm\")\n",
    "print(f\"  Best Val MPJPE: {min(history['val_mpjpe']):.2f}mm\")\n",
    "print(f\"  Average GPU Memory: {np.mean(history['gpu_memory']):.1f} GB\")\n",
    "print(f\"  Average Throughput: {np.mean(history['throughput']):,.0f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training report\n",
    "report = f\"\"\"\n",
    "# GPU-Cached Training Report\n",
    "\n",
    "## Configuration\n",
    "- GPU: {torch.cuda.get_device_name()}\n",
    "- Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\n",
    "- Dataset samples: {config['gpu_max_samples']} train, {config['gpu_max_samples_val']} val\n",
    "- Batch size: {config['batch_size']}\n",
    "- Learning rate: {config['learning_rate']}\n",
    "- Epochs: {config['num_epochs']}\n",
    "- Using BFloat16: {config['use_bfloat16']}\n",
    "\n",
    "## Results\n",
    "- Best Train MPJPE: {min(history['train_mpjpe']):.2f}mm\n",
    "- Best Val MPJPE: {min(history['val_mpjpe']):.2f}mm\n",
    "- Final Train MPJPE: {history['train_mpjpe'][-1]:.2f}mm\n",
    "- Final Val MPJPE: {history['val_mpjpe'][-1]:.2f}mm\n",
    "\n",
    "## Performance\n",
    "- GPU Memory Usage: {np.mean(history['gpu_memory']):.1f} GB average\n",
    "- Peak GPU Memory: {max(history['gpu_memory']):.1f} GB\n",
    "- Average Throughput: {np.mean(history['throughput']):,.0f} samples/sec\n",
    "- Peak Throughput: {max(history['throughput']):,.0f} samples/sec\n",
    "\n",
    "## Dataset Loading\n",
    "- Train dataset memory: {train_dataset._get_memory_usage():.1f} GB\n",
    "- Val dataset memory: {val_dataset._get_memory_usage():.1f} GB\n",
    "- Total dataset memory: {train_dataset._get_memory_usage() + val_dataset._get_memory_usage():.1f} GB\n",
    "\n",
    "## Model\n",
    "- Total parameters: {sum(p.numel() for p in model.parameters()):,}\n",
    "- Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\n",
    "- Model memory: {sum(p.numel() for p in model.parameters()) * 4 / 1e9:.2f} GB\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open(f\"{config['output_dir']}/training_report.txt\", 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "with open(f\"{config['output_dir']}/config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {config['output_dir']}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}