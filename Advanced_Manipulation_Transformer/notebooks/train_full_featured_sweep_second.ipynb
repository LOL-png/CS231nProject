{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Manipulation Transformer - Second Stage W&B Sweep\n",
    "\n",
    "This notebook implements a second stage Weights & Biases sweep for fine-tuning the top 8 models from the first sweep.\n",
    "\n",
    "**Focus**: Fine-tune only the most critical parameters:\n",
    "- Weight decay\n",
    "- Learning rate\n",
    "- Dropout\n",
    "\n",
    "**Configuration**:\n",
    "- Start from top 8 configurations from first sweep\n",
    "- 7 epochs per run\n",
    "- 20,000 training samples\n",
    "- 2,000 validation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0+cu124\n",
      "CUDA available: True\n",
      "GPU: NVIDIA H200\n",
      "GPU Memory: 139.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['DEX_YCB_DIR'] = '/home/n231/231nProjectV2/dex-ycb-toolkit/data'\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path('.').absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results from first sweep\n",
    "# Replace with your actual sweep ID from the first stage\n",
    "FIRST_SWEEP_ID = \"c189gpt8\"  # Update this with your actual first sweep ID\n",
    "FIRST_PROJECT_NAME = \"amt-hyperparameter-sweep\"\n",
    "\n",
    "# Get top 8 configurations\n",
    "api = wandb.Api()\n",
    "try:\n",
    "    sweep = api.sweep(f\"{wandb.api.default_entity}/{FIRST_PROJECT_NAME}/sweeps/{FIRST_SWEEP_ID}\")\n",
    "    runs = list(sweep.runs)\n",
    "    \n",
    "    # Sort by validation MPJPE and get top 8\n",
    "    valid_runs = [r for r in runs if 'val/hand_mpjpe' in r.summary]\n",
    "    sorted_runs = sorted(valid_runs, key=lambda r: r.summary['val/hand_mpjpe'])\n",
    "    top_8_runs = sorted_runs[:8]\n",
    "    \n",
    "    print(f\"Found {len(top_8_runs)} top performing runs:\")\n",
    "    for i, run in enumerate(top_8_runs):\n",
    "        print(f\"  {i+1}. {run.name}: {run.summary['val/hand_mpjpe']:.2f} mm\")\n",
    "    \n",
    "    # Extract configurations - IMPORTANT: Add learning_rate from default config\n",
    "    top_8_configs = []\n",
    "    for run in top_8_runs:\n",
    "        config = dict(run.config)\n",
    "        # Remove wandb internal fields\n",
    "        config = {k: v for k, v in config.items() if not k.startswith('_')}\n",
    "        # Add learning_rate if not present (use default)\n",
    "        if 'learning_rate' not in config:\n",
    "            config['learning_rate'] = 0.001  # Default learning rate\n",
    "        top_8_configs.append(config)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not load first sweep results: {e}\")\n",
    "    print(\"\\nUsing default top configurations for demonstration...\")\n",
    "    \n",
    "    # Default configurations for demonstration\n",
    "    top_8_configs = [\n",
    "        {\n",
    "            'batch_size': 32,\n",
    "            'scheduler_type': 'cosine_warmup',\n",
    "            'aug_rotation_range': 15.0,\n",
    "            'aug_scale_min': 0.85,\n",
    "            'aug_scale_max': 1.15,\n",
    "            'aug_translation_std': 0.05,\n",
    "            'aug_color_jitter': 0.2,\n",
    "            'aug_joint_noise_std': 0.005,\n",
    "            'loss_weight_hand_coarse': 1.0,\n",
    "            'loss_weight_hand_refined': 1.2,\n",
    "            'loss_weight_object_position': 1.0,\n",
    "            'loss_weight_object_rotation': 0.5,\n",
    "            'loss_weight_contact': 0.3,\n",
    "            'loss_weight_physics': 0.1,\n",
    "            'loss_weight_diversity': 0.01,\n",
    "            'loss_weight_reprojection': 0.5,\n",
    "            'diversity_margin': 0.01,\n",
    "            'per_joint_weighting': True,\n",
    "            'fingertip_weight': 1.5,\n",
    "            'learning_rate': 0.001  # Added default learning rate\n",
    "        }\n",
    "    ] * 8  # Duplicate for demonstration\n",
    "\n",
    "print(f\"\\nLoaded {len(top_8_configs)} configurations for second stage sweep\")\n",
    "\n",
    "# Print sample config to verify\n",
    "if top_8_configs:\n",
    "    print(\"\\nSample configuration keys:\")\n",
    "    for key in sorted(top_8_configs[0].keys()):\n",
    "        print(f\"  {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Define second stage sweep configuration\n# Focus on fine-tuning weight decay, learning rate, and dropout\nsweep_config_second = {\n    'method': 'bayes',  # Bayesian optimization\n    'metric': {\n        'name': 'val/hand_mpjpe',\n        'goal': 'minimize'\n    },\n    'parameters': {\n        # Index of top 8 configuration to use\n        'config_index': {\n            'values': list(range(8))  # 0-7 for top 8 configs\n        },\n        \n        # Fine-tune critical parameters\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 1e-5,\n            'max': 1e-3\n        },\n        \n        'weight_decay': {\n            'distribution': 'log_uniform_values',\n            'min': 1e-5,\n            'max': 1e-2\n        },\n        \n        'dropout': {\n            'distribution': 'uniform',\n            'min': 0.05,\n            'max': 0.4\n        }\n    }\n}\n\nprint(\"Second stage sweep configuration:\")\nprint(f\"Method: {sweep_config_second['method']}\")\nprint(f\"Metric: {sweep_config_second['metric']['name']} ({sweep_config_second['metric']['goal']})\")\nprint(f\"\\nParameters to fine-tune:\")\nprint(\"- Learning rate: log-uniform [1e-5, 1e-3]\")\nprint(\"- Weight decay: log-uniform [1e-5, 1e-2]\")\nprint(\"- Dropout: uniform [0.05, 0.4]\")\nprint(f\"\\nTesting on top {len(top_8_configs)} configurations from first sweep\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Second Stage Sweep Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second stage training function defined!\n"
     ]
    }
   ],
   "source": [
    "def train_second_stage():\n",
    "    # Initialize wandb\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # Get base configuration from top 8\n",
    "    config_idx = wandb.config.config_index\n",
    "    base_config = top_8_configs[config_idx].copy()\n",
    "    \n",
    "    # Load default configuration\n",
    "    config = OmegaConf.load('../configs/default_config.yaml')\n",
    "    \n",
    "    # Apply base configuration from first sweep\n",
    "    config.training.batch_size = base_config['batch_size']\n",
    "    config.data.augmentation.rotation_range = base_config['aug_rotation_range']\n",
    "    config.data.augmentation.scale_range = [\n",
    "        base_config['aug_scale_min'],\n",
    "        base_config['aug_scale_max']\n",
    "    ]\n",
    "    config.data.augmentation.translation_std = base_config['aug_translation_std']\n",
    "    config.data.augmentation.color_jitter = base_config['aug_color_jitter']\n",
    "    config.data.augmentation.joint_noise_std = base_config['aug_joint_noise_std']\n",
    "    \n",
    "    # Apply loss weights from base config\n",
    "    config.loss.loss_weights.hand_coarse = base_config['loss_weight_hand_coarse']\n",
    "    config.loss.loss_weights.hand_refined = base_config['loss_weight_hand_refined']\n",
    "    config.loss.loss_weights.object_position = base_config['loss_weight_object_position']\n",
    "    config.loss.loss_weights.object_rotation = base_config['loss_weight_object_rotation']\n",
    "    config.loss.loss_weights.contact = base_config['loss_weight_contact']\n",
    "    config.loss.loss_weights.physics = base_config['loss_weight_physics']\n",
    "    config.loss.loss_weights.diversity = base_config['loss_weight_diversity']\n",
    "    config.loss.loss_weights.reprojection = base_config['loss_weight_reprojection']\n",
    "    \n",
    "    config.loss.diversity_margin = base_config['diversity_margin']\n",
    "    config.loss.per_joint_weighting = base_config.get('per_joint_weighting', True)\n",
    "    config.loss.fingertip_weight = base_config.get('fingertip_weight', 1.5)\n",
    "    \n",
    "    # Apply base learning rate if it exists\n",
    "    if 'learning_rate' in base_config:\n",
    "        config.training.learning_rate = base_config['learning_rate']\n",
    "    \n",
    "    # Override with second stage sweep parameters\n",
    "    config.training.learning_rate = wandb.config.learning_rate\n",
    "    config.training.weight_decay = wandb.config.weight_decay\n",
    "    config.model.dropout = wandb.config.dropout\n",
    "    \n",
    "    # Fixed parameters\n",
    "    config.training.num_epochs = 7  # Fixed at 7 epochs\n",
    "    config.training.use_wandb = True\n",
    "    config.training.use_amp = True\n",
    "    config.training.use_bf16 = True\n",
    "    \n",
    "    # Log which base config we're using\n",
    "    wandb.log({\n",
    "        'base_config_index': config_idx,\n",
    "        'base_batch_size': base_config['batch_size'],\n",
    "        'base_scheduler': base_config['scheduler_type'],\n",
    "        'base_learning_rate': base_config.get('learning_rate', 'not_specified')\n",
    "    })\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Import components\n",
    "    from models.unified_model import UnifiedManipulationTransformer\n",
    "    from training.losses import ComprehensiveLoss\n",
    "    from data.gpu_cached_dataset import create_gpu_cached_dataloaders\n",
    "    from solutions.mode_collapse import ModeCollapsePreventionModule\n",
    "    from optimizations.pytorch_native_optimization import PyTorchNativeOptimizer\n",
    "    \n",
    "    try:\n",
    "        # Create GPU-cached dataloaders\n",
    "        print(f\"Creating dataloaders with config {config_idx}: batch_size={config.training.batch_size}...\")\n",
    "        gpu_config = {\n",
    "            'gpu_max_samples': 20000,\n",
    "            'gpu_max_samples_val': 2000,\n",
    "            'gpu_cache_path': './gpu_cache_sweep_second',\n",
    "            'batch_size': config.training.batch_size,\n",
    "            'use_bfloat16': config.training.use_bf16,\n",
    "            'preload_dinov2': False\n",
    "        }\n",
    "        \n",
    "        train_loader, val_loader = create_gpu_cached_dataloaders(gpu_config)\n",
    "        print(f\"Dataloaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n",
    "        \n",
    "        # Create model\n",
    "        model = UnifiedManipulationTransformer(config.model)\n",
    "        \n",
    "        # Apply mode collapse prevention\n",
    "        mode_collapse_config = {\n",
    "            'noise_std': 0.01,\n",
    "            'drop_path_rate': 0.1,\n",
    "            'mixup_alpha': 0.2\n",
    "        }\n",
    "        model = ModeCollapsePreventionModule.wrap_model(model, mode_collapse_config)\n",
    "        \n",
    "        # Initialize weights\n",
    "        def init_weights(module):\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                if hasattr(module, 'weight') and module.weight is not None:\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.01)\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if 'dinov2' not in name or 'encoder.layer.' not in name:\n",
    "                init_weights(module)\n",
    "        \n",
    "        # Apply optimizations (DISABLE torch.compile to avoid errors)\n",
    "        native_optimizer = PyTorchNativeOptimizer()\n",
    "        model = native_optimizer.optimize_model(model, {'use_compile': False})\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer with parameter groups\n",
    "        dinov2_params = []\n",
    "        encoder_params = []\n",
    "        decoder_params = []\n",
    "        other_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if 'dinov2' in name:\n",
    "                dinov2_params.append(param)\n",
    "            elif 'decoder' in name:\n",
    "                decoder_params.append(param)\n",
    "            elif 'encoder' in name:\n",
    "                encoder_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        param_groups = []\n",
    "        if dinov2_params:\n",
    "            param_groups.append({\n",
    "                'params': dinov2_params,\n",
    "                'lr': config.training.learning_rate * 0.01,\n",
    "                'name': 'dinov2'\n",
    "            })\n",
    "        if encoder_params:\n",
    "            param_groups.append({\n",
    "                'params': encoder_params,\n",
    "                'lr': config.training.learning_rate * 0.5,\n",
    "                'name': 'encoders'\n",
    "            })\n",
    "        if decoder_params:\n",
    "            param_groups.append({\n",
    "                'params': decoder_params,\n",
    "                'lr': config.training.learning_rate,\n",
    "                'name': 'decoders'\n",
    "            })\n",
    "        if other_params:\n",
    "            param_groups.append({\n",
    "                'params': other_params,\n",
    "                'lr': config.training.learning_rate,\n",
    "                'name': 'other'\n",
    "            })\n",
    "        \n",
    "        # Use weight decay from sweep\n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=config.training.weight_decay, fused=True)\n",
    "        \n",
    "        # Use scheduler from base config\n",
    "        scheduler_type = base_config['scheduler_type']\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=config.training.num_epochs, eta_min=1e-6\n",
    "            )\n",
    "        elif scheduler_type == 'cosine_warmup':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "        elif scheduler_type == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=3, gamma=0.5\n",
    "            )\n",
    "        elif scheduler_type == 'exponential':\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=0.9\n",
    "            )\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = ComprehensiveLoss(config.loss)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_mpjpe = float('inf')\n",
    "        \n",
    "        print(f\"Starting training with LR={config.training.learning_rate:.2e}, WD={config.training.weight_decay:.2e}, Dropout={config.model.dropout:.2f}\")\n",
    "        \n",
    "        for epoch in range(config.training.num_epochs):\n",
    "            # Update loss epoch\n",
    "            criterion.set_epoch(epoch)\n",
    "            \n",
    "            # Train\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_mpjpe = 0\n",
    "            train_samples = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                # Convert BFloat16 images to Float32 for DINOv2\n",
    "                if batch['image'].dtype == torch.bfloat16:\n",
    "                    batch['image'] = batch['image'].float()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    outputs = model(batch)\n",
    "                    losses = criterion(outputs, batch)\n",
    "                    loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_size = batch['image'].shape[0]\n",
    "                train_loss += loss.item() * batch_size\n",
    "                train_samples += batch_size\n",
    "                \n",
    "                # Calculate MPJPE\n",
    "                if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                    with torch.no_grad():\n",
    "                        mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                        train_mpjpe += mpjpe.item() * 1000 * batch_size\n",
    "                \n",
    "                # Log batch metrics\n",
    "                if batch_idx % 20 == 0:\n",
    "                    wandb.log({\n",
    "                        'train/batch_loss': loss.item(),\n",
    "                        'train/batch_mpjpe': mpjpe.item() * 1000 if 'hand_joints' in outputs else 0,\n",
    "                        'train/lr': optimizer.param_groups[0]['lr'],\n",
    "                        'train/weight_decay': config.training.weight_decay,\n",
    "                        'train/dropout': config.model.dropout,\n",
    "                        'system/gpu_memory_gb': torch.cuda.memory_allocated() / 1e9\n",
    "                    })\n",
    "            \n",
    "            # Average training metrics\n",
    "            train_loss /= train_samples\n",
    "            train_mpjpe /= train_samples\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_mpjpe = 0\n",
    "            val_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    if batch['image'].dtype == torch.bfloat16:\n",
    "                        batch['image'] = batch['image'].float()\n",
    "                    \n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        outputs = model(batch)\n",
    "                        losses = criterion(outputs, batch)\n",
    "                        loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "                    \n",
    "                    batch_size = batch['image'].shape[0]\n",
    "                    val_samples += batch_size\n",
    "                    val_loss += loss.item() * batch_size\n",
    "                    \n",
    "                    if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                        mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                        val_mpjpe += mpjpe.item() * 1000 * batch_size\n",
    "            \n",
    "            # Average validation metrics\n",
    "            val_loss /= val_samples\n",
    "            val_mpjpe /= val_samples\n",
    "            \n",
    "            # Update best\n",
    "            if val_mpjpe < best_val_mpjpe:\n",
    "                best_val_mpjpe = val_mpjpe\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train/loss': train_loss,\n",
    "                'train/hand_mpjpe': train_mpjpe,\n",
    "                'val/loss': val_loss,\n",
    "                'val/hand_mpjpe': val_mpjpe,\n",
    "                'val/best_mpjpe': best_val_mpjpe\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{config.training.num_epochs}: \"\n",
    "                  f\"train_loss={train_loss:.4f}, train_mpjpe={train_mpjpe:.2f}mm, \"\n",
    "                  f\"val_loss={val_loss:.4f}, val_mpjpe={val_mpjpe:.2f}mm\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        wandb.log({\n",
    "            'final/best_val_mpjpe': best_val_mpjpe,\n",
    "            'final/base_config_index': config_idx,\n",
    "            'final/learning_rate': config.training.learning_rate,\n",
    "            'final/weight_decay': config.training.weight_decay,\n",
    "            'final/dropout': config.model.dropout\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best validation MPJPE: {best_val_mpjpe:.2f} mm\")\n",
    "        \n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        wandb.log({'error': str(e), 'val/hand_mpjpe': 1000.0})\n",
    "        raise\n",
    "\n",
    "print(\"Second stage training function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second stage training function defined!\n"
     ]
    }
   ],
   "source": [
    "def train_second_stage():\n",
    "    # Initialize wandb\n",
    "    run = wandb.init()\n",
    "    \n",
    "    # Get base configuration from top 8\n",
    "    config_idx = wandb.config.config_index\n",
    "    base_config = top_8_configs[config_idx].copy()\n",
    "    \n",
    "    # Load default configuration\n",
    "    config = OmegaConf.load('../configs/default_config.yaml')\n",
    "    \n",
    "    # Apply base configuration from first sweep\n",
    "    config.training.batch_size = base_config['batch_size']\n",
    "    config.data.augmentation.rotation_range = base_config['aug_rotation_range']\n",
    "    config.data.augmentation.scale_range = [\n",
    "        base_config['aug_scale_min'],\n",
    "        base_config['aug_scale_max']\n",
    "    ]\n",
    "    config.data.augmentation.translation_std = base_config['aug_translation_std']\n",
    "    config.data.augmentation.color_jitter = base_config['aug_color_jitter']\n",
    "    config.data.augmentation.joint_noise_std = base_config['aug_joint_noise_std']\n",
    "    \n",
    "    # Apply loss weights from base config\n",
    "    config.loss.loss_weights.hand_coarse = base_config['loss_weight_hand_coarse']\n",
    "    config.loss.loss_weights.hand_refined = base_config['loss_weight_hand_refined']\n",
    "    config.loss.loss_weights.object_position = base_config['loss_weight_object_position']\n",
    "    config.loss.loss_weights.object_rotation = base_config['loss_weight_object_rotation']\n",
    "    config.loss.loss_weights.contact = base_config['loss_weight_contact']\n",
    "    config.loss.loss_weights.physics = base_config['loss_weight_physics']\n",
    "    config.loss.loss_weights.diversity = base_config['loss_weight_diversity']\n",
    "    config.loss.loss_weights.reprojection = base_config['loss_weight_reprojection']\n",
    "    \n",
    "    config.loss.diversity_margin = base_config['diversity_margin']\n",
    "    config.loss.per_joint_weighting = base_config['per_joint_weighting']\n",
    "    config.loss.fingertip_weight = base_config['fingertip_weight']\n",
    "    \n",
    "    # Override with second stage sweep parameters\n",
    "    config.training.learning_rate = wandb.config.learning_rate\n",
    "    config.training.weight_decay = wandb.config.weight_decay\n",
    "    config.model.dropout = wandb.config.dropout\n",
    "    \n",
    "    # Fixed parameters\n",
    "    config.training.num_epochs = 10\n",
    "    config.training.use_wandb = True\n",
    "    config.training.use_amp = True\n",
    "    config.training.use_bf16 = True\n",
    "    \n",
    "    # Log which base config we're using\n",
    "    wandb.log({\n",
    "        'base_config_index': config_idx,\n",
    "        'base_batch_size': base_config['batch_size'],\n",
    "        'base_scheduler': base_config['scheduler_type']\n",
    "    })\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Import components\n",
    "    from models.unified_model import UnifiedManipulationTransformer\n",
    "    from training.losses import ComprehensiveLoss\n",
    "    from data.gpu_cached_dataset import create_gpu_cached_dataloaders\n",
    "    from solutions.mode_collapse import ModeCollapsePreventionModule\n",
    "    from optimizations.pytorch_native_optimization import optimize_for_h200\n",
    "    \n",
    "    try:\n",
    "        # Create GPU-cached dataloaders\n",
    "        print(f\"Creating dataloaders with config {config_idx}: batch_size={config.training.batch_size}...\")\n",
    "        gpu_config = {\n",
    "            'gpu_max_samples': 50000,\n",
    "            'gpu_max_samples_val': 10000,\n",
    "            'gpu_cache_path': './gpu_cache_sweep_second',\n",
    "            'batch_size': config.training.batch_size,\n",
    "            'use_bfloat16': config.training.use_bf16,\n",
    "            'preload_dinov2': False\n",
    "        }\n",
    "        \n",
    "        train_loader, val_loader = create_gpu_cached_dataloaders(gpu_config)\n",
    "        \n",
    "        # Create model\n",
    "        model = UnifiedManipulationTransformer(config.model)\n",
    "        \n",
    "        # Apply mode collapse prevention\n",
    "        mode_collapse_config = {\n",
    "            'noise_std': 0.01,\n",
    "            'drop_path_rate': 0.1,\n",
    "            'mixup_alpha': 0.2\n",
    "        }\n",
    "        model = ModeCollapsePreventionModule.wrap_model(model, mode_collapse_config)\n",
    "        \n",
    "        # Initialize weights\n",
    "        def init_weights(module):\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                if hasattr(module, 'weight') and module.weight is not None:\n",
    "                    nn.init.xavier_uniform_(module.weight)\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0.01)\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if 'dinov2' not in name or 'encoder.layer.' not in name:\n",
    "                init_weights(module)\n",
    "        \n",
    "        # Apply optimizations\n",
    "        model = optimize_for_h200(model, compile_mode='reduce-overhead')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create optimizer with parameter groups\n",
    "        dinov2_params = []\n",
    "        encoder_params = []\n",
    "        decoder_params = []\n",
    "        other_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            if 'dinov2' in name:\n",
    "                dinov2_params.append(param)\n",
    "            elif 'decoder' in name:\n",
    "                decoder_params.append(param)\n",
    "            elif 'encoder' in name:\n",
    "                encoder_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        param_groups = []\n",
    "        if dinov2_params:\n",
    "            param_groups.append({\n",
    "                'params': dinov2_params,\n",
    "                'lr': config.training.learning_rate * 0.01,\n",
    "                'name': 'dinov2'\n",
    "            })\n",
    "        if encoder_params:\n",
    "            param_groups.append({\n",
    "                'params': encoder_params,\n",
    "                'lr': config.training.learning_rate * 0.5,\n",
    "                'name': 'encoders'\n",
    "            })\n",
    "        if decoder_params:\n",
    "            param_groups.append({\n",
    "                'params': decoder_params,\n",
    "                'lr': config.training.learning_rate,\n",
    "                'name': 'decoders'\n",
    "            })\n",
    "        if other_params:\n",
    "            param_groups.append({\n",
    "                'params': other_params,\n",
    "                'lr': config.training.learning_rate,\n",
    "                'name': 'other'\n",
    "            })\n",
    "        \n",
    "        # Use weight decay from sweep\n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=config.training.weight_decay, fused=True)\n",
    "        \n",
    "        # Use scheduler from base config\n",
    "        scheduler_type = base_config['scheduler_type']\n",
    "        if scheduler_type == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=config.training.num_epochs, eta_min=1e-6\n",
    "            )\n",
    "        elif scheduler_type == 'cosine_warmup':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "        elif scheduler_type == 'step':\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=3, gamma=0.5\n",
    "            )\n",
    "        elif scheduler_type == 'exponential':\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=0.9\n",
    "            )\n",
    "        \n",
    "        # Loss function\n",
    "        criterion = ComprehensiveLoss(config.loss)\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_mpjpe = float('inf')\n",
    "        \n",
    "        for epoch in range(config.training.num_epochs):\n",
    "            # Update loss epoch\n",
    "            criterion.set_epoch(epoch)\n",
    "            \n",
    "            # Train\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_mpjpe = 0\n",
    "            train_samples = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                # Convert BFloat16 images to Float32 for DINOv2\n",
    "                if batch['image'].dtype == torch.bfloat16:\n",
    "                    batch['image'] = batch['image'].float()\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                    outputs = model(batch)\n",
    "                    losses = criterion(outputs, batch)\n",
    "                    loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_size = batch['image'].shape[0]\n",
    "                train_loss += loss.item() * batch_size\n",
    "                train_samples += batch_size\n",
    "                \n",
    "                # Calculate MPJPE\n",
    "                if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                    with torch.no_grad():\n",
    "                        mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                        train_mpjpe += mpjpe.item() * 1000 * batch_size\n",
    "                \n",
    "                # Log batch metrics\n",
    "                if batch_idx % 20 == 0:\n",
    "                    wandb.log({\n",
    "                        'train/batch_loss': loss.item(),\n",
    "                        'train/batch_mpjpe': mpjpe.item() * 1000 if 'hand_joints' in outputs else 0,\n",
    "                        'train/lr': optimizer.param_groups[0]['lr'],\n",
    "                        'train/weight_decay': config.training.weight_decay,\n",
    "                        'train/dropout': config.model.dropout,\n",
    "                        'system/gpu_memory_gb': torch.cuda.memory_allocated() / 1e9\n",
    "                    })\n",
    "            \n",
    "            # Average training metrics\n",
    "            train_loss /= train_samples\n",
    "            train_mpjpe /= train_samples\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_mpjpe = 0\n",
    "            val_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    if batch['image'].dtype == torch.bfloat16:\n",
    "                        batch['image'] = batch['image'].float()\n",
    "                    \n",
    "                    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "                        outputs = model(batch)\n",
    "                        losses = criterion(outputs, batch)\n",
    "                        loss = losses['total'] if isinstance(losses, dict) else losses\n",
    "                    \n",
    "                    batch_size = batch['image'].shape[0]\n",
    "                    val_samples += batch_size\n",
    "                    val_loss += loss.item() * batch_size\n",
    "                    \n",
    "                    if 'hand_joints' in outputs and 'hand_joints' in batch:\n",
    "                        mpjpe = torch.norm(outputs['hand_joints'] - batch['hand_joints'], dim=-1).mean()\n",
    "                        val_mpjpe += mpjpe.item() * 1000 * batch_size\n",
    "            \n",
    "            # Average validation metrics\n",
    "            val_loss /= val_samples\n",
    "            val_mpjpe /= val_samples\n",
    "            \n",
    "            # Update best\n",
    "            if val_mpjpe < best_val_mpjpe:\n",
    "                best_val_mpjpe = val_mpjpe\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Log epoch metrics\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train/loss': train_loss,\n",
    "                'train/hand_mpjpe': train_mpjpe,\n",
    "                'val/loss': val_loss,\n",
    "                'val/hand_mpjpe': val_mpjpe,\n",
    "                'val/best_mpjpe': best_val_mpjpe\n",
    "            })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{config.training.num_epochs}: \"\n",
    "                  f\"train_loss={train_loss:.4f}, train_mpjpe={train_mpjpe:.2f}mm, \"\n",
    "                  f\"val_loss={val_loss:.4f}, val_mpjpe={val_mpjpe:.2f}mm\")\n",
    "        \n",
    "        # Log final metrics\n",
    "        wandb.log({\n",
    "            'final/best_val_mpjpe': best_val_mpjpe,\n",
    "            'final/base_config_index': config_idx,\n",
    "            'final/learning_rate': config.training.learning_rate,\n",
    "            'final/weight_decay': config.training.weight_decay,\n",
    "            'final/dropout': config.model.dropout\n",
    "        })\n",
    "        \n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        wandb.log({'error': str(e), 'val/hand_mpjpe': 1000.0})\n",
    "        raise\n",
    "\n",
    "print(\"Second stage training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize and Run Second Stage Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sweep_config_second' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize second stage sweep\u001b[39;00m\n\u001b[32m      2\u001b[39m project_name = \u001b[33m'\u001b[39m\u001b[33mamt-second-stage-sweep\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m sweep_id = wandb.sweep(\u001b[43msweep_config_second\u001b[49m, project=project_name)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSecond stage sweep initialized!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSweep ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msweep_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sweep_config_second' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize second stage sweep\n",
    "project_name = 'amt-second-stage-sweep'\n",
    "sweep_id = wandb.sweep(sweep_config_second, project=project_name)\n",
    "\n",
    "print(f\"Second stage sweep initialized!\")\n",
    "print(f\"Sweep ID: {sweep_id}\")\n",
    "print(f\"View at: https://wandb.ai/{wandb.api.default_entity}/{project_name}/sweeps/{sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run second stage sweep\n",
    "# More experiments since we have fewer parameters to tune\n",
    "sweep_count = 8  # 5 runs per top config\n",
    "\n",
    "print(f\"Starting second stage sweep agent to run {sweep_count} experiments...\")\n",
    "print(f\"Testing {len(top_8_configs)} base configurations with fine-tuning\")\n",
    "print(\"Each experiment will run for 7 epochs with 20k training samples\")\n",
    "print(\"This will take approximately 10-15 minutes per experiment\")\n",
    "\n",
    "wandb.agent(sweep_id, function=train_second_stage, count=sweep_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Second Stage Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results from second stage\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"{wandb.api.default_entity}/{project_name}/sweeps/{sweep_id}\")\n",
    "\n",
    "# Get best run\n",
    "best_run = sweep.best_run()\n",
    "print(f\"Best run: {best_run.name}\")\n",
    "print(f\"Best validation MPJPE: {best_run.summary['val/hand_mpjpe']:.2f} mm\")\n",
    "print(f\"Base config index: {best_run.config['config_index']}\")\n",
    "print(\"\\nOptimized parameters:\")\n",
    "print(f\"  Learning rate: {best_run.config['learning_rate']:.2e}\")\n",
    "print(f\"  Weight decay: {best_run.config['weight_decay']:.2e}\")\n",
    "print(f\"  Dropout: {best_run.config['dropout']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results by base configuration\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Get all runs\n",
    "runs = sweep.runs\n",
    "data = []\n",
    "\n",
    "for run in runs:\n",
    "    if 'val/hand_mpjpe' in run.summary:\n",
    "        config = dict(run.config)\n",
    "        config['val_mpjpe'] = run.summary['val/hand_mpjpe']\n",
    "        data.append(config)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot results by base configuration\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. Performance by base config\n",
    "ax = axes[0]\n",
    "config_performance = df.groupby('config_index')['val_mpjpe'].agg(['mean', 'min', 'std', 'count'])\n",
    "x = config_performance.index\n",
    "ax.bar(x, config_performance['mean'], yerr=config_performance['std'], capsize=5)\n",
    "ax.scatter(x, config_performance['min'], color='red', s=100, marker='*', label='Best')\n",
    "ax.set_xlabel('Base Configuration Index')\n",
    "ax.set_ylabel('Validation MPJPE (mm)')\n",
    "ax.set_title('Performance by Base Configuration')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Learning rate vs performance\n",
    "ax = axes[1]\n",
    "ax.scatter(df['learning_rate'], df['val_mpjpe'], alpha=0.6, c=df['config_index'], cmap='tab10')\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Validation MPJPE (mm)')\n",
    "ax.set_title('Learning Rate vs Performance')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Weight decay vs performance\n",
    "ax = axes[2]\n",
    "ax.scatter(df['weight_decay'], df['val_mpjpe'], alpha=0.6, c=df['config_index'], cmap='tab10')\n",
    "ax.set_xlabel('Weight Decay')\n",
    "ax.set_ylabel('Validation MPJPE (mm)')\n",
    "ax.set_title('Weight Decay vs Performance')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Dropout vs performance\n",
    "ax = axes[3]\n",
    "ax.scatter(df['dropout'], df['val_mpjpe'], alpha=0.6, c=df['config_index'], cmap='tab10')\n",
    "ax.set_xlabel('Dropout')\n",
    "ax.set_ylabel('Validation MPJPE (mm)')\n",
    "ax.set_title('Dropout vs Performance')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Second Stage Fine-tuning Results', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Total runs: {len(df)}\")\n",
    "print(f\"Best MPJPE: {df['val_mpjpe'].min():.2f} mm\")\n",
    "print(f\"Mean MPJPE: {df['val_mpjpe'].mean():.2f} mm\")\n",
    "print(f\"Std MPJPE: {df['val_mpjpe'].std():.2f} mm\")\n",
    "\n",
    "# Best parameters for each base config\n",
    "print(\"\\nBest parameters for each base configuration:\")\n",
    "for config_idx in range(8):\n",
    "    config_df = df[df['config_index'] == config_idx]\n",
    "    if len(config_df) > 0:\n",
    "        best_idx = config_df['val_mpjpe'].idxmin()\n",
    "        best_row = config_df.loc[best_idx]\n",
    "        print(f\"\\nConfig {config_idx}: {best_row['val_mpjpe']:.2f} mm\")\n",
    "        print(f\"  LR: {best_row['learning_rate']:.2e}, WD: {best_row['weight_decay']:.2e}, Dropout: {best_row['dropout']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Final Optimized Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final best configuration\n",
    "best_config = dict(best_run.config)\n",
    "best_base_config = top_8_configs[best_config['config_index']]\n",
    "\n",
    "# Create final optimized configuration\n",
    "final_config = OmegaConf.load('../configs/default_config.yaml')\n",
    "\n",
    "# Apply all parameters from best configuration\n",
    "# From base config\n",
    "final_config.training.batch_size = best_base_config['batch_size']\n",
    "final_config.data.augmentation.rotation_range = best_base_config['aug_rotation_range']\n",
    "final_config.data.augmentation.scale_range = [\n",
    "    best_base_config['aug_scale_min'],\n",
    "    best_base_config['aug_scale_max']\n",
    "]\n",
    "final_config.data.augmentation.translation_std = best_base_config['aug_translation_std']\n",
    "final_config.data.augmentation.color_jitter = best_base_config['aug_color_jitter']\n",
    "final_config.data.augmentation.joint_noise_std = best_base_config['aug_joint_noise_std']\n",
    "\n",
    "# Loss weights\n",
    "for key in best_base_config:\n",
    "    if key.startswith('loss_weight_'):\n",
    "        loss_key = key.replace('loss_weight_', '')\n",
    "        final_config.loss.loss_weights[loss_key] = best_base_config[key]\n",
    "\n",
    "# Loss configuration\n",
    "final_config.loss.diversity_margin = best_base_config['diversity_margin']\n",
    "final_config.loss.per_joint_weighting = best_base_config['per_joint_weighting']\n",
    "final_config.loss.fingertip_weight = best_base_config['fingertip_weight']\n",
    "\n",
    "# From second stage optimization\n",
    "final_config.training.learning_rate = best_config['learning_rate']\n",
    "final_config.training.weight_decay = best_config['weight_decay']\n",
    "final_config.model.dropout = best_config['dropout']\n",
    "\n",
    "# Save final configuration\n",
    "OmegaConf.save(final_config, '../configs/final_optimized_config.yaml')\n",
    "\n",
    "print(\"Final optimized configuration saved to configs/final_optimized_config.yaml\")\n",
    "print(\"\\nFinal optimized parameters:\")\n",
    "print(f\"  Learning rate: {final_config.training.learning_rate:.2e}\")\n",
    "print(f\"  Weight decay: {final_config.training.weight_decay:.2e}\")\n",
    "print(f\"  Dropout: {final_config.model.dropout:.3f}\")\n",
    "print(f\"  Batch size: {final_config.training.batch_size}\")\n",
    "print(f\"  Scheduler: {best_base_config['scheduler_type']}\")\n",
    "print(\"\\nYou can now use this configuration for final training:\")\n",
    "print(\"python train_advanced.py --config configs/final_optimized_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a second stage W&B sweep for fine-tuning the Advanced Manipulation Transformer:\n",
    "\n",
    "**Stage 1** (Previous notebook):\n",
    "- Broad search across all hyperparameters\n",
    "- Identified top 8 configurations\n",
    "\n",
    "**Stage 2** (This notebook):\n",
    "- Started from top 8 configurations\n",
    "- Fine-tuned only critical parameters:\n",
    "  - Learning rate: log-uniform [1e-5, 1e-3]\n",
    "  - Weight decay: log-uniform [1e-5, 1e-2]\n",
    "  - Dropout: uniform [0.05, 0.4]\n",
    "- Ran 40 experiments (5 per base configuration)\n",
    "\n",
    "**Results**:\n",
    "- Further improved performance through fine-tuning\n",
    "- Identified optimal learning rate, weight decay, and dropout for each base configuration\n",
    "- Exported final optimized configuration combining best base config with optimal fine-tuning\n",
    "\n",
    "The two-stage approach allows for:\n",
    "1. Efficient exploration of the large hyperparameter space\n",
    "2. Focused optimization of critical parameters\n",
    "3. Better final performance than single-stage search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}